{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"alwayson/","text":"It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab . But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out. What is Always-ON SR Linux for? # The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user. getting to know SR Linux CLI SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier. New users can make their first steps by looking at the show commands, exploring the datastores, running info from commands and getting the grips of configuration basics by entering into the configuration mode. YANG browsing By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers. gNMI exploration The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux. gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves. Connection details # Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces: Method Details SSH address: ssh guest@on.srlinux.dev -p 44268 password: n0k1asrlinux for key-based authentication use this key to authenticate the guest user gNMI 1 gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\ capabilities JSON-RPC 2 http://http.on.srlinux.dev gNMI # SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate. Guest user # The guest user has the following settings applied to it: Read-only mode bash and file commands are disabled Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that guest can't commit them. Always-ON sandbox setup # The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node. Protocols and Services # We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise. The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services. In the overlay the following services are configured: Layer 2 EVPN with VXLAN dataplane 1 with mac-vrf-100 network instance created on SR Linux Layer 3 EVPN with VXLAN dataplane with ip-vrf-200 network instance created on SR Linux check this tutorial to understand how this service is configured \u21a9 \u21a9 HTTP service running over port 80 \u21a9","title":"Always-ON SR Linux"},{"location":"alwayson/#what-is-always-on-sr-linux-for","text":"The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user. getting to know SR Linux CLI SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier. New users can make their first steps by looking at the show commands, exploring the datastores, running info from commands and getting the grips of configuration basics by entering into the configuration mode. YANG browsing By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers. gNMI exploration The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux. gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves.","title":"What is Always-ON SR Linux for?"},{"location":"alwayson/#connection-details","text":"Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces: Method Details SSH address: ssh guest@on.srlinux.dev -p 44268 password: n0k1asrlinux for key-based authentication use this key to authenticate the guest user gNMI 1 gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\ capabilities JSON-RPC 2 http://http.on.srlinux.dev","title":"Connection details"},{"location":"alwayson/#gnmi","text":"SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate.","title":"gNMI"},{"location":"alwayson/#guest-user","text":"The guest user has the following settings applied to it: Read-only mode bash and file commands are disabled Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that guest can't commit them.","title":"Guest user"},{"location":"alwayson/#always-on-sandbox-setup","text":"The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node.","title":"Always-ON sandbox setup"},{"location":"alwayson/#protocols-and-services","text":"We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise. The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services. In the overlay the following services are configured: Layer 2 EVPN with VXLAN dataplane 1 with mac-vrf-100 network instance created on SR Linux Layer 3 EVPN with VXLAN dataplane with ip-vrf-200 network instance created on SR Linux check this tutorial to understand how this service is configured \u21a9 \u21a9 HTTP service running over port 80 \u21a9","title":"Protocols and Services"},{"location":"community/","text":"SR Linux has lots to offer to various groups of engineers... Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS. Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI. Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support. We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community 1 is the SR Linux Discord Server which everyone can join! Join SR Linux Discord Server this is an unofficial community. Engineers to engineers. \u21a9","title":"SR Linux Community"},{"location":"get-started/","text":"SR Linux packs a lot of unique features that the data center networking teams can leverage. Some of the features being truly new to the networking domain. The goal of this portal is to introduce SR Linux to the visitors through the interactive tutorials centered around SR Linux services and capabilities. We believe that learning by doing yields the best results. With that in mind we made SR Linux container image available to everybody without any registration or licensing requirements The public SR Linux container image when powered by containerlab allows us to create easily deployable labs that everyone can launch at their convenience. All that to let you not only read about the features we offer, but to try them live! SR Linux container image # A single container image that hosts management, control and data plane functions is all you need to get started. Getting the image # To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry . This means that you can pull SR Linux container image exactly the same way as you would pull any other image: docker pull ghcr.io/nokia/srlinux When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the nokia/srlinux image has and change the docker pull command accordingly. Running SR Linux # When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces. A system on which you can run SR Linux containers should conform to the following requirements: Linux OS with a kernel v4+ 1 . Docker container runtime. At least 2 vCPU and 4GB RAM. A user with administrative privileges. Let's explore the different ways you can launch SR Linux container. Docker CLI # docker CLI offers a quick way to run standalone SR Linux container: docker run -t -d --rm --privileged \\ -u $( id -u ) : $( id -g ) \\ --name srlinux ghcr.io/nokia/srlinux \\ sudo bash /opt/srlinux/bin/sr_linux The above command will start the container named srlinux on the host system with a single management interface attached to the default docker network. This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work. For multi-node SR Linux deployments containerlab 3 offers a better way. Containerlab # Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle. A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together: # file: srlinux.clab.yml name : srlinux topology : nodes : srl1 : kind : srl image : ghcr.io/nokia/srlinux srl2 : kind : srl image : ghcr.io/nokia/srlinux links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] By copying this file over to your system you can immediately deploy it with containerlab: containerlab deploy -t srlinux.clab.yml INFO[0000] Parsing & checking topology file: srlinux.clab.yml INFO[0000] Creating lab directory: /root/demo/clab-srlinux INFO[0000] Creating container: srl1 INFO[0000] Creating container: srl2 INFO[0001] Creating virtual wire: srl1:e1-1 <--> srl2:e1-1 INFO[0001] Writing /etc/hosts file +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlinux-srl1 | 50826b3e3703 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/64 | | 2 | clab-srlinux-srl2 | 4d4494aba320 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/64 | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ Deployment verification # Regardless of the way you spin up SR Linux container it will be visible in the output of the docker ps command. If the deployment process went well and the container did not exit, a user can see it with docker ps command: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4d4494aba320 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 32 minutes ago Up 32 minutes clab-learn-01-srl2 The logs of the running container can be displayed with docker logs <container-name> . In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the docker ps output as this command only shows running containers. Containers which are in the exited status will be part of the docker ps -a output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination. Connecting to SR Linux # When SR Linux container is up and running, users can connect to it over different interfaces. CLI # One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface . To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the sr_cli process inside of it: # get SR Linux container name -> clab-srl01-srl $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17a47c58ad59 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 10 seconds ago Up 6 seconds clab-learn-01-srl1 # start the sr_cli process inside this container to get access to CLI docker exec -it clab-learn-01-srl1 sr_cli Using configuration file ( s ) : [] Welcome to the srlinux CLI. Type 'help' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials admin:admin you can connect to the CLI over the network: # containerlab creates local /etc/hosts entries # for container names to resolve to their IP ssh admin@clab-learn-01-srl1 admin@clab-learn-01-srl1 's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type ' help ' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# gNMI # SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400. Using the gNMI client 2 users can explore SR Linux' gNMI interface: gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities gNMI version: 0.7.0 supported models: - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31 - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30 - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31 <SNIP> Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container \u21a9 for example gnmic \u21a9 The labs referenced on this site are deployed with containerlab unless stated otherwise \u21a9","title":"Get SR Linux"},{"location":"get-started/#sr-linux-container-image","text":"A single container image that hosts management, control and data plane functions is all you need to get started.","title":"SR Linux container image"},{"location":"get-started/#getting-the-image","text":"To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry . This means that you can pull SR Linux container image exactly the same way as you would pull any other image: docker pull ghcr.io/nokia/srlinux When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the nokia/srlinux image has and change the docker pull command accordingly.","title":"Getting the image"},{"location":"get-started/#running-sr-linux","text":"When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces. A system on which you can run SR Linux containers should conform to the following requirements: Linux OS with a kernel v4+ 1 . Docker container runtime. At least 2 vCPU and 4GB RAM. A user with administrative privileges. Let's explore the different ways you can launch SR Linux container.","title":"Running SR Linux"},{"location":"get-started/#docker-cli","text":"docker CLI offers a quick way to run standalone SR Linux container: docker run -t -d --rm --privileged \\ -u $( id -u ) : $( id -g ) \\ --name srlinux ghcr.io/nokia/srlinux \\ sudo bash /opt/srlinux/bin/sr_linux The above command will start the container named srlinux on the host system with a single management interface attached to the default docker network. This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work. For multi-node SR Linux deployments containerlab 3 offers a better way.","title":"Docker CLI"},{"location":"get-started/#containerlab","text":"Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle. A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together: # file: srlinux.clab.yml name : srlinux topology : nodes : srl1 : kind : srl image : ghcr.io/nokia/srlinux srl2 : kind : srl image : ghcr.io/nokia/srlinux links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] By copying this file over to your system you can immediately deploy it with containerlab: containerlab deploy -t srlinux.clab.yml INFO[0000] Parsing & checking topology file: srlinux.clab.yml INFO[0000] Creating lab directory: /root/demo/clab-srlinux INFO[0000] Creating container: srl1 INFO[0000] Creating container: srl2 INFO[0001] Creating virtual wire: srl1:e1-1 <--> srl2:e1-1 INFO[0001] Writing /etc/hosts file +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlinux-srl1 | 50826b3e3703 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/64 | | 2 | clab-srlinux-srl2 | 4d4494aba320 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/64 | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+","title":"Containerlab"},{"location":"get-started/#deployment-verification","text":"Regardless of the way you spin up SR Linux container it will be visible in the output of the docker ps command. If the deployment process went well and the container did not exit, a user can see it with docker ps command: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4d4494aba320 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 32 minutes ago Up 32 minutes clab-learn-01-srl2 The logs of the running container can be displayed with docker logs <container-name> . In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the docker ps output as this command only shows running containers. Containers which are in the exited status will be part of the docker ps -a output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination.","title":"Deployment verification"},{"location":"get-started/#connecting-to-sr-linux","text":"When SR Linux container is up and running, users can connect to it over different interfaces.","title":"Connecting to SR Linux"},{"location":"get-started/#cli","text":"One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface . To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the sr_cli process inside of it: # get SR Linux container name -> clab-srl01-srl $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17a47c58ad59 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 10 seconds ago Up 6 seconds clab-learn-01-srl1 # start the sr_cli process inside this container to get access to CLI docker exec -it clab-learn-01-srl1 sr_cli Using configuration file ( s ) : [] Welcome to the srlinux CLI. Type 'help' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials admin:admin you can connect to the CLI over the network: # containerlab creates local /etc/hosts entries # for container names to resolve to their IP ssh admin@clab-learn-01-srl1 admin@clab-learn-01-srl1 's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type ' help ' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1#","title":"CLI"},{"location":"get-started/#gnmi","text":"SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400. Using the gNMI client 2 users can explore SR Linux' gNMI interface: gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities gNMI version: 0.7.0 supported models: - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31 - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30 - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31 <SNIP> Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container \u21a9 for example gnmic \u21a9 The labs referenced on this site are deployed with containerlab unless stated otherwise \u21a9","title":"gNMI"},{"location":"basics/cfgmgmt/","text":"SR Linux employs a transaction-based configuration management system. That allows for a number of changes to be made to the configuration with an explicit commit required to apply the changes as a single transaction. Configuration file # The default location for the configuration file is /etc/opt/srlinux/config.json . If there is no configuration file present, a basic configuration file is auto-generated with the following defaults: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created SSH server is enabled Some default IPv4/v6 CPM filters Configuration modes # Configuration modes define how the system is running when transactions are performed. Supported modes are the following: Running: the default mode when logging in and displays displays the currently running or active configuration. State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: this mode is used to modify configuration. Modifications are not applied until the commit is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes: Shared: this is the default mode when entering the candidate mode with enter candidate command. This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied. Exclusive Candidate: When entering candidate mode with enter candidate exclusive , it locks out other users from making changes to the candidate configuration. You can enter candidate exclusive mode only under the following conditions: The current shared candidate configuration has not been modified. There are no other users in candidate shared mode. No other users have entered candidate exclusive mode. Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed. When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name private-<username> unless a unique name is defined. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device. Setting the configuration mode # After logging in to the CLI, you are initially placed in running mode. The following table provides commands to enter in a specific mode: Candidate mode Command to enter Candidate shared enter candidate Candidate mode for named shared candidate enter candidate name <name> Candidate private enter candidate private Candidate mode for named private candidate enter candidate private name <name> Candidate exclusive enter candidate exclusive Exclusive mode for named candidate enter candidate exclusive name <name> Running enter running State enter state Show enter show Committing configuration # Changes made during a configuration modification session do not take effect until a commit command is issued. Several options are available for commit command, below are the most notable ones: Option Action commit now Apply the changes, exit candidate mode, and enter running mode commit stay Apply the changes and then remain in candidate mode commit save Apply the changes and automatically save the commit to the startup configuration commit confirmed Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted Deleting configuration # Use the delete command to delete configurations while in candidate mode. The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { login-banner \"Welcome to SRLinux!\" } } --{ candidate shared default}--[ ]-- A:leaf1# delete system banner --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { } } Discarding configuration # You can discard previously applied configurations with the discard command. To discard the changes and remain in candidate mode with a new candidate session, enter discard stay . To discard the changes, exit candidate mode, and enter running mode, enter discard now . Displaying configuration diff # Use the diff command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore. The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination. Global arguments include: baseline , candidate , checkpoint , factory , file , from , rescue , running , and startup . The diff command can be used outside of candidate mode, but only if used with arguments. The following shows a basic diff command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: --{ candidate shared default }--[ ]-- # interface ethernet-1/1 admin-state disable --{ * candidate shared default }--[ ]-- # interface ethernet-1/2 description \"updated\" --{ * candidate shared default }--[ ]-- # diff interface ethernet-1/1 { + admin-state disable } + interface ethernet-1/2 { + description updated + } Displaying configuration details # Use the info command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context. To display the entire configuration, enter info from the root context: --{ candidate shared default}--[ ]-- # info <all the configuration is displayed> --{ candidate }--[ ]-- To display the configuration for a specific context, enter info and specify the context: --{ candidate shared default}--[ ]-- # info system lldp system { lldp { admin-state enable hello-timer 600 management-address mgmt0.0 { type [ IPv4 ] } interface mgmt0 { admin-state disable } } } The following info command options are rather useful: as-json - to display JSON-formatted output detail - to display values for all parameters, including those not specifically configured flat - to display the output as a series of set statements, omitting indentation for any sub-contexts","title":"Configuration management"},{"location":"basics/cfgmgmt/#configuration-file","text":"The default location for the configuration file is /etc/opt/srlinux/config.json . If there is no configuration file present, a basic configuration file is auto-generated with the following defaults: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created SSH server is enabled Some default IPv4/v6 CPM filters","title":"Configuration file"},{"location":"basics/cfgmgmt/#configuration-modes","text":"Configuration modes define how the system is running when transactions are performed. Supported modes are the following: Running: the default mode when logging in and displays displays the currently running or active configuration. State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: this mode is used to modify configuration. Modifications are not applied until the commit is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes: Shared: this is the default mode when entering the candidate mode with enter candidate command. This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied. Exclusive Candidate: When entering candidate mode with enter candidate exclusive , it locks out other users from making changes to the candidate configuration. You can enter candidate exclusive mode only under the following conditions: The current shared candidate configuration has not been modified. There are no other users in candidate shared mode. No other users have entered candidate exclusive mode. Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed. When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name private-<username> unless a unique name is defined. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.","title":"Configuration modes"},{"location":"basics/cfgmgmt/#setting-the-configuration-mode","text":"After logging in to the CLI, you are initially placed in running mode. The following table provides commands to enter in a specific mode: Candidate mode Command to enter Candidate shared enter candidate Candidate mode for named shared candidate enter candidate name <name> Candidate private enter candidate private Candidate mode for named private candidate enter candidate private name <name> Candidate exclusive enter candidate exclusive Exclusive mode for named candidate enter candidate exclusive name <name> Running enter running State enter state Show enter show","title":"Setting the configuration mode"},{"location":"basics/cfgmgmt/#committing-configuration","text":"Changes made during a configuration modification session do not take effect until a commit command is issued. Several options are available for commit command, below are the most notable ones: Option Action commit now Apply the changes, exit candidate mode, and enter running mode commit stay Apply the changes and then remain in candidate mode commit save Apply the changes and automatically save the commit to the startup configuration commit confirmed Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted","title":"Committing configuration"},{"location":"basics/cfgmgmt/#deleting-configuration","text":"Use the delete command to delete configurations while in candidate mode. The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { login-banner \"Welcome to SRLinux!\" } } --{ candidate shared default}--[ ]-- A:leaf1# delete system banner --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { } }","title":"Deleting configuration"},{"location":"basics/cfgmgmt/#discarding-configuration","text":"You can discard previously applied configurations with the discard command. To discard the changes and remain in candidate mode with a new candidate session, enter discard stay . To discard the changes, exit candidate mode, and enter running mode, enter discard now .","title":"Discarding configuration"},{"location":"basics/cfgmgmt/#displaying-configuration-diff","text":"Use the diff command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore. The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination. Global arguments include: baseline , candidate , checkpoint , factory , file , from , rescue , running , and startup . The diff command can be used outside of candidate mode, but only if used with arguments. The following shows a basic diff command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: --{ candidate shared default }--[ ]-- # interface ethernet-1/1 admin-state disable --{ * candidate shared default }--[ ]-- # interface ethernet-1/2 description \"updated\" --{ * candidate shared default }--[ ]-- # diff interface ethernet-1/1 { + admin-state disable } + interface ethernet-1/2 { + description updated + }","title":"Displaying configuration diff"},{"location":"basics/cfgmgmt/#displaying-configuration-details","text":"Use the info command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context. To display the entire configuration, enter info from the root context: --{ candidate shared default}--[ ]-- # info <all the configuration is displayed> --{ candidate }--[ ]-- To display the configuration for a specific context, enter info and specify the context: --{ candidate shared default}--[ ]-- # info system lldp system { lldp { admin-state enable hello-timer 600 management-address mgmt0.0 { type [ IPv4 ] } interface mgmt0 { admin-state disable } } } The following info command options are rather useful: as-json - to display JSON-formatted output detail - to display values for all parameters, including those not specifically configured flat - to display the output as a series of set statements, omitting indentation for any sub-contexts","title":"Displaying configuration details"},{"location":"basics/hwtypes/","text":"The SR Linux software supports seven Nokia hardware platforms 1 : 7250 IXR-6 7250 IXR-10 7220 IXR-D1 7220 IXR-D2 7220 IXR-D3 7220 IXR-H2 7220 IXR-H3 Out of those seven hardware variants, the first five are available for emulation within SR Linux container image. The type field under the node configuration sets the emulated hardware type in the containerlab file: # part of the evpn01.clab.yml file nodes : leaf1 : kind : srl type : ixrd2 # <- hardware type this node will emulate The type field defines the hardware variant that this SR Linux node will emulate. The available type values are: type value HW platform ixr6 7250 IXR-6 ixr10 7250 IXR-10 ixrd1 7220 IXR-D1 ixrd2 7220 IXR-D2 ixrd3 7220 IXR-D3 Tip Containerlab-launched nodes are started as ixr6 hardware type unless set to a different type in the clab file. SR Linux can also run on the whitebox/3 rd party switches. \u21a9","title":"Hardware types"},{"location":"basics/ifaces/","text":"On the SR Linux, an interface is any physical or logical port through which packets can be sent to or received from other devices. Loopback # Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named loN , where N is 0 to 255. System # The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces: The system interface can be bound to the default network-instance only. The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses. The system interface cannot be administratively disabled. Once configured, it is always up. The SR Linux supports a single system interface named system0 . When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels. Network # Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic. The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options. In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: ethernet-slot/port . For example, interface ethernet-2/1 refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card. On 7220 IXR-D3 systems, the QSFP28 connector ports (ports 1/3-1/33 ) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: ethernet-slot/port/breakout-port . For example, if interface ethernet 1/3 is enabled for breakout mode, its breakout ports are named as follows: ethernet 1/3/1 ethernet 1/3/2 ethernet 1/3/3 ethernet 1/3/4 Management # Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named mgmt0 . The mgmt0 interface supports the same functionality and defaults as a network interface, except for the following: Packets sent and received on the mgmt0 interface are processed completely in software. The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class. The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port. Integrated Routing and Bridging (IRB) # IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface. Subinterfaces # On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface. Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags. While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance. You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface. The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class SR Linux subinterfaces can be specified as type routed or bridged: Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf. Bridged subinterfaces can be assigned to a network-instance of type mac-vrf. Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.","title":"Interfaces"},{"location":"basics/ifaces/#loopback","text":"Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named loN , where N is 0 to 255.","title":"Loopback"},{"location":"basics/ifaces/#system","text":"The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces: The system interface can be bound to the default network-instance only. The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses. The system interface cannot be administratively disabled. Once configured, it is always up. The SR Linux supports a single system interface named system0 . When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels.","title":"System"},{"location":"basics/ifaces/#network","text":"Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic. The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options. In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: ethernet-slot/port . For example, interface ethernet-2/1 refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card. On 7220 IXR-D3 systems, the QSFP28 connector ports (ports 1/3-1/33 ) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: ethernet-slot/port/breakout-port . For example, if interface ethernet 1/3 is enabled for breakout mode, its breakout ports are named as follows: ethernet 1/3/1 ethernet 1/3/2 ethernet 1/3/3 ethernet 1/3/4","title":"Network"},{"location":"basics/ifaces/#management","text":"Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named mgmt0 . The mgmt0 interface supports the same functionality and defaults as a network interface, except for the following: Packets sent and received on the mgmt0 interface are processed completely in software. The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class. The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port.","title":"Management"},{"location":"basics/ifaces/#integrated-routing-and-bridging-irb","text":"IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface.","title":"Integrated Routing and Bridging (IRB)"},{"location":"basics/ifaces/#subinterfaces","text":"On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface. Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags. While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance. You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface. The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class SR Linux subinterfaces can be specified as type routed or bridged: Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf. Bridged subinterfaces can be assigned to a network-instance of type mac-vrf. Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.","title":"Subinterfaces"},{"location":"basics/mgmt/","text":"Nokia SR Linux is equipped with 100% YANG modelled management interfaces. The supported management interfaces (CLI, JSON-RPC, and gNMI) access the common management API layer via a gRPC interface. Since all interfaces act as a client towards a common management API, SR Linux provides complete consistency across all the management interfaces with regards to the capabilities available to each of them. SR Linux CLI # The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session. Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces. Features # Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands. Accessing the CLI # After the SR Linux device is initialized, you can access the CLI using a console or SSH connection. Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to leaf1 : ssh admin@clab-quickstart-leaf1 Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts. admin@clab-quickstart-leaf1's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# Prompt # By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: --{ modified? mode_and_session_type }--[ context ]-- hostname# Example: --{ * candidate shared }--[ acl ]-- 3-node-srlinux-A# The CLI prompt is configurable and can be changed within the environment prompt configuration context. In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays: the current mode and session type whether the configuration has been modified the user name and session ID of the current AAA session and the local time For example: Current mode: * candidate shared root (36) Wed 09:52PM gNMI # The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system. SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information. Supported gNMI RPCs are: Get Set Subscribe Capabilities JSON-RPC # The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system. If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.","title":"Management interfaces"},{"location":"basics/mgmt/#sr-linux-cli","text":"The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session. Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces.","title":"SR Linux CLI"},{"location":"basics/mgmt/#features","text":"Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.","title":"Features"},{"location":"basics/mgmt/#accessing-the-cli","text":"After the SR Linux device is initialized, you can access the CLI using a console or SSH connection. Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to leaf1 : ssh admin@clab-quickstart-leaf1 Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts. admin@clab-quickstart-leaf1's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1#","title":"Accessing the CLI"},{"location":"basics/mgmt/#prompt","text":"By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: --{ modified? mode_and_session_type }--[ context ]-- hostname# Example: --{ * candidate shared }--[ acl ]-- 3-node-srlinux-A# The CLI prompt is configurable and can be changed within the environment prompt configuration context. In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays: the current mode and session type whether the configuration has been modified the user name and session ID of the current AAA session and the local time For example: Current mode: * candidate shared root (36) Wed 09:52PM","title":"Prompt"},{"location":"basics/mgmt/#gnmi","text":"The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system. SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information. Supported gNMI RPCs are: Get Set Subscribe Capabilities","title":"gNMI"},{"location":"basics/mgmt/#json-rpc","text":"The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system. If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.","title":"JSON-RPC"},{"location":"basics/netwinstance/","text":"On the SR Linux, you can configure one or more virtual routing instances, known as network instances. Each network instance has its own interfaces, its own protocol instances, its own route table, and its own FIB. When a packet arrives on a subinterface associated with a network instance, it is forwarded according to the FIB of that network instance. Transit packets are normally forwarded out another subinterface of the network instance. SR Linux supports the following types of network instances: default ip-vrf mac-vrf The initial startup configuration for SR Linux has a single default network instance. By default, there are no ip-vrf or mac-vrf network instances; these must be created by explicit configuration. The ip-vrf network instances are the building blocks of Layer 3 IP VPN services, and mac-vrf network instances are the building blocks of EVPN services. Within a network instance, you can configure BGP, OSPF, and IS-IS protocol options that apply only to that network instance.","title":"Network instances"},{"location":"tutorials/about/","text":"Learning by doing is not only the most effective method, but also an extremely fun one. The hands-on tutorials we provide in this section are designed in such a way that anyone can launch them at absolutely no cost whenever they want it whatever machine they have and run it for as long as required The tutorials use the opensource containerlab project to deploy the lab environment with all the needed components. This ensures that both the tutorial authors and the readers work on exactly the same environment. No more second guessing why the tutorial's outputs differ from yours!","title":"SR Linux tutorials"},{"location":"tutorials/l2evpn/evpn/","text":"Ethernet Virtual Private Network (EVPN), along with Virtual eXtensible LAN (VXLAN), is a technology that allows Layer 2 and Layer 3 traffic to be tunneled across an IP network. The SR Linux EVPN-VXLAN solution enables Layer 2 Broadcast Domains (BDs) in multi-tenant data centers using EVPN for the control plane and VXLAN as the data plane. It includes the following features: EVPN for VXLAN tunnels (Layer 2), extending a BD in overlay multi-tenant DCs EVPN for VXLAN tunnels (Layer 3), allowing inter-subnet-forwarding for unicast traffic within the same tenant infrastructure This tutorial is focused on EVPN for VXLAN tunnels Layer 2. Overview # EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch. VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently. The following figure demonstrates this concept where servers srv1 and srv2 are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain. Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable 1 , we can proceed with the VXLAN based EVPN service configuration. While doing that we will cover the following topics: VXLAN tunnel interface configuration Network instances of type mac-vrf Bridged subinterfaces and BGP EVPN control plane configuration IBGP for EVPN # Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches. For that iBGP configuration we will create a group called iBGP-overlay which will have the peer-as and local-as set to 100 to form an iBGP neighborship. The group will also host the same permissive all routing policy, enabled evpn and disabled ipv4-unicast address families. Then for each leaf we add a new BGP neighbor addressed by the remote system0 interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config: leaf1 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { peer-group iBGP-overlay transport { local-address 10.0.0.1 } } commit now leaf2 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { peer-group iBGP-overlay transport { local-address 10.0.0.2 } } commit now Ensure that the iBGP session is established before proceeding any further: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [0/0/0] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above. Access interfaces # Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches: Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be bridged , this makes the interfaces only attachable to a network instance of mac-vrf type with MAC learning and layer-2 forwarding enabled. The following config is applied to both leaf switches: enter candidate /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } commit now As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers. Tunnel/VXLAN interface # After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric. The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name vxlan<N> where N = 0..255 . A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI). The index can be a number in the range 0-4294967295. The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed). The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of 1..16777215 . The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes. SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets. Note The source IP used in the vxlan-interfaces is the IPv4 address of subinterface system0.0 in the default network-instance. The above information translates to a configuration snippet which is applicable both to leaf1 and leaf2 nodes. enter candidate /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } commit now To verify the tunnel interface configuration: A:leaf2# show tunnel-interface vxlan-interface brief --------------------------------------------------------------------------------- Show report for vxlan-tunnels --------------------------------------------------------------------------------- +------------------+-----------------+---------+-------------+------------------+ | Tunnel Interface | VxLAN Interface | Type | Ingress VNI | Egress source-ip | +==================+=================+=========+=============+==================+ | vxlan1 | vxlan1.1 | bridged | 1 | 10.0.0.2/32 | +------------------+-----------------+---------+-------------+------------------+ --------------------------------------------------------------------------------- Summary 1 tunnel-interfaces, 1 vxlan interfaces 0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip --------------------------------------------------------------------------------- MAC-VRF # Now it is a turn of MAC-VRF to get configured. The network-instance type mac-vrf functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration. By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance: enter candidate /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } } commit now Server interfaces # The servers in our fabric do not have any addresses on their eth1 interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration. By the end of this section, we will have the following addressing scheme complete: To connect to a shell of a server execute docker exec -it <container-name> bash : srv1 docker exec -it clab-evpn01-srv1 bash srv2 docker exec -it clab-evpn01-srv2 bash Within the shell, configure MAC address 2 and IPv4 address for the eth1 interface according to the diagram above, as with this interface the server is connected to the leaf switch. srv1 ip link set address 00:c1:ab:00:00:01 dev eth1 ip addr add 192.168.0.1/24 dev eth1 srv2 ip link set address 00:c1:ab:00:00:02 dev eth1 ip addr add 192.168.0.2/24 dev eth1 Let's try to ping server2 from server1: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 2028ms That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the srv1 interface MAC to get learned by the leaf1 mac-vrf network instance: A:leaf1# show network-instance vrf-1 bridge-table mac-table all ---------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ---------------------------------------------------------------------------------------------------------------------- +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+==========================+===========+========+========+=======+==========================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 242 | 2021-07-13T17:36:23.000Z | +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 1 Total 1 Active ---------------------------------------------------------------------------------------------------------------------- EVPN in MAC-VRF # To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our vrf-1 network-instance. EVPN configuration under the mac-vrf network instance will require two configuration containers: bgp-vpn - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist. bgp-evpn - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to bgp-instance 1 is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI). The following configuration is entered on both leafs: enter candidate /network-instance vrf-1 protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } commit now Once configured, the bgp-vpn instance can be checked to have the RT/RD values set: A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1 ===================================================================== Net Instance : vrf-1 bgp Instance 1 --------------------------------------------------------------------- route-distinguisher: 10.0.0.1:111, auto-derived-from-evi export-route-target: target:100:111, manual import-route-target: target:100:111, manual ===================================================================== VNI to EVI mapping As of release 21.6, SR Linux uses only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI. 5 Final configurations # For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration 6 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks. pastable snippets leaf1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.1 } } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } /interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } commit now leaf2 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.2 } } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } commit now spine1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /network-instance default { interface ethernet-1/1.0 { } interface ethernet-1/2.0 { } interface system0.0 { } protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-as 101 peer-group eBGP-underlay } neighbor 192.168.12.1 { peer-as 102 peer-group eBGP-underlay } } } } /interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } commit now srv1 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv1 bash ip link set address 00 :c1:ab:00:00:01 dev eth1 ip addr add 192 .168.0.1/24 dev eth1 srv2 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv2 bash ip link set address 00 :c1:ab:00:00:02 dev eth1 ip addr add 192 .168.0.2/24 dev eth1 Verification # EVPN IMET routes # When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [1/1/1] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3). The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose: Auto-discovery of the remote VTEPs attached to the same EVI Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated The IMET/RT3 routes can be viewed in summary and detailed modes: RT3 summary A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary ---------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------- Type 3 Inclusive Multicast Ethernet Tag Routes +--------+---------------------+------------+---------------------+---------------------+---------------------+ | Status | Route-distinguisher | Tag-ID | Originator-IP | neighbor | Next-Hop | +========+=====================+============+=====================+=====================+=====================+ | u*> | 10.0.0.2:111 | 0 | 10.0.0.2 | 10.0.0.2 | 10.0.0.2 | +--------+---------------------+------------+---------------------+---------------------+---------------------+ ---------------------------------------------------------------------------------------------------------------- 1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid ---------------------------------------------------------------------------------------------------------------- RT3 detailed A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail ------------------------------------------------------------------------------------- Show report for the EVPN routes in network-instance \"default\" ------------------------------------------------------------------------------------- Route Distinguisher: 10.0.0.2:111 Tag-ID : 0 Originating router : 10.0.0.2 neighbor : 10.0.0.2 Received paths : 1 Path 1: <Best,Valid,Used,> VNI : 1 Route source : neighbor 10.0.0.2 (last modified 2m3s ago) Route preference: No MED, LocalPref is 100 Atomic Aggr : false BGP next-hop : 10.0.0.2 AS Path : i Communities : [target:100:111, bgp-tunnel-encap:VXLAN] RR Attributes : No Originator-ID, Cluster-List is [] Aggregation : None Unknown Attr : None Invalid Reason : None Tie Break Reason: none -------------------------------------------------------------------------------------- Lets capture those routes? Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers . By capturing on the e1-49 interface of the clab-evpn01-leaf1 container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the vrf-1 network instance to trigger the BGP updates to flow and see them in the live capture. Here is the pcap file with the IMET routes advertisements between leaf1 and leaf2 . When the IMET routes from leaf2 are imported for vrf-1 network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 multicast destinations (flooding-list) ------------------------------------------------------------------------------- +--------------+------------+-------------------+----------------------+ | VTEP Address | Egress VNI | Destination-index | Multicast-forwarding | +==============+============+===================+======================+ | 10.0.0.2 | 1 | 160078821962 | BUM | +--------------+------------+-------------------+----------------------+ ------------------------------------------------------------------------------- Summary 1 multicast-destinations ------------------------------------------------------------------------------- This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table. As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes. VXLAN tunnels # After receiving EVPN routes from the remote leafs with VXLAN encapsulation 4 , SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the leaf1 switch. A:leaf1# /show tunnel vxlan-tunnel all ---------------------------------------------------------- Show report for vxlan-tunnels ---------------------------------------------------------- +--------------+--------------+--------------------------+ | VTEP Address | Index | Last Change | +==============+==============+==========================+ | 10.0.0.2 | 160078821947 | 2021-07-13T21:13:50.000Z | +--------------+--------------+--------------------------+ 1 VXLAN tunnels, 1 active, 0 inactive ---------------------------------------------------------- The VXLAN tunnel is built between the vxlan interfaces in the MAC-VRF network instances, which internally use system interfaces of the default network instance as a VTEP: Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index 3 , an entry in the tunnel-table is also created for the tunnel. A:leaf1# /show network-instance default tunnel-table all ------------------------------------------------------------------------------------------------------- Show report for network instance \"default\" tunnel table ------------------------------------------------------------------------------------------------------- +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ | IPv4 Prefix | Owner | Type | Index | Metric | Preference | Fib-prog | Last Update | +=============+===========+=======+=======+========+============+==========+==========================+ | 10.0.0.2/32 | vxlan_mgr | vxlan | 1 | 0 | 0 | Y | 2021-07-13T21:13:43.424Z | +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ ------------------------------------------------------------------------------------------------------- 1 VXLAN tunnels, 1 active, 0 inactive EVPN MAC/IP routes # As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations ------------------------------------------------------------------------------- Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Ethernet Segment Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Summary 0 unicast-destinations, 0 non-es, 0 es 0 MAC addresses, 0 active, 0 non-active This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the vrf-1 , we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs 7 . A:leaf1# show network-instance vrf-1 bridge-table mac-table all ------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ------------------------------------------------------------------------------- Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 0 Total 0 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 0 Total 0 Active ------------------------------------------------------------------------------- Let's try that ping from srv1 towards srv2 once again and see what happens: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. 64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms 64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms 64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2013ms rtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms Much better! The dataplane works and we can check that the MAC table in the vrf-1 network-instance has been populated with local and EVPN-learned MACs: A:leaf1# show network-instance vrf-1 bridge-table mac-table all --------------------------------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 --------------------------------------------------------------------------------------------------------------------------------------------- +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+====================================+===========+===========+========+=======+====================================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 240 | 2021-07-18T14:22:55.000Z | | 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1 | 160078821 | evpn | true | N/A | 2021-07-18T14:22:56.000Z | | | vtep:10.0.0.2 vni:1 | 962 | | | | | +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 1 Total 1 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 2 Total 2 Active --------------------------------------------------------------------------------------------------------------------------------------------- When traffic is exchanged between srv1 and srv2 , the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2) . The MAC/IP routes are imported, and the MACs programmed in the mac-table. The below output shows the MAC/IP EVPN route that leaf1 received from its neighbor. The NLRI information contains the MAC of the srv2 : A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Type 2 MAC-IP Advertisement Routes +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ | Statu | Route- | Tag-ID | MAC-address | IP-address | neighbor | Next-Hop | VNI | ESI | MAC Mobility | | s | distinguisher | | | | | | | | | +=======+================+===========+==================+================+================+================+================+===============================+================+ | u*> | 10.0.0.2:111 | 0 | 00:C1:AB:00:00:0 | 0.0.0.0 | 10.0.0.2 | 10.0.0.2 | 1 | 00:00:00:00:00:00:00:00:00:00 | - | | | | | 2 | | | | | | | +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 MAC-IP Advertisement routes 1 used, 1 valid ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * --------------------------------------------------------------------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations --------------------------------------------------------------------------------------------------------------------------------------------- Destinations --------------------------------------------------------------------------------------------------------------------------------------------- +--------------+------------+-------------------+-----------------------------+ | VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) | +==============+============+===================+=============================+ | 10.0.0.2 | 1 | 160078821962 | 1(1/0) | +--------------+------------+-------------------+-----------------------------+ --------------------------------------------------------------------------------------------------------------------------------------------- Ethernet Segment Destinations --------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------- Summary 1 unicast-destinations, 1 non-es, 0 es 1 MAC addresses, 1 active, 0 non-active ------------------------------------------------------------------------------- packet capture The following pcap was captured a moment before srv1 started to ping srv2 on leaf1 interface e1-49 . It shows how: ARP frames were first exchanged using the multicast destination, next the first ICMP request was sent out by leaf1 again using the BUM destination, since RT2 routes were not received yet and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane. after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes. This concludes the verification steps, as we have a working data plane connectivity between the servers. as was verified before \u21a9 containerlab assigns mac addresses to the interfaces with OUI 00:C1:AB . We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables. \u21a9 If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created. \u21a9 IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference. \u21a9 Per section 5.1.2 of RFC 8365 \u21a9 Easily extracted with doing info <container> where container is routing-policy , network-instance * , interface * , tunnel-interface * \u21a9 We did try to ping from srv1 to srv2 in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again. \u21a9","title":"EVPN configuration"},{"location":"tutorials/l2evpn/evpn/#overview","text":"EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch. VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently. The following figure demonstrates this concept where servers srv1 and srv2 are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain. Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable 1 , we can proceed with the VXLAN based EVPN service configuration. While doing that we will cover the following topics: VXLAN tunnel interface configuration Network instances of type mac-vrf Bridged subinterfaces and BGP EVPN control plane configuration","title":"Overview"},{"location":"tutorials/l2evpn/evpn/#ibgp-for-evpn","text":"Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches. For that iBGP configuration we will create a group called iBGP-overlay which will have the peer-as and local-as set to 100 to form an iBGP neighborship. The group will also host the same permissive all routing policy, enabled evpn and disabled ipv4-unicast address families. Then for each leaf we add a new BGP neighbor addressed by the remote system0 interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config: leaf1 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { peer-group iBGP-overlay transport { local-address 10.0.0.1 } } commit now leaf2 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { peer-group iBGP-overlay transport { local-address 10.0.0.2 } } commit now Ensure that the iBGP session is established before proceeding any further: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [0/0/0] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above.","title":"IBGP for EVPN"},{"location":"tutorials/l2evpn/evpn/#access-interfaces","text":"Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches: Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be bridged , this makes the interfaces only attachable to a network instance of mac-vrf type with MAC learning and layer-2 forwarding enabled. The following config is applied to both leaf switches: enter candidate /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } commit now As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers.","title":"Access interfaces"},{"location":"tutorials/l2evpn/evpn/#tunnelvxlan-interface","text":"After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric. The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name vxlan<N> where N = 0..255 . A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI). The index can be a number in the range 0-4294967295. The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed). The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of 1..16777215 . The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes. SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets. Note The source IP used in the vxlan-interfaces is the IPv4 address of subinterface system0.0 in the default network-instance. The above information translates to a configuration snippet which is applicable both to leaf1 and leaf2 nodes. enter candidate /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } commit now To verify the tunnel interface configuration: A:leaf2# show tunnel-interface vxlan-interface brief --------------------------------------------------------------------------------- Show report for vxlan-tunnels --------------------------------------------------------------------------------- +------------------+-----------------+---------+-------------+------------------+ | Tunnel Interface | VxLAN Interface | Type | Ingress VNI | Egress source-ip | +==================+=================+=========+=============+==================+ | vxlan1 | vxlan1.1 | bridged | 1 | 10.0.0.2/32 | +------------------+-----------------+---------+-------------+------------------+ --------------------------------------------------------------------------------- Summary 1 tunnel-interfaces, 1 vxlan interfaces 0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip ---------------------------------------------------------------------------------","title":"Tunnel/VXLAN interface"},{"location":"tutorials/l2evpn/evpn/#mac-vrf","text":"Now it is a turn of MAC-VRF to get configured. The network-instance type mac-vrf functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration. By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance: enter candidate /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } } commit now","title":"MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#server-interfaces","text":"The servers in our fabric do not have any addresses on their eth1 interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration. By the end of this section, we will have the following addressing scheme complete: To connect to a shell of a server execute docker exec -it <container-name> bash : srv1 docker exec -it clab-evpn01-srv1 bash srv2 docker exec -it clab-evpn01-srv2 bash Within the shell, configure MAC address 2 and IPv4 address for the eth1 interface according to the diagram above, as with this interface the server is connected to the leaf switch. srv1 ip link set address 00:c1:ab:00:00:01 dev eth1 ip addr add 192.168.0.1/24 dev eth1 srv2 ip link set address 00:c1:ab:00:00:02 dev eth1 ip addr add 192.168.0.2/24 dev eth1 Let's try to ping server2 from server1: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 2028ms That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the srv1 interface MAC to get learned by the leaf1 mac-vrf network instance: A:leaf1# show network-instance vrf-1 bridge-table mac-table all ---------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ---------------------------------------------------------------------------------------------------------------------- +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+==========================+===========+========+========+=======+==========================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 242 | 2021-07-13T17:36:23.000Z | +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 1 Total 1 Active ----------------------------------------------------------------------------------------------------------------------","title":"Server interfaces"},{"location":"tutorials/l2evpn/evpn/#evpn-in-mac-vrf","text":"To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our vrf-1 network-instance. EVPN configuration under the mac-vrf network instance will require two configuration containers: bgp-vpn - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist. bgp-evpn - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to bgp-instance 1 is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI). The following configuration is entered on both leafs: enter candidate /network-instance vrf-1 protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } commit now Once configured, the bgp-vpn instance can be checked to have the RT/RD values set: A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1 ===================================================================== Net Instance : vrf-1 bgp Instance 1 --------------------------------------------------------------------- route-distinguisher: 10.0.0.1:111, auto-derived-from-evi export-route-target: target:100:111, manual import-route-target: target:100:111, manual ===================================================================== VNI to EVI mapping As of release 21.6, SR Linux uses only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI. 5","title":"EVPN in MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#final-configurations","text":"For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration 6 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks. pastable snippets leaf1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.1 } } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } /interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } commit now leaf2 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.2 } } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } commit now spine1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /network-instance default { interface ethernet-1/1.0 { } interface ethernet-1/2.0 { } interface system0.0 { } protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-as 101 peer-group eBGP-underlay } neighbor 192.168.12.1 { peer-as 102 peer-group eBGP-underlay } } } } /interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } commit now srv1 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv1 bash ip link set address 00 :c1:ab:00:00:01 dev eth1 ip addr add 192 .168.0.1/24 dev eth1 srv2 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv2 bash ip link set address 00 :c1:ab:00:00:02 dev eth1 ip addr add 192 .168.0.2/24 dev eth1","title":"Final configurations"},{"location":"tutorials/l2evpn/evpn/#verification","text":"","title":"Verification"},{"location":"tutorials/l2evpn/evpn/#evpn-imet-routes","text":"When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [1/1/1] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3). The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose: Auto-discovery of the remote VTEPs attached to the same EVI Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated The IMET/RT3 routes can be viewed in summary and detailed modes: RT3 summary A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary ---------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------- Type 3 Inclusive Multicast Ethernet Tag Routes +--------+---------------------+------------+---------------------+---------------------+---------------------+ | Status | Route-distinguisher | Tag-ID | Originator-IP | neighbor | Next-Hop | +========+=====================+============+=====================+=====================+=====================+ | u*> | 10.0.0.2:111 | 0 | 10.0.0.2 | 10.0.0.2 | 10.0.0.2 | +--------+---------------------+------------+---------------------+---------------------+---------------------+ ---------------------------------------------------------------------------------------------------------------- 1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid ---------------------------------------------------------------------------------------------------------------- RT3 detailed A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail ------------------------------------------------------------------------------------- Show report for the EVPN routes in network-instance \"default\" ------------------------------------------------------------------------------------- Route Distinguisher: 10.0.0.2:111 Tag-ID : 0 Originating router : 10.0.0.2 neighbor : 10.0.0.2 Received paths : 1 Path 1: <Best,Valid,Used,> VNI : 1 Route source : neighbor 10.0.0.2 (last modified 2m3s ago) Route preference: No MED, LocalPref is 100 Atomic Aggr : false BGP next-hop : 10.0.0.2 AS Path : i Communities : [target:100:111, bgp-tunnel-encap:VXLAN] RR Attributes : No Originator-ID, Cluster-List is [] Aggregation : None Unknown Attr : None Invalid Reason : None Tie Break Reason: none -------------------------------------------------------------------------------------- Lets capture those routes? Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers . By capturing on the e1-49 interface of the clab-evpn01-leaf1 container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the vrf-1 network instance to trigger the BGP updates to flow and see them in the live capture. Here is the pcap file with the IMET routes advertisements between leaf1 and leaf2 . When the IMET routes from leaf2 are imported for vrf-1 network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 multicast destinations (flooding-list) ------------------------------------------------------------------------------- +--------------+------------+-------------------+----------------------+ | VTEP Address | Egress VNI | Destination-index | Multicast-forwarding | +==============+============+===================+======================+ | 10.0.0.2 | 1 | 160078821962 | BUM | +--------------+------------+-------------------+----------------------+ ------------------------------------------------------------------------------- Summary 1 multicast-destinations ------------------------------------------------------------------------------- This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table. As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes.","title":"EVPN IMET routes"},{"location":"tutorials/l2evpn/evpn/#vxlan-tunnels","text":"After receiving EVPN routes from the remote leafs with VXLAN encapsulation 4 , SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the leaf1 switch. A:leaf1# /show tunnel vxlan-tunnel all ---------------------------------------------------------- Show report for vxlan-tunnels ---------------------------------------------------------- +--------------+--------------+--------------------------+ | VTEP Address | Index | Last Change | +==============+==============+==========================+ | 10.0.0.2 | 160078821947 | 2021-07-13T21:13:50.000Z | +--------------+--------------+--------------------------+ 1 VXLAN tunnels, 1 active, 0 inactive ---------------------------------------------------------- The VXLAN tunnel is built between the vxlan interfaces in the MAC-VRF network instances, which internally use system interfaces of the default network instance as a VTEP: Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index 3 , an entry in the tunnel-table is also created for the tunnel. A:leaf1# /show network-instance default tunnel-table all ------------------------------------------------------------------------------------------------------- Show report for network instance \"default\" tunnel table ------------------------------------------------------------------------------------------------------- +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ | IPv4 Prefix | Owner | Type | Index | Metric | Preference | Fib-prog | Last Update | +=============+===========+=======+=======+========+============+==========+==========================+ | 10.0.0.2/32 | vxlan_mgr | vxlan | 1 | 0 | 0 | Y | 2021-07-13T21:13:43.424Z | +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ ------------------------------------------------------------------------------------------------------- 1 VXLAN tunnels, 1 active, 0 inactive","title":"VXLAN tunnels"},{"location":"tutorials/l2evpn/evpn/#evpn-macip-routes","text":"As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations ------------------------------------------------------------------------------- Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Ethernet Segment Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Summary 0 unicast-destinations, 0 non-es, 0 es 0 MAC addresses, 0 active, 0 non-active This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the vrf-1 , we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs 7 . A:leaf1# show network-instance vrf-1 bridge-table mac-table all ------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ------------------------------------------------------------------------------- Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 0 Total 0 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 0 Total 0 Active ------------------------------------------------------------------------------- Let's try that ping from srv1 towards srv2 once again and see what happens: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. 64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms 64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms 64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2013ms rtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms Much better! The dataplane works and we can check that the MAC table in the vrf-1 network-instance has been populated with local and EVPN-learned MACs: A:leaf1# show network-instance vrf-1 bridge-table mac-table all --------------------------------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 --------------------------------------------------------------------------------------------------------------------------------------------- +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+====================================+===========+===========+========+=======+====================================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 240 | 2021-07-18T14:22:55.000Z | | 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1 | 160078821 | evpn | true | N/A | 2021-07-18T14:22:56.000Z | | | vtep:10.0.0.2 vni:1 | 962 | | | | | +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 1 Total 1 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 2 Total 2 Active --------------------------------------------------------------------------------------------------------------------------------------------- When traffic is exchanged between srv1 and srv2 , the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2) . The MAC/IP routes are imported, and the MACs programmed in the mac-table. The below output shows the MAC/IP EVPN route that leaf1 received from its neighbor. The NLRI information contains the MAC of the srv2 : A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Type 2 MAC-IP Advertisement Routes +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ | Statu | Route- | Tag-ID | MAC-address | IP-address | neighbor | Next-Hop | VNI | ESI | MAC Mobility | | s | distinguisher | | | | | | | | | +=======+================+===========+==================+================+================+================+================+===============================+================+ | u*> | 10.0.0.2:111 | 0 | 00:C1:AB:00:00:0 | 0.0.0.0 | 10.0.0.2 | 10.0.0.2 | 1 | 00:00:00:00:00:00:00:00:00:00 | - | | | | | 2 | | | | | | | +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 MAC-IP Advertisement routes 1 used, 1 valid ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * --------------------------------------------------------------------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations --------------------------------------------------------------------------------------------------------------------------------------------- Destinations --------------------------------------------------------------------------------------------------------------------------------------------- +--------------+------------+-------------------+-----------------------------+ | VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) | +==============+============+===================+=============================+ | 10.0.0.2 | 1 | 160078821962 | 1(1/0) | +--------------+------------+-------------------+-----------------------------+ --------------------------------------------------------------------------------------------------------------------------------------------- Ethernet Segment Destinations --------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------- Summary 1 unicast-destinations, 1 non-es, 0 es 1 MAC addresses, 1 active, 0 non-active ------------------------------------------------------------------------------- packet capture The following pcap was captured a moment before srv1 started to ping srv2 on leaf1 interface e1-49 . It shows how: ARP frames were first exchanged using the multicast destination, next the first ICMP request was sent out by leaf1 again using the BUM destination, since RT2 routes were not received yet and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane. after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes. This concludes the verification steps, as we have a working data plane connectivity between the servers. as was verified before \u21a9 containerlab assigns mac addresses to the interfaces with OUI 00:C1:AB . We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables. \u21a9 If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created. \u21a9 IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference. \u21a9 Per section 5.1.2 of RFC 8365 \u21a9 Easily extracted with doing info <container> where container is routing-policy , network-instance * , interface * , tunnel-interface * \u21a9 We did try to ping from srv1 to srv2 in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again. \u21a9","title":"EVPN MAC/IP routes"},{"location":"tutorials/l2evpn/fabric/","text":"Prior to configuring EVPN based overlay, a routing protocol needs to be deployed in the fabric to advertise the reachability of all the leaf VXLAN Termination End Point (VTEP) addresses throughout the IP fabric. With SR Linux, the following routing protocols can be used in the underlay: ISIS OSPF EBGP We will use a BGP based fabric design as described in RFC7938 due to its simplicity, scalability, and ease of multi-vendor interoperability. Leaf-Spine interfaces # Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the 192.168.xx.0/30 network to address the links, we will implement the following underlay addressing design: On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity. We begin with connecting to the CLI of our nodes via SSH 1 : # connecting to leaf1 ssh admin@clab-evpn01-leaf1 Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration. Let's witness the step by step process of an interface configuration on a leaf1 switch with providing the paste-ables snippets for the rest of the nodes Enter the candidate configuration mode to make edits to the configuration Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# enter candidate The prompt will indicate the changed active mode --{ candidate shared default }--[ ]-- A:leaf1# Enter into the interface configuration context --{ candidate shared default }--[ ]-- A:leaf1# interface ethernet-1/49 Create a subinterface under the parent interface to configure IPv4 address on it --{ * candidate shared default }--[ interface ethernet-1/49 ]-- A:leaf1# subinterface 0 --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]-- A:leaf1# ipv4 address 192.168.11.1/30 Apply the configuration changes by issuing a commit now command. The changes will be written to the running configuration. --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. Below you will find the relevant configuration snippets 2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode. leaf1 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } leaf2 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } spine1 interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } Once those snippets are committed to the running configuration with commit now command, we can ensure that the changes have been applied by showing the interface status: --{ + running }--[ ]-- A:spine1# show interface ethernet-1/1 ==================================================== ethernet-1/1 is up, speed 10G, type None ethernet-1/1.0 is up Network-instance: Encapsulation : null Type : routed IPv4 addr : 192.168.11.2/30 (static, None) ---------------------------------------------------- ==================================================== At this moment, the configured interfaces can not be used as they are not yet associated with any network instance . Below we are placing the interfaces to the network-instance default that is created automatically by SR Linux. leaf1 & leaf2 --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default interface ethernet-1/49.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. spine1 --{ + candidate shared default }--[ ]-- A:spine1# network-instance default interface ethernet-1/1.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]-- A:spine1# /network-instance default interface ethernet-1/2.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]-- A:spine2# commit now All changes have been committed. Leaving candidate mode. When interfaces are owned by the network-instance default , we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from spine1 to leaf2 : --{ + running }--[ ]-- A:spine1# ping 192.168.12.1 network-instance default Using network instance default PING 192.168.12.1 (192.168.12.1) 56(84) bytes of data. 64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms 64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms 64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms 64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms ^C --- 192.168.12.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3003ms rtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms EBGP # Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol. The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering. Let's turn this diagram with the ASN/Router ID allocation into a working configuration: Here is a breakdown of the steps that are needed to configure EBGP on leaf1 towards spine1 : Add BGP protocol to network-instance Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default protocols bgp Assign Autonomous System Number The ASN is reported to peers when BGP speaker opens a session towards another router. According to the diagram above, leaf1 has ASN 101. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# autonomous-system 101 Assign Router ID This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router. Leaf1 has a router-id of 10.0.0.1. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# router-id 10.0.0.1 Enable AF Enable all address families that should be enabled globally as a default for all peers of the BGP instance. When you later configure individual neighbors or groups, you can override the enabled families at those levels. For the sake of IPv4 loopbacks advertisement, we only need to enable ipv4-unicast address family: --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# ipv4-unicast admin-state enable Create export/import policies The export/import policy is required for an EBGP peer to advertise and install routes. The policy named all that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received 4 . The routing policies are configured at /routing-policy context, so first, we switch to it from the current bgp context: --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# /routing-policy --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# Now that we are in the right context, we can paste the policy definition: --{ +* candidate shared default }--[ routing-policy ]-- A:leaf1# info policy all { default-action { accept { } } } Create peer-group config A peer group should include sessions that have a similar or almost identical configuration. In this example, the peer group is named eBGP-underlay since it will be used to enable underlay routing between the leafs and spines. New groups are administratively enabled by default. First, we come back to the bgp context from the routing-policy context: --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# /network-instance default protocols bgp --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# Now create the peer group. The common group configuration includes the peer-as and export-policy statements. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# group eBGP-underlay --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# peer-as 201 --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# export-policy all --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# import-policy all Configure neighbor Configure the BGP session with spine1 . In this example, spine1 is reachable through the ethernet-1/49.0 subinterface. On this subnet, spine1 has the IPv4 address 192.168.11.2 . In this minimal configuration example, the only required configuration for the neighbor is its association with the group eBGP-underlay that was previously created. New neighbors are administratively enabled by default. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay Commit configuration It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far. Regardless of which context you are currently in, you can see the diff against the baseline config by doing diff / --{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# diff / network-instance default { protocols { + bgp { + autonomous-system 101 + router-id 10.0.0.1 + group eBGP-underlay { + export-policy all + import-policy all + peer-as 201 + } + ipv4-unicast { + admin-state enable + } + neighbor 192.168.11.2 { + peer-group eBGP-underlay + } + } } } + routing-policy { + policy all { + default-action { + accept { + } + } + } + } That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# commit now EBGP configuration on leaf2 and spine1 is almost a twin of the one we did for leaf1 . Here is a copy-paste-able 3 config snippets for all of the nodes: leaf1 network-instance default { protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } } leaf2 network-instance default { protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } } spine1 Spine configuration is a bit different, in a way that peer-as is specified under the neighbor context, and not the group one. network-instance default { protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-group eBGP-underlay peer-as 101 } neighbor 192.168.12.1 { peer-group eBGP-underlay peer-as 102 } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } } Loopbacks # As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces. In the context of the VXLAN data plane, a special kind of a loopback needs to be created - system0 interface. Info The system0.0 interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes. Configuration of the system0 interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to system0 interfaces will match the Router-ID of a given BGP speaker. leaf1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } /network-instance default { interface system0.0 { } } leaf2 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } /network-instance default { interface system0.0 { } } spine1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } /network-instance default { interface system0.0 { } } Verification # As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The system0 interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that. BGP status # The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on leaf1 : --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp summary ------------------------------------------------------------- BGP is enabled and up in network-instance \"default\" Global AS number : 101 BGP identifier : 10.0.0.1 ------------------------------------------------------------- Total paths : 3 Received routes : 3 Received and active routes: None Total UP peers : 1 Configured peers : 1, 0 are disabled Dynamic peers : None ------------------------------------------------------------- Default preferences BGP Local Preference attribute: 100 EBGP route-table preference : 170 IBGP route-table preference : 170 ------------------------------------------------------------- Wait for FIB install to advertise: True Send rapid withdrawals : disabled ------------------------------------------------------------- Ipv4-unicast AFI/SAFI Received routes : 3 Received and active routes : None Max number of multipaths : 1, 1 Multipath can transit multi AS: True ------------------------------------------------------------- Ipv6-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : 1,1 Multipath can transit multi AS: True ------------------------------------------------------------- EVPN-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : N/A Multipath can transit multi AS: N/A ------------------------------------------------------------- BGP neighbor status # Equally important is the neighbor summary status that we can observe with the following: --{ + running }--[ ]-- A:spine1# show network-instance default protocols bgp neighbor ---------------------------------------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------- +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ | Net-Inst | Peer | Group | Flag | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Active/Tx] | | | | | s | | | | | | +================+=======================+================+======+=========+=============+=============+===========+=======================+ | default | 192.168.11.1 | eBGP-underlay | S | 101 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 49s | ast | | | default | 192.168.12.1 | eBGP-underlay | S | 102 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 9s | ast | | +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ ---------------------------------------------------------------------------------------------------------------------------------------------- Summary: 2 configured neighbors, 2 configured sessions are established,0 disabled peers 0 dynamic peers With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state. Received/Advertised routes # The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - system0 interfaces. In the below output we verify that leaf1 advertises the prefix of system0 ( 10.0.0.1/32 ) interface towards its EBGP spine1 peer: --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou tes ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.11.2, remote AS: 201, local AS: 101 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Origin codes: i=IGP, e=EGP, ?=incomplete +-------------------------------------------------------------------------------------+ | Network Next Hop MED LocPref AsPath Origin | +=====================================================================================+ | 10.0.0.1/32 192.168.11. - 100 [101] i | | 1 | | 192.168.11.0/3 192.168.11. - 100 [101] i | | 0 1 | +-------------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 2 advertised BGP routes ----------------------------------------------------------------------------------------- On the far end of the fabric, leaf2 receives both the leaf1 and spine1 system interface prefixes: --{ + running }--[ ]-- A:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route s ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.12.2, remote AS: 201, local AS: 102 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete +-----------------------------------------------------------------------------------+ | Status Network Next Hop MED LocPref AsPath Origin | +===================================================================================+ | u*> 10.0.0.1/ 192.168.1 - 100 [201, i | | 32 2.2 101] | | u*> 10.0.1.1/ 192.168.1 - 100 [201] i | | 32 2.2 | | u*> 192.168.1 192.168.1 - 100 [201] i | | 1.0/30 2.2 | | * 192.168.1 192.168.1 - 100 [201] i | | 2.0/30 2.2 | +-----------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 4 received BGP routes : 3 used 4 valid ----------------------------------------------------------------------------------------- Route table # The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the default network-instance where we expect them to be: --{ running }--[ ]-- A:leaf1# show network-instance default route-table ipv4-unicast summary ----------------------------------------------------------------------------------------------------------------------------------- IPv4 Unicast route table of network instance default ----------------------------------------------------------------------------------------------------------------------------------- +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ | Prefix | ID | Route Type | Route Owner | Best/Fib- | Metric | Pref | Next-hop | Next-hop | | | | | | status(slot) | | | (Type) | Interface | +=================+=======+============+======================+======================+==========+=========+===========+===========+ | 10.0.0.1/32 | 3 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 10.0.0.2/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 10.0.1.1/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 192.168.11.0/30 | 1 | local | net_inst_mgr | True/success | 0 | 0 | 192.168.1 | ethernet- | | | | | | | | | 1.1 | 1/49.0 | | | | | | | | | (direct) | | | 192.168.11.1/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 192.168.11.3/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None (bro | None | | | | | | | | | adcast) | | | 192.168.12.0/30 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ ----------------------------------------------------------------------------------------------------------------------------------- 7 IPv4 routes total 7 IPv4 prefixes with active routes 0 IPv4 prefixes with active ECMP routes ----------------------------------------------------------------------------------------------------------------------------------- Both leaf2 and spine1 prefixes are found in the route table of network-instance default and the bgp_mgr is the owner of those prefixes, which means that they have been added to the route-table by the BGP app. Dataplane # To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay. For that we will use the ping command with src/dst set to loopback addresses: --{ running }--[ ]-- A:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2 Using network instance default PING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration! Resulting configs # Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with info flat command. Note enter candidate and commit now commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config. leaf1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 101 set / network-instance default protocols bgp router-id 10.0.0.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.2 set / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay commit now leaf2 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.2/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 102 set / network-instance default protocols bgp router-id 10.0.0.2 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.12.2 set / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay commit now spine1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/1 set / interface ethernet-1/1 subinterface 0 set / interface ethernet-1/1 subinterface 0 ipv4 set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30 set / interface ethernet-1/2 set / interface ethernet-1/2 subinterface 0 set / interface ethernet-1/2 subinterface 0 ipv4 set / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.1.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/1.0 set / network-instance default interface ethernet-1/2.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 201 set / network-instance default protocols bgp router-id 10.0.1.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.1 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay set / network-instance default protocols bgp neighbor 192.168.12.1 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay commit now default SR Linux credentials are admin:admin . \u21a9 the snippets were extracted with info interface ethernet-1/x command issued in running mode. \u21a9 you can paste those snippets right after you do enter candidate \u21a9 a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines. \u21a9","title":"Fabric configuration"},{"location":"tutorials/l2evpn/fabric/#leaf-spine-interfaces","text":"Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the 192.168.xx.0/30 network to address the links, we will implement the following underlay addressing design: On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity. We begin with connecting to the CLI of our nodes via SSH 1 : # connecting to leaf1 ssh admin@clab-evpn01-leaf1 Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration. Let's witness the step by step process of an interface configuration on a leaf1 switch with providing the paste-ables snippets for the rest of the nodes Enter the candidate configuration mode to make edits to the configuration Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# enter candidate The prompt will indicate the changed active mode --{ candidate shared default }--[ ]-- A:leaf1# Enter into the interface configuration context --{ candidate shared default }--[ ]-- A:leaf1# interface ethernet-1/49 Create a subinterface under the parent interface to configure IPv4 address on it --{ * candidate shared default }--[ interface ethernet-1/49 ]-- A:leaf1# subinterface 0 --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]-- A:leaf1# ipv4 address 192.168.11.1/30 Apply the configuration changes by issuing a commit now command. The changes will be written to the running configuration. --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. Below you will find the relevant configuration snippets 2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode. leaf1 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } leaf2 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } spine1 interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } Once those snippets are committed to the running configuration with commit now command, we can ensure that the changes have been applied by showing the interface status: --{ + running }--[ ]-- A:spine1# show interface ethernet-1/1 ==================================================== ethernet-1/1 is up, speed 10G, type None ethernet-1/1.0 is up Network-instance: Encapsulation : null Type : routed IPv4 addr : 192.168.11.2/30 (static, None) ---------------------------------------------------- ==================================================== At this moment, the configured interfaces can not be used as they are not yet associated with any network instance . Below we are placing the interfaces to the network-instance default that is created automatically by SR Linux. leaf1 & leaf2 --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default interface ethernet-1/49.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. spine1 --{ + candidate shared default }--[ ]-- A:spine1# network-instance default interface ethernet-1/1.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]-- A:spine1# /network-instance default interface ethernet-1/2.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]-- A:spine2# commit now All changes have been committed. Leaving candidate mode. When interfaces are owned by the network-instance default , we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from spine1 to leaf2 : --{ + running }--[ ]-- A:spine1# ping 192.168.12.1 network-instance default Using network instance default PING 192.168.12.1 (192.168.12.1) 56(84) bytes of data. 64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms 64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms 64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms 64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms ^C --- 192.168.12.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3003ms rtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms","title":"Leaf-Spine interfaces"},{"location":"tutorials/l2evpn/fabric/#ebgp","text":"Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol. The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering. Let's turn this diagram with the ASN/Router ID allocation into a working configuration: Here is a breakdown of the steps that are needed to configure EBGP on leaf1 towards spine1 : Add BGP protocol to network-instance Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default protocols bgp Assign Autonomous System Number The ASN is reported to peers when BGP speaker opens a session towards another router. According to the diagram above, leaf1 has ASN 101. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# autonomous-system 101 Assign Router ID This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router. Leaf1 has a router-id of 10.0.0.1. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# router-id 10.0.0.1 Enable AF Enable all address families that should be enabled globally as a default for all peers of the BGP instance. When you later configure individual neighbors or groups, you can override the enabled families at those levels. For the sake of IPv4 loopbacks advertisement, we only need to enable ipv4-unicast address family: --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# ipv4-unicast admin-state enable Create export/import policies The export/import policy is required for an EBGP peer to advertise and install routes. The policy named all that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received 4 . The routing policies are configured at /routing-policy context, so first, we switch to it from the current bgp context: --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# /routing-policy --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# Now that we are in the right context, we can paste the policy definition: --{ +* candidate shared default }--[ routing-policy ]-- A:leaf1# info policy all { default-action { accept { } } } Create peer-group config A peer group should include sessions that have a similar or almost identical configuration. In this example, the peer group is named eBGP-underlay since it will be used to enable underlay routing between the leafs and spines. New groups are administratively enabled by default. First, we come back to the bgp context from the routing-policy context: --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# /network-instance default protocols bgp --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# Now create the peer group. The common group configuration includes the peer-as and export-policy statements. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# group eBGP-underlay --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# peer-as 201 --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# export-policy all --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# import-policy all Configure neighbor Configure the BGP session with spine1 . In this example, spine1 is reachable through the ethernet-1/49.0 subinterface. On this subnet, spine1 has the IPv4 address 192.168.11.2 . In this minimal configuration example, the only required configuration for the neighbor is its association with the group eBGP-underlay that was previously created. New neighbors are administratively enabled by default. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay Commit configuration It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far. Regardless of which context you are currently in, you can see the diff against the baseline config by doing diff / --{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# diff / network-instance default { protocols { + bgp { + autonomous-system 101 + router-id 10.0.0.1 + group eBGP-underlay { + export-policy all + import-policy all + peer-as 201 + } + ipv4-unicast { + admin-state enable + } + neighbor 192.168.11.2 { + peer-group eBGP-underlay + } + } } } + routing-policy { + policy all { + default-action { + accept { + } + } + } + } That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# commit now EBGP configuration on leaf2 and spine1 is almost a twin of the one we did for leaf1 . Here is a copy-paste-able 3 config snippets for all of the nodes: leaf1 network-instance default { protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } } leaf2 network-instance default { protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } } spine1 Spine configuration is a bit different, in a way that peer-as is specified under the neighbor context, and not the group one. network-instance default { protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-group eBGP-underlay peer-as 101 } neighbor 192.168.12.1 { peer-group eBGP-underlay peer-as 102 } } } } routing-policy { policy pass-all-bgp { default-action { accept { } } } }","title":"EBGP"},{"location":"tutorials/l2evpn/fabric/#loopbacks","text":"As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces. In the context of the VXLAN data plane, a special kind of a loopback needs to be created - system0 interface. Info The system0.0 interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes. Configuration of the system0 interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to system0 interfaces will match the Router-ID of a given BGP speaker. leaf1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } /network-instance default { interface system0.0 { } } leaf2 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } /network-instance default { interface system0.0 { } } spine1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } /network-instance default { interface system0.0 { } }","title":"Loopbacks"},{"location":"tutorials/l2evpn/fabric/#verification","text":"As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The system0 interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that.","title":"Verification"},{"location":"tutorials/l2evpn/fabric/#bgp-status","text":"The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on leaf1 : --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp summary ------------------------------------------------------------- BGP is enabled and up in network-instance \"default\" Global AS number : 101 BGP identifier : 10.0.0.1 ------------------------------------------------------------- Total paths : 3 Received routes : 3 Received and active routes: None Total UP peers : 1 Configured peers : 1, 0 are disabled Dynamic peers : None ------------------------------------------------------------- Default preferences BGP Local Preference attribute: 100 EBGP route-table preference : 170 IBGP route-table preference : 170 ------------------------------------------------------------- Wait for FIB install to advertise: True Send rapid withdrawals : disabled ------------------------------------------------------------- Ipv4-unicast AFI/SAFI Received routes : 3 Received and active routes : None Max number of multipaths : 1, 1 Multipath can transit multi AS: True ------------------------------------------------------------- Ipv6-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : 1,1 Multipath can transit multi AS: True ------------------------------------------------------------- EVPN-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : N/A Multipath can transit multi AS: N/A -------------------------------------------------------------","title":"BGP status"},{"location":"tutorials/l2evpn/fabric/#bgp-neighbor-status","text":"Equally important is the neighbor summary status that we can observe with the following: --{ + running }--[ ]-- A:spine1# show network-instance default protocols bgp neighbor ---------------------------------------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------- +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ | Net-Inst | Peer | Group | Flag | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Active/Tx] | | | | | s | | | | | | +================+=======================+================+======+=========+=============+=============+===========+=======================+ | default | 192.168.11.1 | eBGP-underlay | S | 101 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 49s | ast | | | default | 192.168.12.1 | eBGP-underlay | S | 102 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 9s | ast | | +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ ---------------------------------------------------------------------------------------------------------------------------------------------- Summary: 2 configured neighbors, 2 configured sessions are established,0 disabled peers 0 dynamic peers With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state.","title":"BGP neighbor status"},{"location":"tutorials/l2evpn/fabric/#receivedadvertised-routes","text":"The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - system0 interfaces. In the below output we verify that leaf1 advertises the prefix of system0 ( 10.0.0.1/32 ) interface towards its EBGP spine1 peer: --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou tes ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.11.2, remote AS: 201, local AS: 101 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Origin codes: i=IGP, e=EGP, ?=incomplete +-------------------------------------------------------------------------------------+ | Network Next Hop MED LocPref AsPath Origin | +=====================================================================================+ | 10.0.0.1/32 192.168.11. - 100 [101] i | | 1 | | 192.168.11.0/3 192.168.11. - 100 [101] i | | 0 1 | +-------------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 2 advertised BGP routes ----------------------------------------------------------------------------------------- On the far end of the fabric, leaf2 receives both the leaf1 and spine1 system interface prefixes: --{ + running }--[ ]-- A:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route s ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.12.2, remote AS: 201, local AS: 102 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete +-----------------------------------------------------------------------------------+ | Status Network Next Hop MED LocPref AsPath Origin | +===================================================================================+ | u*> 10.0.0.1/ 192.168.1 - 100 [201, i | | 32 2.2 101] | | u*> 10.0.1.1/ 192.168.1 - 100 [201] i | | 32 2.2 | | u*> 192.168.1 192.168.1 - 100 [201] i | | 1.0/30 2.2 | | * 192.168.1 192.168.1 - 100 [201] i | | 2.0/30 2.2 | +-----------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 4 received BGP routes : 3 used 4 valid -----------------------------------------------------------------------------------------","title":"Received/Advertised routes"},{"location":"tutorials/l2evpn/fabric/#route-table","text":"The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the default network-instance where we expect them to be: --{ running }--[ ]-- A:leaf1# show network-instance default route-table ipv4-unicast summary ----------------------------------------------------------------------------------------------------------------------------------- IPv4 Unicast route table of network instance default ----------------------------------------------------------------------------------------------------------------------------------- +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ | Prefix | ID | Route Type | Route Owner | Best/Fib- | Metric | Pref | Next-hop | Next-hop | | | | | | status(slot) | | | (Type) | Interface | +=================+=======+============+======================+======================+==========+=========+===========+===========+ | 10.0.0.1/32 | 3 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 10.0.0.2/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 10.0.1.1/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 192.168.11.0/30 | 1 | local | net_inst_mgr | True/success | 0 | 0 | 192.168.1 | ethernet- | | | | | | | | | 1.1 | 1/49.0 | | | | | | | | | (direct) | | | 192.168.11.1/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 192.168.11.3/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None (bro | None | | | | | | | | | adcast) | | | 192.168.12.0/30 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ ----------------------------------------------------------------------------------------------------------------------------------- 7 IPv4 routes total 7 IPv4 prefixes with active routes 0 IPv4 prefixes with active ECMP routes ----------------------------------------------------------------------------------------------------------------------------------- Both leaf2 and spine1 prefixes are found in the route table of network-instance default and the bgp_mgr is the owner of those prefixes, which means that they have been added to the route-table by the BGP app.","title":"Route table"},{"location":"tutorials/l2evpn/fabric/#dataplane","text":"To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay. For that we will use the ping command with src/dst set to loopback addresses: --{ running }--[ ]-- A:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2 Using network instance default PING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration!","title":"Dataplane"},{"location":"tutorials/l2evpn/fabric/#resulting-configs","text":"Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with info flat command. Note enter candidate and commit now commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config. leaf1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 101 set / network-instance default protocols bgp router-id 10.0.0.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.2 set / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay commit now leaf2 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.2/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 102 set / network-instance default protocols bgp router-id 10.0.0.2 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.12.2 set / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay commit now spine1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/1 set / interface ethernet-1/1 subinterface 0 set / interface ethernet-1/1 subinterface 0 ipv4 set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30 set / interface ethernet-1/2 set / interface ethernet-1/2 subinterface 0 set / interface ethernet-1/2 subinterface 0 ipv4 set / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.1.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/1.0 set / network-instance default interface ethernet-1/2.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 201 set / network-instance default protocols bgp router-id 10.0.1.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.1 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay set / network-instance default protocols bgp neighbor 192.168.12.1 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay commit now default SR Linux credentials are admin:admin . \u21a9 the snippets were extracted with info interface ethernet-1/x command issued in running mode. \u21a9 you can paste those snippets right after you do enter candidate \u21a9 a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines. \u21a9","title":"Resulting configs"},{"location":"tutorials/l2evpn/intro/","text":"Tutorial name L2 EVPN-VXLAN with SR Linux Lab components 3 SR Linux nodes Resource requirements 2vCPU 4 GB Containerlab topology file evpn01.clab.yml Lab name evpn01 Packet captures EVPN IMET routes exchange , RT2 routes exchange with ICMP in datapath Main ref documents RFC 7432 - BGP MPLS-Based Ethernet VPN , RFC 8365 - A Network Virtualization Overlay Solution Using Ethernet VPN (EVPN) Version information 3 containerlab:0.15.4 , srlinux:21.6.1-250 , docker-ce:20.10.2 Ethernet Virtual Private Network (EVPN) is a standard technology in multi-tenant Data Centers (DCs) and provides a control plane framework for many functions. In this tutorial we will configure a VXLAN based Layer 2 EVPN service 5 in a tiny CLOS fabric and at the same get to know SR Linux better! The DC fabric that we will build for this tutorial consists of the two leaf switches (acting as Top-Of-Rack) and a single spine: The two servers are connected to the leafs via an L2 interface. Service-wise the servers will appear to be on the same L2 network by means of the deployed EVPN Layer 2 service. The tutorial will consist of the following major parts: Fabric configuration - here we will configure the routing protocol in the underlay of a fabric to advertise the Virtual Tunnel Endpoints (VTEP) of the leaf switches. EVPN configuration - this chapter is dedicated to the EVPN service configuration and validation. Lab deployment # To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM: The containerlab file that describes the lab topology is referenced below in full: name : evpn01 topology : kinds : srl : image : ghcr.io/nokia/srlinux linux : image : ghcr.io/hellt/network-multitool nodes : leaf1 : kind : srl type : ixrd2 leaf2 : kind : srl type : ixrd2 spine1 : kind : srl type : ixrd3 srv1 : kind : linux srv2 : kind : linux links : # inter-switch links - endpoints : [ \"leaf1:e1-49\" , \"spine1:e1-1\" ] - endpoints : [ \"leaf2:e1-49\" , \"spine1:e1-2\" ] # server links - endpoints : [ \"srv1:eth1\" , \"leaf1:e1-1\" ] - endpoints : [ \"srv2:eth1\" , \"leaf2:e1-1\" ] Save 4 the contents of this file under evpn01.clab.yml name and you are ready to deploy: $ containerlab deploy -t evpn01.clab.yml INFO[0000] Parsing & checking topology file: evpn01.clab.yml INFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 INFO[0000] Creating root CA INFO[0001] Creating container: srv2 INFO[0001] Creating container: srv1 INFO[0001] Creating container: leaf2 INFO[0001] Creating container: spine1 INFO[0001] Creating container: leaf1 INFO[0002] Creating virtual wire: leaf1:e1-49 <--> spine1:e1-1 INFO[0002] Creating virtual wire: srv2:eth1 <--> leaf2:e1-1 INFO[0002] Creating virtual wire: leaf2:e1-49 <--> spine1:e1-2 INFO[0002] Creating virtual wire: srv1:eth1 <--> leaf1:e1-1 INFO[0003] Writing /etc/hosts file +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | 1 | clab-evpn01-leaf1 | 4b81c65af558 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.7/24 | 2001:172:20:20::7/64 | | 2 | clab-evpn01-leaf2 | de000e791dd6 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.8/24 | 2001:172:20:20::8/64 | | 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.6/24 | 2001:172:20:20::6/64 | | 4 | clab-evpn01-srv1 | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.3/24 | 2001:172:20:20::3/64 | | 5 | clab-evpn01-srv2 | fb722453d715 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.5/24 | 2001:172:20:20::5/64 | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of leaf1 : # default credentials admin:admin ssh admin@clab-evpn01-leaf1 With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey! Note We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough 2 details to survive in the configuration waters we are about to get. To ensure reproducibility and consistency of the examples provided in this quickstart, we will pin to a particular SR Linux version in the containerlab file. \u21a9 For a complete documentation coverage don't hesitate to visit our documentation portal . \u21a9 the following versions have been used to create this tutorial. The newer versions might work, but if they pin the version to the mentioned ones. \u21a9 Or download it with curl -LO https://github.com/learn-srlinux/site/blob/master/labs/evpn01.clab.yml \u21a9 Per RFC 8365 & RFC 7432 \u21a9","title":"Introduction"},{"location":"tutorials/l2evpn/intro/#lab-deployment","text":"To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM: The containerlab file that describes the lab topology is referenced below in full: name : evpn01 topology : kinds : srl : image : ghcr.io/nokia/srlinux linux : image : ghcr.io/hellt/network-multitool nodes : leaf1 : kind : srl type : ixrd2 leaf2 : kind : srl type : ixrd2 spine1 : kind : srl type : ixrd3 srv1 : kind : linux srv2 : kind : linux links : # inter-switch links - endpoints : [ \"leaf1:e1-49\" , \"spine1:e1-1\" ] - endpoints : [ \"leaf2:e1-49\" , \"spine1:e1-2\" ] # server links - endpoints : [ \"srv1:eth1\" , \"leaf1:e1-1\" ] - endpoints : [ \"srv2:eth1\" , \"leaf2:e1-1\" ] Save 4 the contents of this file under evpn01.clab.yml name and you are ready to deploy: $ containerlab deploy -t evpn01.clab.yml INFO[0000] Parsing & checking topology file: evpn01.clab.yml INFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 INFO[0000] Creating root CA INFO[0001] Creating container: srv2 INFO[0001] Creating container: srv1 INFO[0001] Creating container: leaf2 INFO[0001] Creating container: spine1 INFO[0001] Creating container: leaf1 INFO[0002] Creating virtual wire: leaf1:e1-49 <--> spine1:e1-1 INFO[0002] Creating virtual wire: srv2:eth1 <--> leaf2:e1-1 INFO[0002] Creating virtual wire: leaf2:e1-49 <--> spine1:e1-2 INFO[0002] Creating virtual wire: srv1:eth1 <--> leaf1:e1-1 INFO[0003] Writing /etc/hosts file +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | 1 | clab-evpn01-leaf1 | 4b81c65af558 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.7/24 | 2001:172:20:20::7/64 | | 2 | clab-evpn01-leaf2 | de000e791dd6 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.8/24 | 2001:172:20:20::8/64 | | 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.6/24 | 2001:172:20:20::6/64 | | 4 | clab-evpn01-srv1 | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.3/24 | 2001:172:20:20::3/64 | | 5 | clab-evpn01-srv2 | fb722453d715 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.5/24 | 2001:172:20:20::5/64 | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of leaf1 : # default credentials admin:admin ssh admin@clab-evpn01-leaf1 With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey! Note We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough 2 details to survive in the configuration waters we are about to get. To ensure reproducibility and consistency of the examples provided in this quickstart, we will pin to a particular SR Linux version in the containerlab file. \u21a9 For a complete documentation coverage don't hesitate to visit our documentation portal . \u21a9 the following versions have been used to create this tutorial. The newer versions might work, but if they pin the version to the mentioned ones. \u21a9 Or download it with curl -LO https://github.com/learn-srlinux/site/blob/master/labs/evpn01.clab.yml \u21a9 Per RFC 8365 & RFC 7432 \u21a9","title":"Lab deployment"},{"location":"tutorials/l2evpn/summary/","text":"Layer 2 EVPN services with VXLAN dataplane are very common in multi-tenant data centers. In this tutorial we walked through every step that is needed to configure a basic Layer 2 EVPN with VXLAN dataplane service deployed on SR Linux switches: IP fabric config using eBGP in the underlay EVPN service config on leaf switches with the control and data plane verification The highly detailed configuration & verification steps helped us achieve the goal of creating an overlay Layer 2 broadcast domain for the two servers in our topology. So that the high level service diagram transformed into a detailed map of configuration constructs and instances. During the verification phases we collected the following packet captures to prove the control/data plane behavior: Exchange of the IMET/RT3 EVPN routes . IMET/RT3 routes are the starting point in the L2 EVPN-VXLAN services, as they are used to dynamically discover the VXLAN VTEPs participating in the same EVI. Exchange of MAC-IP/RT2 EVPN routes which convey the MAC information of the attached servers. These routes are used to create unicast tunnel destinations that the dataplane frames will use. Info The more advanced EVPN topics listed below will be covered in separate tutorials: EVPN L2 multi-homing MAC mobility MAC duplication and loop protection","title":"Summary"}]}