{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"alwayson/","text":"Alway-ON SR Linux Instance # It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab . But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out. What is Always-ON SR Linux for? # The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user. getting to know SR Linux CLI SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier. New users can make their first steps by looking at the show commands, exploring the datastores, running info from commands and getting the grips of configuration basics by entering into the configuration mode. YANG browsing By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers. gNMI exploration The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux. gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves. Connection details # Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces: Method Details SSH address: ssh guest@on.srlinux.dev -p 44268 password: n0k1asrlinux for key-based authentication use this key to authenticate the guest user gNMI 1 gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\ capabilities JSON-RPC 2 http://http.on.srlinux.dev gNMI # SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate. Guest user # The guest user has the following settings applied to it: Read-only mode bash and file commands are disabled Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that guest can't commit them. Always-ON sandbox setup # The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node. Protocols and Services # We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise. The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services. In the overlay the following services are configured: Layer 2 EVPN with VXLAN dataplane 1 with mac-vrf-100 network instance created on SR Linux Layer 3 EVPN with VXLAN dataplane with ip-vrf-200 network instance created on SR Linux check this tutorial to understand how this service is configured \u21a9 \u21a9 HTTP service running over port 80 \u21a9","title":"Alway-ON SR Linux Instance"},{"location":"alwayson/#alway-on-sr-linux-instance","text":"It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab . But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.","title":"Alway-ON SR Linux Instance"},{"location":"alwayson/#what-is-always-on-sr-linux-for","text":"The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user. getting to know SR Linux CLI SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier. New users can make their first steps by looking at the show commands, exploring the datastores, running info from commands and getting the grips of configuration basics by entering into the configuration mode. YANG browsing By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers. gNMI exploration The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux. gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves.","title":"What is Always-ON SR Linux for?"},{"location":"alwayson/#connection-details","text":"Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces: Method Details SSH address: ssh guest@on.srlinux.dev -p 44268 password: n0k1asrlinux for key-based authentication use this key to authenticate the guest user gNMI 1 gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\ capabilities JSON-RPC 2 http://http.on.srlinux.dev","title":"Connection details"},{"location":"alwayson/#gnmi","text":"SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate.","title":"gNMI"},{"location":"alwayson/#guest-user","text":"The guest user has the following settings applied to it: Read-only mode bash and file commands are disabled Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that guest can't commit them.","title":"Guest user"},{"location":"alwayson/#always-on-sandbox-setup","text":"The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node.","title":"Always-ON sandbox setup"},{"location":"alwayson/#protocols-and-services","text":"We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise. The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services. In the overlay the following services are configured: Layer 2 EVPN with VXLAN dataplane 1 with mac-vrf-100 network instance created on SR Linux Layer 3 EVPN with VXLAN dataplane with ip-vrf-200 network instance created on SR Linux check this tutorial to understand how this service is configured \u21a9 \u21a9 HTTP service running over port 80 \u21a9","title":"Protocols and Services"},{"location":"community/","text":"Discord server # SR Linux has lots to offer to various groups of engineers... Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS. Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI. Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support. We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community 1 is the SR Linux Discord Server which everyone can join! Join SR Linux Discord Server Always-ON SR Linux # It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab . But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out . this is an unofficial community. Engineers to engineers. \u21a9","title":"Community"},{"location":"community/#discord-server","text":"SR Linux has lots to offer to various groups of engineers... Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS. Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI. Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support. We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community 1 is the SR Linux Discord Server which everyone can join! Join SR Linux Discord Server","title":"Discord server"},{"location":"community/#always-on-sr-linux","text":"It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab . But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out . this is an unofficial community. Engineers to engineers. \u21a9","title":"Always-ON SR Linux"},{"location":"get-started/","text":"SR Linux packs a lot of unique features that the data center networking teams can leverage. Some of the features being truly new to the networking domain. The goal of this portal is to introduce SR Linux to the visitors through the interactive tutorials centered around SR Linux services and capabilities. We believe that learning by doing yields the best results. With that in mind we made SR Linux container image available to everybody without any registration or licensing requirements The public SR Linux container image when powered by containerlab allows us to create easily deployable labs that everyone can launch at their convenience. All that to let you not only read about the features we offer, but to try them live! SR Linux container image # A single container image that hosts management, control and data plane functions is all you need to get started. Getting the image # To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry . This means that you can pull SR Linux container image exactly the same way as you would pull any other image: docker pull ghcr.io/nokia/srlinux When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the nokia/srlinux image has and change the docker pull command accordingly. Running SR Linux # When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces. A system on which you can run SR Linux containers should conform to the following requirements: Linux OS with a kernel v4+ 1 . Docker container runtime. At least 2 vCPU and 4GB RAM. A user with administrative privileges. Let's explore the different ways you can launch SR Linux container. Docker CLI # docker CLI offers a quick way to run standalone SR Linux container: docker run -t -d --rm --privileged \\ -u $( id -u ) : $( id -g ) \\ --name srlinux ghcr.io/nokia/srlinux \\ sudo bash /opt/srlinux/bin/sr_linux The above command will start the container named srlinux on the host system with a single management interface attached to the default docker network. This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work. For multi-node SR Linux deployments containerlab 3 offers a better way. Containerlab # Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle. A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together: # file: srlinux.clab.yml name : srlinux topology : nodes : srl1 : kind : srl image : ghcr.io/nokia/srlinux srl2 : kind : srl image : ghcr.io/nokia/srlinux links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] By copying this file over to your system you can immediately deploy it with containerlab: containerlab deploy -t srlinux.clab.yml INFO[0000] Parsing & checking topology file: srlinux.clab.yml INFO[0000] Creating lab directory: /root/demo/clab-srlinux INFO[0000] Creating container: srl1 INFO[0000] Creating container: srl2 INFO[0001] Creating virtual wire: srl1:e1-1 <--> srl2:e1-1 INFO[0001] Writing /etc/hosts file +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlinux-srl1 | 50826b3e3703 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/64 | | 2 | clab-srlinux-srl2 | 4d4494aba320 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/64 | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ Deployment verification # Regardless of the way you spin up SR Linux container it will be visible in the output of the docker ps command. If the deployment process went well and the container did not exit, a user can see it with docker ps command: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4d4494aba320 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 32 minutes ago Up 32 minutes clab-learn-01-srl2 The logs of the running container can be displayed with docker logs <container-name> . In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the docker ps output as this command only shows running containers. Containers which are in the exited status will be part of the docker ps -a output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination. Connecting to SR Linux # When SR Linux container is up and running, users can connect to it over different interfaces. CLI # One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface . To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the sr_cli process inside of it: # get SR Linux container name -> clab-srl01-srl $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17a47c58ad59 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 10 seconds ago Up 6 seconds clab-learn-01-srl1 # start the sr_cli process inside this container to get access to CLI docker exec -it clab-learn-01-srl1 sr_cli Using configuration file ( s ) : [] Welcome to the srlinux CLI. Type 'help' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials admin:admin you can connect to the CLI over the network: # containerlab creates local /etc/hosts entries # for container names to resolve to their IP ssh admin@clab-learn-01-srl1 admin@clab-learn-01-srl1 's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type ' help ' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# gNMI # SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400. Using the gNMI client 2 users can explore SR Linux' gNMI interface: gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities gNMI version: 0.7.0 supported models: - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31 - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30 - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31 <SNIP> Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container \u21a9 for example gnmic \u21a9 The labs referenced on this site are deployed with containerlab unless stated otherwise \u21a9","title":"Get SR Linux"},{"location":"get-started/#sr-linux-container-image","text":"A single container image that hosts management, control and data plane functions is all you need to get started.","title":"SR Linux container image"},{"location":"get-started/#getting-the-image","text":"To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry . This means that you can pull SR Linux container image exactly the same way as you would pull any other image: docker pull ghcr.io/nokia/srlinux When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the nokia/srlinux image has and change the docker pull command accordingly.","title":"Getting the image"},{"location":"get-started/#running-sr-linux","text":"When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces. A system on which you can run SR Linux containers should conform to the following requirements: Linux OS with a kernel v4+ 1 . Docker container runtime. At least 2 vCPU and 4GB RAM. A user with administrative privileges. Let's explore the different ways you can launch SR Linux container.","title":"Running SR Linux"},{"location":"get-started/#docker-cli","text":"docker CLI offers a quick way to run standalone SR Linux container: docker run -t -d --rm --privileged \\ -u $( id -u ) : $( id -g ) \\ --name srlinux ghcr.io/nokia/srlinux \\ sudo bash /opt/srlinux/bin/sr_linux The above command will start the container named srlinux on the host system with a single management interface attached to the default docker network. This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work. For multi-node SR Linux deployments containerlab 3 offers a better way.","title":"Docker CLI"},{"location":"get-started/#containerlab","text":"Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle. A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together: # file: srlinux.clab.yml name : srlinux topology : nodes : srl1 : kind : srl image : ghcr.io/nokia/srlinux srl2 : kind : srl image : ghcr.io/nokia/srlinux links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] By copying this file over to your system you can immediately deploy it with containerlab: containerlab deploy -t srlinux.clab.yml INFO[0000] Parsing & checking topology file: srlinux.clab.yml INFO[0000] Creating lab directory: /root/demo/clab-srlinux INFO[0000] Creating container: srl1 INFO[0000] Creating container: srl2 INFO[0001] Creating virtual wire: srl1:e1-1 <--> srl2:e1-1 INFO[0001] Writing /etc/hosts file +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlinux-srl1 | 50826b3e3703 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/64 | | 2 | clab-srlinux-srl2 | 4d4494aba320 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/64 | +---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+","title":"Containerlab"},{"location":"get-started/#deployment-verification","text":"Regardless of the way you spin up SR Linux container it will be visible in the output of the docker ps command. If the deployment process went well and the container did not exit, a user can see it with docker ps command: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4d4494aba320 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 32 minutes ago Up 32 minutes clab-learn-01-srl2 The logs of the running container can be displayed with docker logs <container-name> . In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the docker ps output as this command only shows running containers. Containers which are in the exited status will be part of the docker ps -a output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination.","title":"Deployment verification"},{"location":"get-started/#connecting-to-sr-linux","text":"When SR Linux container is up and running, users can connect to it over different interfaces.","title":"Connecting to SR Linux"},{"location":"get-started/#cli","text":"One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface . To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the sr_cli process inside of it: # get SR Linux container name -> clab-srl01-srl $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17a47c58ad59 ghcr.io/nokia/srlinux \"/tini -- fixuid -q \u2026\" 10 seconds ago Up 6 seconds clab-learn-01-srl1 # start the sr_cli process inside this container to get access to CLI docker exec -it clab-learn-01-srl1 sr_cli Using configuration file ( s ) : [] Welcome to the srlinux CLI. Type 'help' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1# The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials admin:admin you can connect to the CLI over the network: # containerlab creates local /etc/hosts entries # for container names to resolve to their IP ssh admin@clab-learn-01-srl1 admin@clab-learn-01-srl1 's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type ' help ' ( and press <ENTER> ) if you need any help using this. -- { running } -- [ ] -- A:srl1#","title":"CLI"},{"location":"get-started/#gnmi","text":"SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400. Using the gNMI client 2 users can explore SR Linux' gNMI interface: gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities gNMI version: 0.7.0 supported models: - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31 - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30 - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31 <SNIP> Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container \u21a9 for example gnmic \u21a9 The labs referenced on this site are deployed with containerlab unless stated otherwise \u21a9","title":"gNMI"},{"location":"kb/cfgmgmt/","text":"SR Linux employs a transaction-based configuration management system. That allows for a number of changes to be made to the configuration with an explicit commit required to apply the changes as a single transaction. Configuration file # The default location for the configuration file is /etc/opt/srlinux/config.json . If there is no configuration file present, a basic configuration file is auto-generated with the following defaults: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created SSH server is enabled Some default IPv4/v6 CPM filters Configuration modes # Configuration modes define how the system is running when transactions are performed. Supported modes are the following: Running: the default mode when logging in and displays displays the currently running or active configuration. State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: this mode is used to modify configuration. Modifications are not applied until the commit is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes: Shared: this is the default mode when entering the candidate mode with enter candidate command. This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied. Exclusive Candidate: When entering candidate mode with enter candidate exclusive , it locks out other users from making changes to the candidate configuration. You can enter candidate exclusive mode only under the following conditions: The current shared candidate configuration has not been modified. There are no other users in candidate shared mode. No other users have entered candidate exclusive mode. Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed. When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name private-<username> unless a unique name is defined. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device. Setting the configuration mode # After logging in to the CLI, you are initially placed in running mode. The following table provides commands to enter in a specific mode: Candidate mode Command to enter Candidate shared enter candidate Candidate mode for named shared candidate enter candidate name <name> Candidate private enter candidate private Candidate mode for named private candidate enter candidate private name <name> Candidate exclusive enter candidate exclusive Exclusive mode for named candidate enter candidate exclusive name <name> Running enter running State enter state Show enter show Committing configuration # Changes made during a configuration modification session do not take effect until a commit command is issued. Several options are available for commit command, below are the most notable ones: Option Action commit now Apply the changes, exit candidate mode, and enter running mode commit stay Apply the changes and then remain in candidate mode commit save Apply the changes and automatically save the commit to the startup configuration commit confirmed Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted Deleting configuration # Use the delete command to delete configurations while in candidate mode. The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { login-banner \"Welcome to SRLinux!\" } } --{ candidate shared default}--[ ]-- A:leaf1# delete system banner --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { } } Discarding configuration # You can discard previously applied configurations with the discard command. To discard the changes and remain in candidate mode with a new candidate session, enter discard stay . To discard the changes, exit candidate mode, and enter running mode, enter discard now . Displaying configuration diff # Use the diff command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore. The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination. Global arguments include: baseline , candidate , checkpoint , factory , file , from , rescue , running , and startup . The diff command can be used outside of candidate mode, but only if used with arguments. The following shows a basic diff command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: --{ candidate shared default }--[ ]-- # interface ethernet-1/1 admin-state disable --{ * candidate shared default }--[ ]-- # interface ethernet-1/2 description \"updated\" --{ * candidate shared default }--[ ]-- # diff interface ethernet-1/1 { + admin-state disable } + interface ethernet-1/2 { + description updated + } Displaying configuration details # Use the info command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context. To display the entire configuration, enter info from the root context: --{ candidate shared default}--[ ]-- # info <all the configuration is displayed> --{ candidate }--[ ]-- To display the configuration for a specific context, enter info and specify the context: --{ candidate shared default}--[ ]-- # info system lldp system { lldp { admin-state enable hello-timer 600 management-address mgmt0.0 { type [ IPv4 ] } interface mgmt0 { admin-state disable } } } The following info command options are rather useful: as-json - to display JSON-formatted output detail - to display values for all parameters, including those not specifically configured flat - to display the output as a series of set statements, omitting indentation for any sub-contexts","title":"Configuration management"},{"location":"kb/cfgmgmt/#configuration-file","text":"The default location for the configuration file is /etc/opt/srlinux/config.json . If there is no configuration file present, a basic configuration file is auto-generated with the following defaults: Creation of a management network instance Management interface is added to the mgmt network instance DHCP v4/v6 is enabled on mgmt interface A set of default of logs are created SSH server is enabled Some default IPv4/v6 CPM filters","title":"Configuration file"},{"location":"kb/cfgmgmt/#configuration-modes","text":"Configuration modes define how the system is running when transactions are performed. Supported modes are the following: Running: the default mode when logging in and displays displays the currently running or active configuration. State: the running configuration plus the addition of any dynamically added data. Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc. Candidate: this mode is used to modify configuration. Modifications are not applied until the commit is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes: Shared: this is the default mode when entering the candidate mode with enter candidate command. This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied. Exclusive Candidate: When entering candidate mode with enter candidate exclusive , it locks out other users from making changes to the candidate configuration. You can enter candidate exclusive mode only under the following conditions: The current shared candidate configuration has not been modified. There are no other users in candidate shared mode. No other users have entered candidate exclusive mode. Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed. When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name private-<username> unless a unique name is defined. Note gNMI & JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.","title":"Configuration modes"},{"location":"kb/cfgmgmt/#setting-the-configuration-mode","text":"After logging in to the CLI, you are initially placed in running mode. The following table provides commands to enter in a specific mode: Candidate mode Command to enter Candidate shared enter candidate Candidate mode for named shared candidate enter candidate name <name> Candidate private enter candidate private Candidate mode for named private candidate enter candidate private name <name> Candidate exclusive enter candidate exclusive Exclusive mode for named candidate enter candidate exclusive name <name> Running enter running State enter state Show enter show","title":"Setting the configuration mode"},{"location":"kb/cfgmgmt/#committing-configuration","text":"Changes made during a configuration modification session do not take effect until a commit command is issued. Several options are available for commit command, below are the most notable ones: Option Action commit now Apply the changes, exit candidate mode, and enter running mode commit stay Apply the changes and then remain in candidate mode commit save Apply the changes and automatically save the commit to the startup configuration commit confirmed Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted","title":"Committing configuration"},{"location":"kb/cfgmgmt/#deleting-configuration","text":"Use the delete command to delete configurations while in candidate mode. The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { login-banner \"Welcome to SRLinux!\" } } --{ candidate shared default}--[ ]-- A:leaf1# delete system banner --{ candidate shared default}--[ ]-- A:leaf1# info system banner system { banner { } }","title":"Deleting configuration"},{"location":"kb/cfgmgmt/#discarding-configuration","text":"You can discard previously applied configurations with the discard command. To discard the changes and remain in candidate mode with a new candidate session, enter discard stay . To discard the changes, exit candidate mode, and enter running mode, enter discard now .","title":"Discarding configuration"},{"location":"kb/cfgmgmt/#displaying-configuration-diff","text":"Use the diff command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore. The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination. Global arguments include: baseline , candidate , checkpoint , factory , file , from , rescue , running , and startup . The diff command can be used outside of candidate mode, but only if used with arguments. The following shows a basic diff command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: --{ candidate shared default }--[ ]-- # interface ethernet-1/1 admin-state disable --{ * candidate shared default }--[ ]-- # interface ethernet-1/2 description \"updated\" --{ * candidate shared default }--[ ]-- # diff interface ethernet-1/1 { + admin-state disable } + interface ethernet-1/2 { + description updated + }","title":"Displaying configuration diff"},{"location":"kb/cfgmgmt/#displaying-configuration-details","text":"Use the info command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context. To display the entire configuration, enter info from the root context: --{ candidate shared default}--[ ]-- # info <all the configuration is displayed> --{ candidate }--[ ]-- To display the configuration for a specific context, enter info and specify the context: --{ candidate shared default}--[ ]-- # info system lldp system { lldp { admin-state enable hello-timer 600 management-address mgmt0.0 { type [ IPv4 ] } interface mgmt0 { admin-state disable } } } The following info command options are rather useful: as-json - to display JSON-formatted output detail - to display values for all parameters, including those not specifically configured flat - to display the output as a series of set statements, omitting indentation for any sub-contexts","title":"Displaying configuration details"},{"location":"kb/hwtypes/","text":"The SR Linux software supports seven Nokia hardware platforms 1 : 7250 IXR-6 7250 IXR-10 7220 IXR-D1 7220 IXR-D2 7220 IXR-D3 7220 IXR-H2 7220 IXR-H3 The type field under the node configuration sets the emulated hardware type in the containerlab file: # part of the evpn01.clab.yml file nodes : leaf1 : kind : srl type : ixrd3 # <- hardware type this node will emulate The type field defines the hardware variant that this SR Linux node will emulate. The available type values are: type value HW platform ixr6 7250 IXR-6 ixr10 7250 IXR-10 ixrd1 7220 IXR-D1 ixrd2 7220 IXR-D2 ixrd3 7220 IXR-D3 ixrh2 7220 IXR-H2 ixrh3 7220 IXR-H3 Tip Containerlab-launched nodes are started as ixrd2 hardware type unless set to a different type in the clab file. SR Linux can also run on the whitebox/3 rd party switches. \u21a9","title":"Hardware types"},{"location":"kb/ifaces/","text":"On the SR Linux, an interface is any physical or logical port through which packets can be sent to or received from other devices. Loopback # Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named loN , where N is 0 to 255. System # The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces: The system interface can be bound to the default network-instance only. The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses. The system interface cannot be administratively disabled. Once configured, it is always up. The SR Linux supports a single system interface named system0 . When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels. Network # Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic. The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options. In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: ethernet-slot/port . For example, interface ethernet-2/1 refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card. On 7220 IXR-D3 systems, the QSFP28 connector ports (ports 1/3-1/33 ) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: ethernet-slot/port/breakout-port . For example, if interface ethernet 1/3 is enabled for breakout mode, its breakout ports are named as follows: ethernet 1/3/1 ethernet 1/3/2 ethernet 1/3/3 ethernet 1/3/4 Management # Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named mgmt0 . The mgmt0 interface supports the same functionality and defaults as a network interface, except for the following: Packets sent and received on the mgmt0 interface are processed completely in software. The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class. The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port. Integrated Routing and Bridging (IRB) # IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface. Subinterfaces # On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface. Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags. While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance. You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface. The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class SR Linux subinterfaces can be specified as type routed or bridged: Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf. Bridged subinterfaces can be assigned to a network-instance of type mac-vrf. Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.","title":"Interfaces"},{"location":"kb/ifaces/#loopback","text":"Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named loN , where N is 0 to 255.","title":"Loopback"},{"location":"kb/ifaces/#system","text":"The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces: The system interface can be bound to the default network-instance only. The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses. The system interface cannot be administratively disabled. Once configured, it is always up. The SR Linux supports a single system interface named system0 . When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels.","title":"System"},{"location":"kb/ifaces/#network","text":"Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic. The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options. In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: ethernet-slot/port . For example, interface ethernet-2/1 refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card. On 7220 IXR-D3 systems, the QSFP28 connector ports (ports 1/3-1/33 ) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: ethernet-slot/port/breakout-port . For example, if interface ethernet 1/3 is enabled for breakout mode, its breakout ports are named as follows: ethernet 1/3/1 ethernet 1/3/2 ethernet 1/3/3 ethernet 1/3/4","title":"Network"},{"location":"kb/ifaces/#management","text":"Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named mgmt0 . The mgmt0 interface supports the same functionality and defaults as a network interface, except for the following: Packets sent and received on the mgmt0 interface are processed completely in software. The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class. The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port.","title":"Management"},{"location":"kb/ifaces/#integrated-routing-and-bridging-irb","text":"IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface.","title":"Integrated Routing and Bridging (IRB)"},{"location":"kb/ifaces/#subinterfaces","text":"On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface. Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags. While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance. You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface. The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class SR Linux subinterfaces can be specified as type routed or bridged: Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf. Bridged subinterfaces can be assigned to a network-instance of type mac-vrf. Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.","title":"Subinterfaces"},{"location":"kb/mgmt/","text":"Nokia SR Linux is equipped with 100% YANG modelled management interfaces. The supported management interfaces (CLI, JSON-RPC, and gNMI) access the common management API layer via a gRPC interface. Since all interfaces act as a client towards a common management API, SR Linux provides complete consistency across all the management interfaces with regards to the capabilities available to each of them. SR Linux CLI # The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session. Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces. Features # Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands. Accessing the CLI # After the SR Linux device is initialized, you can access the CLI using a console or SSH connection. Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to leaf1 : ssh admin@clab-quickstart-leaf1 Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts. admin@clab-quickstart-leaf1's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# Prompt # By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: --{ modified? mode_and_session_type }--[ context ]-- hostname# Example: --{ * candidate shared }--[ acl ]-- 3-node-srlinux-A# The CLI prompt is configurable and can be changed within the environment prompt configuration context. In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays: the current mode and session type whether the configuration has been modified the user name and session ID of the current AAA session and the local time For example: Current mode: * candidate shared root (36) Wed 09:52PM gNMI # The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system. SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information. Supported gNMI RPCs are: Get Set Subscribe Capabilities JSON-RPC # The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system. If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.","title":"Management interfaces"},{"location":"kb/mgmt/#sr-linux-cli","text":"The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session. Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces.","title":"SR Linux CLI"},{"location":"kb/mgmt/#features","text":"Output Modifiers. Advanced Linux output modifiers grep , more , wc , head , and tail are exposed directly through the SR Linux CLI. Suggestions & List Completions. As commands are typed suggestions are provided. Tab can be used to list options available. Output Format. When displaying info from a given datastore, the output can be formatted in one of three ways: Text: this is the default out, it is JSON-like but not quite JSON. JSON: the output will be in JSON format. Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful. Aliases. An alias is used to map a CLI command to a shorter easier to remember command. For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long. An alias could be configured so that a shorter string of text could be used to execute that long CLI command. Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.","title":"Features"},{"location":"kb/mgmt/#accessing-the-cli","text":"After the SR Linux device is initialized, you can access the CLI using a console or SSH connection. Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to leaf1 : ssh admin@clab-quickstart-leaf1 Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts. admin@clab-quickstart-leaf1's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1#","title":"Accessing the CLI"},{"location":"kb/mgmt/#prompt","text":"By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: --{ modified? mode_and_session_type }--[ context ]-- hostname# Example: --{ * candidate shared }--[ acl ]-- 3-node-srlinux-A# The CLI prompt is configurable and can be changed within the environment prompt configuration context. In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays: the current mode and session type whether the configuration has been modified the user name and session ID of the current AAA session and the local time For example: Current mode: * candidate shared root (36) Wed 09:52PM","title":"Prompt"},{"location":"kb/mgmt/#gnmi","text":"The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system. SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information. Supported gNMI RPCs are: Get Set Subscribe Capabilities","title":"gNMI"},{"location":"kb/mgmt/#json-rpc","text":"The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system. If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.","title":"JSON-RPC"},{"location":"kb/netwinstance/","text":"On the SR Linux, you can configure one or more virtual routing instances, known as network instances. Each network instance has its own interfaces, its own protocol instances, its own route table, and its own FIB. When a packet arrives on a subinterface associated with a network instance, it is forwarded according to the FIB of that network instance. Transit packets are normally forwarded out another subinterface of the network instance. SR Linux supports the following types of network instances: default ip-vrf mac-vrf The initial startup configuration for SR Linux has a single default network instance. By default, there are no ip-vrf or mac-vrf network instances; these must be created by explicit configuration. The ip-vrf network instances are the building blocks of Layer 3 IP VPN services, and mac-vrf network instances are the building blocks of EVPN services. Within a network instance, you can configure BGP, OSPF, and IS-IS protocol options that apply only to that network instance.","title":"Network instances"},{"location":"ndk/intro/","text":"NetOps Development Kit # Nokia SR Linux enables its users to create high-performance applications which run alongside native apps on SR Linux Network OS. These \"on-box custom applications\" can be deeply integrated with the rest of the SR Linux system and thus can perform tasks which are not possible with traditional management interfaces common for the typical network operating systems. Custom applications run natively on SR Linux NOS The on-box applications (which we also refer to as \"agents\") leverage the SR Linux SDK called NetOps Development Kit or NDK for short. Applications developed with SR Linux NDK have a set of unique characteristics which set them aside from the traditional off-box automation solutions: Native integration with SR Linux system SR Linux architecture is built in a way that NDK agents look and feel like any other regular application such as bgp or acl. This seamless integration is achieved on several levels: System integration: when deployed on SR Linux system NDK agent renders itself like any other \"standard\" application. That makes lifecycle management unified between Nokia provided system apps and custom agents. CLI integration: every NDK agent automatically becomes a part of the global CLI tree, making it possible to configure the agent and query its state in the same way as with any other configuration region. Telemetry integration: an NDK agent configuration and state data will automatically become available for Streaming Telemetry consumption. Programming language neutral With SR Linux NDK the developers are not forced to use any particular language when writing their apps. As NDK is a gRPC service defined with Protocol Buffers it is possible to use any 1 programming language for which protobuf compiler is available. Deep integration with system components NDK apps are not constrained to only configuration and state management as often happens with traditional north-bound interfaces. On the contrary, NDK service exposes additional services that enable deep integration with the SR Linux system such as listening to RIB/FIB updates or having direct access to datapath. With the information outlined in the NDK Developers Guide you will learn about NDK architecture and how to develop apps with this kit. Navigate to the Apps Catalog to browse our growing catalog of NDK apps that were written by Nokia or 3 rd parties. Which in practice covers all popular programming languages: Python, Go, C#, C, C++, Java, JS, etc. \u21a9","title":"Introduction"},{"location":"ndk/intro/#netops-development-kit","text":"Nokia SR Linux enables its users to create high-performance applications which run alongside native apps on SR Linux Network OS. These \"on-box custom applications\" can be deeply integrated with the rest of the SR Linux system and thus can perform tasks which are not possible with traditional management interfaces common for the typical network operating systems. Custom applications run natively on SR Linux NOS The on-box applications (which we also refer to as \"agents\") leverage the SR Linux SDK called NetOps Development Kit or NDK for short. Applications developed with SR Linux NDK have a set of unique characteristics which set them aside from the traditional off-box automation solutions: Native integration with SR Linux system SR Linux architecture is built in a way that NDK agents look and feel like any other regular application such as bgp or acl. This seamless integration is achieved on several levels: System integration: when deployed on SR Linux system NDK agent renders itself like any other \"standard\" application. That makes lifecycle management unified between Nokia provided system apps and custom agents. CLI integration: every NDK agent automatically becomes a part of the global CLI tree, making it possible to configure the agent and query its state in the same way as with any other configuration region. Telemetry integration: an NDK agent configuration and state data will automatically become available for Streaming Telemetry consumption. Programming language neutral With SR Linux NDK the developers are not forced to use any particular language when writing their apps. As NDK is a gRPC service defined with Protocol Buffers it is possible to use any 1 programming language for which protobuf compiler is available. Deep integration with system components NDK apps are not constrained to only configuration and state management as often happens with traditional north-bound interfaces. On the contrary, NDK service exposes additional services that enable deep integration with the SR Linux system such as listening to RIB/FIB updates or having direct access to datapath. With the information outlined in the NDK Developers Guide you will learn about NDK architecture and how to develop apps with this kit. Navigate to the Apps Catalog to browse our growing catalog of NDK apps that were written by Nokia or 3 rd parties. Which in practice covers all popular programming languages: Python, Go, C#, C, C++, Java, JS, etc. \u21a9","title":"NetOps Development Kit"},{"location":"ndk/apps/catalog/","text":"App Catalog # SR Linux NetOps Development Kit (NDK) enables its users to write apps which can solve many automation tasks, operational hurdles or optimization problems. gRPC based service that provides a deep integration with Network OS is quite a novel thing for a networking domain, which makes NDK application examples the second most valuable asset after the NDK documentation provided here. Sometimes the best applications are born after getting inspired by other's work or as a combination of ideas implemented in different examples. With App Catalog our intention is to collect references to the noteworthy NDK applications that have been open sourced by Nokia engineers or 3 rd parties. We hope that with that growing catalog of examples both new and seasoned NDK users will find something that can inspire them to create their next app. Disclaimer The examples listed in the App Catalog are not of a production quality and should not be used as such. Visitors of App Catalog should treat those applications/agents as a demo examples of what can be achieved with NDK. The applications that are kept under srl-labs or nokia GitHub organizations are not official Nokia products, unless explicitly mentioned. NDK agents # EVPN Proxy # \u00b7 jbemmel/srl-evpn-proxy SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN. Read more kButler # \u00b7 brwallis/srlinux-kbutler kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node. Read more Prometheus Exporter # \u00b7 karimra/srl-prometheus-exporter SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector. Read more","title":"About"},{"location":"ndk/apps/catalog/#app-catalog","text":"SR Linux NetOps Development Kit (NDK) enables its users to write apps which can solve many automation tasks, operational hurdles or optimization problems. gRPC based service that provides a deep integration with Network OS is quite a novel thing for a networking domain, which makes NDK application examples the second most valuable asset after the NDK documentation provided here. Sometimes the best applications are born after getting inspired by other's work or as a combination of ideas implemented in different examples. With App Catalog our intention is to collect references to the noteworthy NDK applications that have been open sourced by Nokia engineers or 3 rd parties. We hope that with that growing catalog of examples both new and seasoned NDK users will find something that can inspire them to create their next app. Disclaimer The examples listed in the App Catalog are not of a production quality and should not be used as such. Visitors of App Catalog should treat those applications/agents as a demo examples of what can be achieved with NDK. The applications that are kept under srl-labs or nokia GitHub organizations are not official Nokia products, unless explicitly mentioned.","title":"App Catalog"},{"location":"ndk/apps/catalog/#ndk-agents","text":"","title":"NDK agents"},{"location":"ndk/apps/catalog/#evpn-proxy","text":"\u00b7 jbemmel/srl-evpn-proxy SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN. Read more","title":"EVPN Proxy"},{"location":"ndk/apps/catalog/#kbutler","text":"\u00b7 brwallis/srlinux-kbutler kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node. Read more","title":"kButler"},{"location":"ndk/apps/catalog/#prometheus-exporter","text":"\u00b7 karimra/srl-prometheus-exporter SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector. Read more","title":"Prometheus Exporter"},{"location":"ndk/apps/evpn-proxy/","text":"SR Linux EVPN Proxy # Description SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN Components Nokia SR Linux , Cumulus VX Programming Language Python Source Code jbemmel/srl-evpn-proxy Authors Jeroen van Bemmel Introduction # Most data center designs start small before they evolve. At small scale, it may make sense to manually configure static VXLAN tunnels between leaf switches, as implemented on the 2 virtual lab nodes on the left side. There is nothing wrong with such an initial design, but as the fabric grows and the number of leaves reaches a certain threshold, having to touch every switch each time a device is added can get cumbersome and error prone. The internet and most modern large scale data center designs use dynamic control plane protocols and volatile in-memory configuration to configure packet forwarding. BGP is a popular choice, and the Ethernet VPN address family ( EVPN RFC8365 ) can support both L2 and L3 overlay services. However, legacy fabrics continue to support business critical applications, and there is a desire to keep doing so without service interruptions, and with minimal changes. So how can we move to the new dynamic world of EVPN based data center fabrics, while transitioning gradually and smoothly from these static configurations? EVPN Proxy Agent # The evpn-proxy agent developed with NDK can answer the need of gradually transitioning from the static VXLAN dataplane to the EVPN based service. It has a lot of embedded functionality, we will cover the core feature here which is the Static VXLAN <-> EVPN Proxy functionality for point to point tunnels. The agent gets installed on SR Linux NOS and enables the control plane stitching between static VXLAN VTEP and EVPN-enabled service by generating EVPN routes on behalf of a legacy VTEP device.","title":"EVPN Proxy"},{"location":"ndk/apps/evpn-proxy/#sr-linux-evpn-proxy","text":"Description SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN Components Nokia SR Linux , Cumulus VX Programming Language Python Source Code jbemmel/srl-evpn-proxy Authors Jeroen van Bemmel","title":"SR Linux EVPN Proxy"},{"location":"ndk/apps/evpn-proxy/#introduction","text":"Most data center designs start small before they evolve. At small scale, it may make sense to manually configure static VXLAN tunnels between leaf switches, as implemented on the 2 virtual lab nodes on the left side. There is nothing wrong with such an initial design, but as the fabric grows and the number of leaves reaches a certain threshold, having to touch every switch each time a device is added can get cumbersome and error prone. The internet and most modern large scale data center designs use dynamic control plane protocols and volatile in-memory configuration to configure packet forwarding. BGP is a popular choice, and the Ethernet VPN address family ( EVPN RFC8365 ) can support both L2 and L3 overlay services. However, legacy fabrics continue to support business critical applications, and there is a desire to keep doing so without service interruptions, and with minimal changes. So how can we move to the new dynamic world of EVPN based data center fabrics, while transitioning gradually and smoothly from these static configurations?","title":"Introduction"},{"location":"ndk/apps/evpn-proxy/#evpn-proxy-agent","text":"The evpn-proxy agent developed with NDK can answer the need of gradually transitioning from the static VXLAN dataplane to the EVPN based service. It has a lot of embedded functionality, we will cover the core feature here which is the Static VXLAN <-> EVPN Proxy functionality for point to point tunnels. The agent gets installed on SR Linux NOS and enables the control plane stitching between static VXLAN VTEP and EVPN-enabled service by generating EVPN routes on behalf of a legacy VTEP device.","title":"EVPN Proxy Agent"},{"location":"ndk/apps/kbutler/","text":"kButler - k8s aware agent # Description kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node Components Nokia SR Linux , Kubernetes, MetalLB Programming Language Go Source Code brwallis/srlinux-kbutler Additional resources This agent was demonstrated at NFD 25 Authors Bruce Wallis Introduction # In the datacenter fabrics where applications run in Kubernetes clusters it is common to see Metallb to be used as a mean to advertise k8s services external IP addresses towards the fabric switches over BGP. kButler agent demo setup From the application owner standpoint as long as all the nodes advertise IP addresses of the application-related services things are considered to work as expected. But applications users do not get connected to the apps directly, there is always a network in-between which needs to play in unison with the applications. How can we make sure, that the network state matches the expectations of the applications? The networking folks may have little to no visibility into the application land, thus they may not have the necessary information to say if a network state reflects the applications configuration. Consider the diagram above, and the following state of affairs: application App1 is scaled to run on all three nodes of a cluster a service is created to make this application available from the outside of the k8s cluster all three nodes advertise the virtual IP of the App1 with its own nexthop via BGP If all goes well, the Data Center leaf switch will install three routes in its forwarding and will ECMP load balance requests towards the nodes running application pods. But what if the leaf switch has installed only two routes in its FIB? This can be a result of a fat fingering during the BGP configuration, or a less likely event of a resources congestion. In any case, the disparity between the network state and the application can arise. The questions becomes, how can we make the network to be aware of the applications configuration and make sure that those deviations can be easily spotted by the NetOps teams? kButler # The kButler NDK agent is designed to demonstrate how data center switches can tap into the application land and correlated the network state with the application configuration. At a high level, the agent does the following: subscribes to the K8S service API and is specifically interested in any new services being exposed or changes to existing exposed services. Objective is to gain view of which worker nodes host an application which has an associated exposed service subscribes to the SR Linux NDK API listening for any changes to the FIB ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node reports the operational status within SR Linux allowing quick alerts of any K8S service degradation provides contextualized monitoring, alerting and troubleshooting, exposing its data model through all available SR Linux management interfaces","title":"kButler"},{"location":"ndk/apps/kbutler/#kbutler-k8s-aware-agent","text":"Description kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node Components Nokia SR Linux , Kubernetes, MetalLB Programming Language Go Source Code brwallis/srlinux-kbutler Additional resources This agent was demonstrated at NFD 25 Authors Bruce Wallis","title":"kButler - k8s aware agent"},{"location":"ndk/apps/kbutler/#introduction","text":"In the datacenter fabrics where applications run in Kubernetes clusters it is common to see Metallb to be used as a mean to advertise k8s services external IP addresses towards the fabric switches over BGP. kButler agent demo setup From the application owner standpoint as long as all the nodes advertise IP addresses of the application-related services things are considered to work as expected. But applications users do not get connected to the apps directly, there is always a network in-between which needs to play in unison with the applications. How can we make sure, that the network state matches the expectations of the applications? The networking folks may have little to no visibility into the application land, thus they may not have the necessary information to say if a network state reflects the applications configuration. Consider the diagram above, and the following state of affairs: application App1 is scaled to run on all three nodes of a cluster a service is created to make this application available from the outside of the k8s cluster all three nodes advertise the virtual IP of the App1 with its own nexthop via BGP If all goes well, the Data Center leaf switch will install three routes in its forwarding and will ECMP load balance requests towards the nodes running application pods. But what if the leaf switch has installed only two routes in its FIB? This can be a result of a fat fingering during the BGP configuration, or a less likely event of a resources congestion. In any case, the disparity between the network state and the application can arise. The questions becomes, how can we make the network to be aware of the applications configuration and make sure that those deviations can be easily spotted by the NetOps teams?","title":"Introduction"},{"location":"ndk/apps/kbutler/#kbutler","text":"The kButler NDK agent is designed to demonstrate how data center switches can tap into the application land and correlated the network state with the application configuration. At a high level, the agent does the following: subscribes to the K8S service API and is specifically interested in any new services being exposed or changes to existing exposed services. Objective is to gain view of which worker nodes host an application which has an associated exposed service subscribes to the SR Linux NDK API listening for any changes to the FIB ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node reports the operational status within SR Linux allowing quick alerts of any K8S service degradation provides contextualized monitoring, alerting and troubleshooting, exposing its data model through all available SR Linux management interfaces","title":"kButler"},{"location":"ndk/apps/srl-prom-exporter/","text":"SR Linux Prometheus Exporter # Description SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector. Components Nokia SR Linux , Prometheus Programming Language Go Source Code karimra/srl-prometheus-exporter Authors Karim Radhouani Introduction # Most Streaming Telemetry stacks are built with a telemetry collector 1 playing a key part in getting data out of the network elements via gNMI subscriptions. While this deployment model is valid and common it is not the only model that can be used. With SR Linux Prometheus Exporter agent we offer SR Linux users another way to consume Streaming Telemetry in a scaled out fashion. Classic and agent-enabled telemetry stacks With Prometheus Exporter agent deployed on SR Linux switches the telemetry deployment model changes from a \"single collector - many targets\" to a \"many collectors - single target\" mode. The collection role is now distributed across the network with Prometheus TSDB scraping metrics endpoints exposed by the agents. Adopting this model has some interesting benefits beyond load sharing the collection task across the network fleet: \"Removing\" gNMI complexity As gNMI based collection now happens \"inside\" the switch, the monitoring teams do not need to be exposed to gNMI subscription internals or to worry about managing collectors. This streamlines the telemetry scraping workflows, as now the switches practically behave the same way as any other system that provides telemetry metrics. Easy way to add/remove subscription Since SR Linux NDK agents provide seamless integration with all the management interfaces, the subscription handling can be done via CLI/gNMI/JSON-RPC. Users will add them the same way they do any configuration on their switches. Most common subscriptions come pre-baked into the agent, removing the need to do anything for getting basic statistics out of the switches. Auto discovery of nodes Agents can register the prometheus endpoints they expose in Consul , which will enable Prometheus server to auto-discover the new nodes as they come This is your self-organizing telemetry fleet. Agent's operations # Agent's core components and interactions map The high level operations model of the srl-prometheus-exported consists of the following steps: Agent maps metric names to gNMI XPATHs. A user can disable/enable metrics via any mgmt interface (CLI, gNMI, JSON-RPC) On each scrape request, agent performs a gNMI subscription with mode ONCE for all paths mapped to metrics with state enable (one subscription per metric). The agent will then transform the subscribe responses into prometheus metrics and send them back in the HTTP GET response body. The following diagram outlines the core components of the agent. Consult with the repository's readme on how to install and configure this agent. collectors such as gnmic and others. \u21a9","title":"Prometheus Telemetry Exporter"},{"location":"ndk/apps/srl-prom-exporter/#sr-linux-prometheus-exporter","text":"Description SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector. Components Nokia SR Linux , Prometheus Programming Language Go Source Code karimra/srl-prometheus-exporter Authors Karim Radhouani","title":"SR Linux Prometheus Exporter"},{"location":"ndk/apps/srl-prom-exporter/#introduction","text":"Most Streaming Telemetry stacks are built with a telemetry collector 1 playing a key part in getting data out of the network elements via gNMI subscriptions. While this deployment model is valid and common it is not the only model that can be used. With SR Linux Prometheus Exporter agent we offer SR Linux users another way to consume Streaming Telemetry in a scaled out fashion. Classic and agent-enabled telemetry stacks With Prometheus Exporter agent deployed on SR Linux switches the telemetry deployment model changes from a \"single collector - many targets\" to a \"many collectors - single target\" mode. The collection role is now distributed across the network with Prometheus TSDB scraping metrics endpoints exposed by the agents. Adopting this model has some interesting benefits beyond load sharing the collection task across the network fleet: \"Removing\" gNMI complexity As gNMI based collection now happens \"inside\" the switch, the monitoring teams do not need to be exposed to gNMI subscription internals or to worry about managing collectors. This streamlines the telemetry scraping workflows, as now the switches practically behave the same way as any other system that provides telemetry metrics. Easy way to add/remove subscription Since SR Linux NDK agents provide seamless integration with all the management interfaces, the subscription handling can be done via CLI/gNMI/JSON-RPC. Users will add them the same way they do any configuration on their switches. Most common subscriptions come pre-baked into the agent, removing the need to do anything for getting basic statistics out of the switches. Auto discovery of nodes Agents can register the prometheus endpoints they expose in Consul , which will enable Prometheus server to auto-discover the new nodes as they come This is your self-organizing telemetry fleet.","title":"Introduction"},{"location":"ndk/apps/srl-prom-exporter/#agents-operations","text":"Agent's core components and interactions map The high level operations model of the srl-prometheus-exported consists of the following steps: Agent maps metric names to gNMI XPATHs. A user can disable/enable metrics via any mgmt interface (CLI, gNMI, JSON-RPC) On each scrape request, agent performs a gNMI subscription with mode ONCE for all paths mapped to metrics with state enable (one subscription per metric). The agent will then transform the subscribe responses into prometheus metrics and send them back in the HTTP GET response body. The following diagram outlines the core components of the agent. Consult with the repository's readme on how to install and configure this agent. collectors such as gnmic and others. \u21a9","title":"Agent's operations"},{"location":"ndk/guide/agent-install/","text":"The onboarding of an NDK agent onto SR Linux system is simply a task of copying the agent and its files over to the SR Linux filesystem and placing them in the relevant directories. This table summarizes agent's components and the recommended locations to use. Component Filesystem location Executable file /usr/local/bin/ YANG modules /opt/$agentName/yang Agent's config file /etc/opt/srlinux/appmgr/$agentName.yml Other files /opt/$agentName/ The agent installation procedure can be carried out in different ways: manual copy of files via scp or similar tools automated files delivery via configuration management tools (Ansible, etc) creating an rpm package for agent and its files and installing the package on SR Linux The first two options are straightforward and easy to execute, but they are a bit more involved as the installers need to maintain the remote paths for the copy commands. When using the rpm option though, it becomes less cumbersome to install the package, as all the installers deal with is a single .rpm file and a copy command. Of course, the build process of the rpm package is required, and we would like to explain this process in more details. RPM package # One of the easiest ways to create an rpm, deb or apk package is to use the nFPM tool which is a is a simple, 0-dependencies packager. The only thing that nFPM requires of a user is to create a configuration file with the general instructions on how to build a package, and the rest will be taken care of. nFPM installation # nFPM offers many installation options for all kinds of operating systems and environments. In the course of this guide we will use the universal nFPM docker image . nFPM configuration file # nFPM configuration file is the way of letting nFPM know how to build a package for the software artifacts that users created. The full list of options an nfpm.yml file can have is documented on the project's site . Here we will have a look at the configuration file that is suitable for a typical NDK application written in Go. The file named ndkDemo.yml with the following contents will instruct nFPM how to build a package: name : \"ndkDemo\" # name of the golang package arch : \"amd64\" # architecture you are using version : \"v1.0.0\" # version of this rpm package maintainer : \"John Doe <john@doe.com>\" description : Sample NDK agent # description of a package vendor : \"JD Corp\" # optional information about the creator of the package license : \"BSD 2\" contents : # contents to add to the package - src : ./ndkDemo # local path of agent binary dst : /usr/local/bin/ndkDemo # destination path of agent binary - src : ./yang # local path of agent's YANG directory dst : /opt/ndkDemo/yang # destination path of agent YANG - src : ./ndkDemo.yml # local path of agent yml dst : /etc/opt/srlinux/appmgr/ # destination path of agent yml Running nFPM # When nFPM configuration file and NDK agent files are present it is time to build an rpm package. Consider the following file layout: . \u251c\u2500\u2500 ndkDemo # agent binary file \u251c\u2500\u2500 ndkDemo.yml # agent config file \u251c\u2500\u2500 nfpm.yml # nFPM config file \u2514\u2500\u2500 yang # directory with agent YANG modules \u2514\u2500\u2500 ndkDemo.yang 1 directory, 4 files With these files present we can build an RPM package using the containerized nFPM image like that: docker run --rm -v $PWD :/tmp -w /tmp goreleaser/nfpm package \\ --config /tmp/nfpm.yml \\ --target /tmp \\ --packager rpm This command will create ndkDemo-1.0.0.x86_64.rpm file in the current directory that can be copied over to SR Linux system for installation. Installing RPM # Delivering the available rpm package to a fleet of SR Linux boxes can be done with some configuration management tools. For demo puposes we will utilize the scp utility: # this example copies the rpm via scp command to /tmp dir scp ndkDemo-1.0.0.x86_64.rpm admin@<srlinux-mgmt-address>:/tmp Once the package has been delivered to the SR Linux system it can be installed there. First, we login to SR Linux CLI and drill down to the linux shell: ssh admin@<srlinux-address> admin@clab-srl-srl's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl# bash Once in the bash shell, install the package with yum install or rpm : sudo rpm -U /tmp/ndkDemo-1.0.0.x86_64.rpm Tip To check if the package was installed, issue rpm -qa | grep ndkDemo admin@srl ~ ] $ rpm -qa | grep ndkDemo ndkDemo-1.0.0-1.x86_64 During the package installation the agent related files are copied over to the relevant paths as set with nfpm config file: # check the executable location [ admin@srl ~ ] $ ls -la /usr/local/bin/ | grep ndkDemo -rw-r--r-- 1 root root 12312 Nov 4 11 :28 ndkDemo # check YANG modules dir is present [ admin@srl ~ ] $ ls -la /opt/ndkDemo/yang/ total 8 drwxr-xr-x 2 root root 4096 Nov 4 12 :58 . drwxr-xr-x 3 root root 4096 Nov 4 12 :53 .. -rw-r--r-- 1 root root 0 Nov 4 11 :28 ndkDemo.yang # check ndkDemo config file is present [ admin@srl ~ ] $ ls -la /etc/opt/srlinux/appmgr/ total 16 drwxr-xr-x+ 2 root root 4096 Nov 4 12 :58 . drwxrwxrwx+ 10 srlinux srlinux 4096 Nov 4 12 :53 .. -rw-r--r--+ 1 root root 0 Nov 4 11 :28 ndkDemo.yml All the agent components are available by the paths specified in the nfpm configuration file which completes the agent installation. Note To update SR Linux NDK app, the package has to be removed first sudo yum remove ndkDemo-1.0.0 # using yum sudo rpm -e ndkDemo-1.0.0 # using rpm","title":"Agent Installation"},{"location":"ndk/guide/agent-install/#rpm-package","text":"One of the easiest ways to create an rpm, deb or apk package is to use the nFPM tool which is a is a simple, 0-dependencies packager. The only thing that nFPM requires of a user is to create a configuration file with the general instructions on how to build a package, and the rest will be taken care of.","title":"RPM package"},{"location":"ndk/guide/agent-install/#nfpm-installation","text":"nFPM offers many installation options for all kinds of operating systems and environments. In the course of this guide we will use the universal nFPM docker image .","title":"nFPM installation"},{"location":"ndk/guide/agent-install/#nfpm-configuration-file","text":"nFPM configuration file is the way of letting nFPM know how to build a package for the software artifacts that users created. The full list of options an nfpm.yml file can have is documented on the project's site . Here we will have a look at the configuration file that is suitable for a typical NDK application written in Go. The file named ndkDemo.yml with the following contents will instruct nFPM how to build a package: name : \"ndkDemo\" # name of the golang package arch : \"amd64\" # architecture you are using version : \"v1.0.0\" # version of this rpm package maintainer : \"John Doe <john@doe.com>\" description : Sample NDK agent # description of a package vendor : \"JD Corp\" # optional information about the creator of the package license : \"BSD 2\" contents : # contents to add to the package - src : ./ndkDemo # local path of agent binary dst : /usr/local/bin/ndkDemo # destination path of agent binary - src : ./yang # local path of agent's YANG directory dst : /opt/ndkDemo/yang # destination path of agent YANG - src : ./ndkDemo.yml # local path of agent yml dst : /etc/opt/srlinux/appmgr/ # destination path of agent yml","title":"nFPM configuration file"},{"location":"ndk/guide/agent-install/#running-nfpm","text":"When nFPM configuration file and NDK agent files are present it is time to build an rpm package. Consider the following file layout: . \u251c\u2500\u2500 ndkDemo # agent binary file \u251c\u2500\u2500 ndkDemo.yml # agent config file \u251c\u2500\u2500 nfpm.yml # nFPM config file \u2514\u2500\u2500 yang # directory with agent YANG modules \u2514\u2500\u2500 ndkDemo.yang 1 directory, 4 files With these files present we can build an RPM package using the containerized nFPM image like that: docker run --rm -v $PWD :/tmp -w /tmp goreleaser/nfpm package \\ --config /tmp/nfpm.yml \\ --target /tmp \\ --packager rpm This command will create ndkDemo-1.0.0.x86_64.rpm file in the current directory that can be copied over to SR Linux system for installation.","title":"Running nFPM"},{"location":"ndk/guide/agent-install/#installing-rpm","text":"Delivering the available rpm package to a fleet of SR Linux boxes can be done with some configuration management tools. For demo puposes we will utilize the scp utility: # this example copies the rpm via scp command to /tmp dir scp ndkDemo-1.0.0.x86_64.rpm admin@<srlinux-mgmt-address>:/tmp Once the package has been delivered to the SR Linux system it can be installed there. First, we login to SR Linux CLI and drill down to the linux shell: ssh admin@<srlinux-address> admin@clab-srl-srl's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl# bash Once in the bash shell, install the package with yum install or rpm : sudo rpm -U /tmp/ndkDemo-1.0.0.x86_64.rpm Tip To check if the package was installed, issue rpm -qa | grep ndkDemo admin@srl ~ ] $ rpm -qa | grep ndkDemo ndkDemo-1.0.0-1.x86_64 During the package installation the agent related files are copied over to the relevant paths as set with nfpm config file: # check the executable location [ admin@srl ~ ] $ ls -la /usr/local/bin/ | grep ndkDemo -rw-r--r-- 1 root root 12312 Nov 4 11 :28 ndkDemo # check YANG modules dir is present [ admin@srl ~ ] $ ls -la /opt/ndkDemo/yang/ total 8 drwxr-xr-x 2 root root 4096 Nov 4 12 :58 . drwxr-xr-x 3 root root 4096 Nov 4 12 :53 .. -rw-r--r-- 1 root root 0 Nov 4 11 :28 ndkDemo.yang # check ndkDemo config file is present [ admin@srl ~ ] $ ls -la /etc/opt/srlinux/appmgr/ total 16 drwxr-xr-x+ 2 root root 4096 Nov 4 12 :58 . drwxrwxrwx+ 10 srlinux srlinux 4096 Nov 4 12 :53 .. -rw-r--r--+ 1 root root 0 Nov 4 11 :28 ndkDemo.yml All the agent components are available by the paths specified in the nfpm configuration file which completes the agent installation. Note To update SR Linux NDK app, the package has to be removed first sudo yum remove ndkDemo-1.0.0 # using yum sudo rpm -e ndkDemo-1.0.0 # using rpm","title":"Installing RPM"},{"location":"ndk/guide/agent/","text":"As was explained in the NDK Architecture section, an agent 1 is a custom software that can extend SR Linux capabilities by running alongside SR Linux native applications and performing some user-defined tasks. To deeply integrate with the rest of the SR Linux architecture the agents have to be defined like an application that SR Linux's application manager can take control of. The structure of the agents is the main topic of this chapter. The main three components of an agent: Agent's executable file YANG module Agent configuration file Executable file # An executable file is called when the agent starts running on SR Linux system. It contains the application logic and is typically an executable binary or a script. The application logic takes care of handling the agents configuration that may be provided via any of the management interfaces (CLI, gNMI, etc), and contains the core logic of interfacing with gRPC based NDK services. In the subsequent sections of the Developers Guide we will cover how to write the logic of an agent and interact with various NDK services. An executable file can be placed at /usr/local/bin directory. YANG module # SR Linux is a fully modelled Network OS. As a result, any native or custom application that is meant to be configurable or to have state is required to have a proper YANG model. The \"cost\" associated with requiring users to write YAN G models for their apps pays off greatly as this enables seamless integration of an agent with all management interfaces: CLI, gNMI, JSON-RPC. Any agent's configuration knobs that users expressed in YANG will be immediately available in the SR Linux CLI, as if it was part of it from the beginning. Yes, with auto-suggestion of the fields as well. provides out-of-the-box Streaming Telemetry (gNMI) support for any config or state data that agent maintains And secondly, the YANG modules for custom apps are not that hard to write as their data model is typically rather small. Note YANG module is only needed if a developer wants their agent to be configurable via any of the management interfaces or to keep state. YANG files that are related to the particular agent can be placed by the /opt/$agentName/yang directory. Configuration file # Due to SR Linux modular architecture, each application, be it internal app like bgp or a custom NDK agent, needs to have a configuration file. This file contains application parameters which are read by the Application Manager service to onboard the application onto the system. With agent's config file users define important properties of an application, for example: application version location of the executable file YANG modules related to this app lifecycle management policy and others Custom agents must have their config file present by the /etc/opt/srlinux/appmgr directory. It is a good idea to name agent config file after the agent name, so if we, say, named our agent myCoolAgent , then its config file can be named as myCoolAgent.yml under the /etc/opt/srlinux/appmgr directory. Through the subsequent chapters of the Developers Guide we will cover the most important options, but here is a full list of config file parameters: Full list of config files parameters # Example configuration file for the applications on sr_linux # All valid options are shown and explained # The name of the application. # This must be unique. application-name : # [Mandatory] The source path where the binary can be found path : /usr/local/bin # [Optional, default='./<application-name>'] The command to launch the application. # Note these replacement rules: # {slot-num} will be replaced by the slot number the process is running on # {0}, {1}, ... can be replaced by parameters provided in the launch request (launch-by-request: Yes) launch-command : \"VALUE=2 ./binary_name --log-level debug\" # [Optional, default='<launch-command>'] The command to search for when checking if the application is running. # This will be executed as a prefix search, so if the application was launched using './app-name -loglevel debug' # a search-command './app-name' would work. # Note: same replacement rules as launch-command search-command : \"./binary_name\" # [Optional, default=No] Indicates whether the application needs to be launched automatically never-start : No # [Optional, default=No] Indicates whether the application can be restarted automatically when it crashes. # Applies only when never-start is No (if the app is not started by app_mgr it would not be restarted either). # Applications are only restarted when running app_mgr in restart mode (e.g. sr_linux --restart) never-restart : No # [Optional, default=No] Indicates whether the application will be shown in app manager status never-show : No # [Optional, default=No] Indicates whether the launch of the application is delayed # until any configuration is loaded in the application's YANG modules. wait-for-config : No # [Optional] Indicates the application is run as 'user' including 'root' run-as-user : root # [Optional, default=200] Indicates the order in which the application needs to be launched. # The applications with the lowest value are launched first. # Applications with the same value are launched in an undefined order. # By convention, start-order >= 100 require idb. 1 is reserved for device-mgr, which determines chassis type. start-order : 123 # [Optional, default=No] Indicates whether this application is launched via an request (idb only at this point). launch-by-request : No # [Optional, default=No] Indicates whether this application is launched in a net namespace (launch-by-request # must be set to Yes). launch-in-net-namespace : No # [Optional, default=3] Indicates the number of restarts within failure-window which will trigger the system restart failure-threshold : 3 # [Optional, default=300] Indicates the window in seconds over which to count restarts towards failure-threshold failure-window : 400 # [Optional, default=reboot] Indicates the action taken after 'failure-threshold' failures within 'failure-window' failure-action : 'reboot' # [Optional, default=Nokia] Indicates the author of the application author : 'Nokia' # [Optional, default=\u201d\u201d] The command for app_mgr to run to read the application version version-command : 'snmpd --version' # [Optional The operations that may not be manually performed on this application restricted-operations : [ 'start' , 'stop' , 'restart' , 'quit' , 'kill' ] # [Optional, default No] app-mgr will wait for app to acknowledge it via oob channel oob-init : No # [Optional] The list of launch restrictions - if of all of the restrictions of an element in the list are met, # then the application is launched. The restrictions are separated by a ':'. Valid restrictions are: # 'sim' - running in sim mode (like in container env.) # 'hw' - running on real h/w # 'chassis' - running on a chassis (cpm and imm are running on different processors) # 'imm' - runs on the imm # 'cpm' - runs on the cpm (default) launch-restrictions : [ 'hw:cpm' , 'hw:chassis:imm' ] yang-modules : # [Mandatory] The names of the YANG modules to load. This is usually the file-name without '.yang' names : [ module-name , other-module-name ] # [Optional] List of enabled YANG features. Each needs to be qualified (e.g. srl_nokia-common:foo) enabled-features : [ 'module-name:foo' , 'other-module-name:bar' ] # [Optional] The names of the YANG validation plugins to load. validation-plugins : [ plugin-name , other-plugin-name ] # [Mandatory] All the source-directories where we should search for: # - The YANG modules listed here # - any YANG module included/imported in these modules source-directories : [ /path/one , /path/two ] # [Optional] The names of the not owned YANG modules to load for commit confirmation purposes. not-owned-names : [ module-name , other-module-name ] # [Optional] Multiple applications can be defined in the same YAML file other-application-name : command : \"./other-binary\" path : /other/path Dependency and other files # Quite often an agent may require additional files for its operation. It can be a specific virtual environment for your Python agent, or some JSON file that your agents reads some data from. All those auxiliary files can be saved by the /opt/$agentName/ directory. terms NDK agent and NDK app are used interchangeably \u21a9","title":"Agent structure"},{"location":"ndk/guide/agent/#executable-file","text":"An executable file is called when the agent starts running on SR Linux system. It contains the application logic and is typically an executable binary or a script. The application logic takes care of handling the agents configuration that may be provided via any of the management interfaces (CLI, gNMI, etc), and contains the core logic of interfacing with gRPC based NDK services. In the subsequent sections of the Developers Guide we will cover how to write the logic of an agent and interact with various NDK services. An executable file can be placed at /usr/local/bin directory.","title":"Executable file"},{"location":"ndk/guide/agent/#yang-module","text":"SR Linux is a fully modelled Network OS. As a result, any native or custom application that is meant to be configurable or to have state is required to have a proper YANG model. The \"cost\" associated with requiring users to write YAN G models for their apps pays off greatly as this enables seamless integration of an agent with all management interfaces: CLI, gNMI, JSON-RPC. Any agent's configuration knobs that users expressed in YANG will be immediately available in the SR Linux CLI, as if it was part of it from the beginning. Yes, with auto-suggestion of the fields as well. provides out-of-the-box Streaming Telemetry (gNMI) support for any config or state data that agent maintains And secondly, the YANG modules for custom apps are not that hard to write as their data model is typically rather small. Note YANG module is only needed if a developer wants their agent to be configurable via any of the management interfaces or to keep state. YANG files that are related to the particular agent can be placed by the /opt/$agentName/yang directory.","title":"YANG module"},{"location":"ndk/guide/agent/#configuration-file","text":"Due to SR Linux modular architecture, each application, be it internal app like bgp or a custom NDK agent, needs to have a configuration file. This file contains application parameters which are read by the Application Manager service to onboard the application onto the system. With agent's config file users define important properties of an application, for example: application version location of the executable file YANG modules related to this app lifecycle management policy and others Custom agents must have their config file present by the /etc/opt/srlinux/appmgr directory. It is a good idea to name agent config file after the agent name, so if we, say, named our agent myCoolAgent , then its config file can be named as myCoolAgent.yml under the /etc/opt/srlinux/appmgr directory. Through the subsequent chapters of the Developers Guide we will cover the most important options, but here is a full list of config file parameters: Full list of config files parameters # Example configuration file for the applications on sr_linux # All valid options are shown and explained # The name of the application. # This must be unique. application-name : # [Mandatory] The source path where the binary can be found path : /usr/local/bin # [Optional, default='./<application-name>'] The command to launch the application. # Note these replacement rules: # {slot-num} will be replaced by the slot number the process is running on # {0}, {1}, ... can be replaced by parameters provided in the launch request (launch-by-request: Yes) launch-command : \"VALUE=2 ./binary_name --log-level debug\" # [Optional, default='<launch-command>'] The command to search for when checking if the application is running. # This will be executed as a prefix search, so if the application was launched using './app-name -loglevel debug' # a search-command './app-name' would work. # Note: same replacement rules as launch-command search-command : \"./binary_name\" # [Optional, default=No] Indicates whether the application needs to be launched automatically never-start : No # [Optional, default=No] Indicates whether the application can be restarted automatically when it crashes. # Applies only when never-start is No (if the app is not started by app_mgr it would not be restarted either). # Applications are only restarted when running app_mgr in restart mode (e.g. sr_linux --restart) never-restart : No # [Optional, default=No] Indicates whether the application will be shown in app manager status never-show : No # [Optional, default=No] Indicates whether the launch of the application is delayed # until any configuration is loaded in the application's YANG modules. wait-for-config : No # [Optional] Indicates the application is run as 'user' including 'root' run-as-user : root # [Optional, default=200] Indicates the order in which the application needs to be launched. # The applications with the lowest value are launched first. # Applications with the same value are launched in an undefined order. # By convention, start-order >= 100 require idb. 1 is reserved for device-mgr, which determines chassis type. start-order : 123 # [Optional, default=No] Indicates whether this application is launched via an request (idb only at this point). launch-by-request : No # [Optional, default=No] Indicates whether this application is launched in a net namespace (launch-by-request # must be set to Yes). launch-in-net-namespace : No # [Optional, default=3] Indicates the number of restarts within failure-window which will trigger the system restart failure-threshold : 3 # [Optional, default=300] Indicates the window in seconds over which to count restarts towards failure-threshold failure-window : 400 # [Optional, default=reboot] Indicates the action taken after 'failure-threshold' failures within 'failure-window' failure-action : 'reboot' # [Optional, default=Nokia] Indicates the author of the application author : 'Nokia' # [Optional, default=\u201d\u201d] The command for app_mgr to run to read the application version version-command : 'snmpd --version' # [Optional The operations that may not be manually performed on this application restricted-operations : [ 'start' , 'stop' , 'restart' , 'quit' , 'kill' ] # [Optional, default No] app-mgr will wait for app to acknowledge it via oob channel oob-init : No # [Optional] The list of launch restrictions - if of all of the restrictions of an element in the list are met, # then the application is launched. The restrictions are separated by a ':'. Valid restrictions are: # 'sim' - running in sim mode (like in container env.) # 'hw' - running on real h/w # 'chassis' - running on a chassis (cpm and imm are running on different processors) # 'imm' - runs on the imm # 'cpm' - runs on the cpm (default) launch-restrictions : [ 'hw:cpm' , 'hw:chassis:imm' ] yang-modules : # [Mandatory] The names of the YANG modules to load. This is usually the file-name without '.yang' names : [ module-name , other-module-name ] # [Optional] List of enabled YANG features. Each needs to be qualified (e.g. srl_nokia-common:foo) enabled-features : [ 'module-name:foo' , 'other-module-name:bar' ] # [Optional] The names of the YANG validation plugins to load. validation-plugins : [ plugin-name , other-plugin-name ] # [Mandatory] All the source-directories where we should search for: # - The YANG modules listed here # - any YANG module included/imported in these modules source-directories : [ /path/one , /path/two ] # [Optional] The names of the not owned YANG modules to load for commit confirmation purposes. not-owned-names : [ module-name , other-module-name ] # [Optional] Multiple applications can be defined in the same YAML file other-application-name : command : \"./other-binary\" path : /other/path","title":"Configuration file"},{"location":"ndk/guide/agent/#dependency-and-other-files","text":"Quite often an agent may require additional files for its operation. It can be a specific virtual environment for your Python agent, or some JSON file that your agents reads some data from. All those auxiliary files can be saved by the /opt/$agentName/ directory. terms NDK agent and NDK app are used interchangeably \u21a9","title":"Dependency and other files"},{"location":"ndk/guide/architecture/","text":"SR Linux provides a Software Development Kit (SDK) to assist operators with developing agents that run alongside SR Linux applications. This SDK is named NetOps Development Kit, or NDK for short. NDK allows operators to write applications (a.k.a agents) that function similar to other applications provided with SR Linux. This is achieved via NDK gRPC service that allows custom applications to interact with other SR Linux applications via Impart Database (IDB). In Fig. 1 custom NDK applications app-1 and app-2 are able to interact with other subsystems of SR Linux operating system via gRPC-based NDK service. Fig 1. NDK applications integration In addition to traditional tasks of reading and writing configuration, custom NDK applications gain low-level access to the SR Linux system, for example to install FIB routes or listen to LLDP events. gRPC & Protocol buffers # NDK uses gRPC - a high-performance open source framework for remote procedure calls. gRPC framework by default uses Protocol buffers as its Interface Definition Language as well as the underlying message exchange format. Info Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client provides the same methods as the server. Fig 2. gRPC client-server interactions Leveraging gRPC and protobufs provides some substantial benefits for NDK users: Language neutrality: NDK apps can be written in any language for which protobuf compiler exists. Go, Python, C, Java, Ruby and more languages are supported by Protocol buffers enabling SR Linux users to write apps in the language of their choice. High-performance: protobuf encoded messaging is an efficient way to exchange data in a client-server environment. Applications which consume high-volume streams of data (for example route updates) benefit from an efficient and fast message delivery enabled by protobuf. Backwards API compatibility: a protobuf design property of using distinctive IDs for data fields makes it possible to evolve API over time without ever breaking backwards compatibility. Old clients will still be able to consume data stored in the original fields, whereas new clients will benefit from accessing data stored in the new fields. NDK Service # NDK provides a collection of gRPC services each of which enables custom applications to interact with a certain subsystem on an SR Linux OS delivering a high level of integration and extensibility. With this architecture, NDK agents act as gRPC clients that execute remote procedure calls (RPC) on a system that implements a gRPC server. On SR Linux, ndk_mgr is the application that runs NDK gRPC server. Fig 3. shows how custom agents interact via gRPC with NDK, and NDK in its turn executes the called procedure and communicates with other system applications through IDB and pub/sub interface to return the result of the RPC to a client. Fig 3. gRPC as an Inter Process Communication (IPC) protocol As a result, custom applications are able to communicate with the native SR Linux apps as if they were shipped with SR Linux OS. Proto files # NDK services, underlying RPCs and messages are defined in .proto files. These files are used to generate language bindings which are essential in building NDK clients as well as serve as the data modeling language for the NDK itself. The source .proto files for NDK are open and published in nokia/srlinux-ndk-protobufs repository. Anyone can clone this repository and explore the NDK gRPC services or build language bindings for the programming language of their choice. Documentation # Although the source proto files themselves are human readable, it is easier to browse the NDK services using the generated documentation that we keep in the same nokia/srlinux-ndk-protobufs repo. The HTML document is linked in the readme file that appears when a user selects a tag that matches the NDK release version 1 . The generated documentation provides the developers with a human readable reference of all the services, messages and types that comprise the NDK service. Operations flow # Regardless of a language in which the agents are written, at a high level the following flow of operations applies to all agents when interacting with the NDK service: Fig 4. NDK operations flow Establish gRPC channel with NDK manager and instantiate an NDK client Register the agent with the NDK manager Register notification streams for different types of NDK services (config, lldp, interface, etc.) Start streaming notifications Handle the streamed notifications Update agent's state data if needed Exit gracefully if required To better understand the operations flow that agents use when talking to NDK we will explain each step in a language-abstracted manner, for language specific implementations go to \"Developing with NDK\" chapter. gRPC Channel and NDK Manager Client # NDK agents communicate with gRPC based NDK service by invoking RPCs and handling responses. An RPC generally takes in a client request message and returns a response message from the server. To call service methods, a gRPC channel needs to be established prior to communicating with the NDK manager application running on SR Linux 2 . NDK server runs on port 50053 , hence agents which are installed on SR Linux OS use localhost:50053 socket to establish gRPC channel. Once the gRPC channel is setup, an a gRPC client (often called stub ) needs to be created to perform RPCs. Each gRPC service needs to have its own client. In NDK the SdkMgrService service is the first service that agents interact with, hence first users need to create NDK Manager Client (Mgr Client on diagram) that will be able to call RPCs defined for SdkMgrService . Agent registration # Agent must be first registered with SRLinux NDK by calling AgentRegister RPC of SdkMgrService . Initial agent state is created during the registration process. An AgentRegistrationResponse is returned back (omitted in the Fig. 4) with the status of the registration process. Registering notifications # Agents interact with other services like Network Instance, Config, LLDP, BFD by subscribing to notification updates from these services. Before subscribing to a notification stream of a certain service the subscription stream needs to be created. To create it, a client of SdkMgrService calls NotificationRegister RPC with NotificationRegistrationRequest field Op set to Create and other fields absent. Info NotificationRegistrationRequest message's field Op (for Operation) may have one of the following values: Create creates a subscription stream and returns a StreamId that is used when adding subscriptions with the AddSubscription operation. Delete deletes the existing subscription stream that has a particular SubId . AddSubscription adds a subscription. The stream will now be able to stream notifications of that subscription type (e.g. Intf, NwInst, etc). DeleteSubscription deletes the previously added subscription. When Op field is set to Create , NDK Manager responds with NotificationRegisterResponse message with stream_id field set to some value. This indicates that the stream has been created and subscriptions now can be added to the created stream. To subscribe to a certain service notification updates another call of NotificationRegister RPC is made with the following fields set: stream_id set to an obtained value from the NotificationRegisterResponse Op is set to AddSubscription one of the subscription_types is set according to the desired service notifications. For example, if notifications from the Config service are of interest, then config field of type ConfigSubscriptionRequest is set. NotificationRegisterResponse message follows the request and contains the same stream_id but now also the sub_id field - subscription identifier. At this point agent successfully indicated its desire to receive notifications from certain services, but the notification streams haven't been started yet. Streaming notifications # Requesting applications to send notifications is done by interfacing with SdkNotificationService . As this is another gRPC service, it requires its own client - Notification client - to be used to execute RPCs. To initiate streaming of updates based on the agent subscriptions the Notification Client executes NotificationStream RPC which has [ NotificationStreamRequest ][notif_stream_req_doc] message with stream_id field set to the ID of a stream to be used. This RPC returns a stream of NotificationStreamResponse , which makes this RPC of type \"server streaming RPC\". Server streaming RPC A server-streaming RPC is similar to a unary RPC, except that the server returns a stream of messages in response to a client\u2019s request. After sending all its messages, the server\u2019s status details (status code and optional status message) and optional trailing metadata are sent to the client. This completes processing on the server side. The client completes once it has all the server\u2019s messages. NotificationStreamResponse message represents notification stream response that contains one or more notification. The Notification message contains one of the subscription_types notifications, which will be set in accordance to what notifications were subscribed by the agent. In our example, we sent ConfigSubscriptionRequest inside the NotificationRegisterRequest , hence the notifications that we will get back for that stream_id will contain ConfigNotification messages inside Notification of a NotificationStreamResponse . Handling notifications # The agent handles the stream of notifications by analyzing which concrete type of notification was read from the stream. The Server streaming RPC will provide notifications till the last one available, the agent then reads out the incoming notifications and handles the messages contained within them. The handling of notifications is done when the last notification is sent by the server. At this point the agent may perform some work on the received data and, if needed, update the agent's state if it has one. Updating agent's state data # Each agent may keep state and configuration data which is modeled in YANG. When agent needs to set/update its own state data (for example when it made some calculations based on received notifications), it needs to use SdkMgrTelemetryService and a corresponding client. Fig 5. Updating agent's state flow The state that agent intends to have will be available for gNMI telemetry, CLI access and JSON-RPC retrieval, as it essentially becomes part of the SR Linux state. Updating or initializing agent's state with data is done with TelemetryAddOrUpdate RPC that has a request of type TelemetryUpdateRequest that encloses a list of TelemetryInfo messages. Each TelemetryInfo message contains a key field that points to a subtree of agent's YANG model that needs to be updated with the JSON data contained within data field. Exiting gracefully # When agent needs to stop it's operation and be removed from the SR Linux system, it needs to be Unregistered, by invoking AgentUnRegister RPC of the SdkMgrService . The gRPC connection to the NDK server needs to be closed. When unregistered, the agent's state data will be removed from SR Linux system and will no longer be accessible to any of the management interfaces. For example, here you will find the auto-generated documentation for the latest NDK version at the moment of this writing. \u21a9 ndk_mgr is the name of the applications that implements NDK gRPC server side and runs on SR Linux OS. \u21a9","title":"NDK Architecture"},{"location":"ndk/guide/architecture/#grpc-protocol-buffers","text":"NDK uses gRPC - a high-performance open source framework for remote procedure calls. gRPC framework by default uses Protocol buffers as its Interface Definition Language as well as the underlying message exchange format. Info Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client provides the same methods as the server. Fig 2. gRPC client-server interactions Leveraging gRPC and protobufs provides some substantial benefits for NDK users: Language neutrality: NDK apps can be written in any language for which protobuf compiler exists. Go, Python, C, Java, Ruby and more languages are supported by Protocol buffers enabling SR Linux users to write apps in the language of their choice. High-performance: protobuf encoded messaging is an efficient way to exchange data in a client-server environment. Applications which consume high-volume streams of data (for example route updates) benefit from an efficient and fast message delivery enabled by protobuf. Backwards API compatibility: a protobuf design property of using distinctive IDs for data fields makes it possible to evolve API over time without ever breaking backwards compatibility. Old clients will still be able to consume data stored in the original fields, whereas new clients will benefit from accessing data stored in the new fields.","title":"gRPC &amp; Protocol buffers"},{"location":"ndk/guide/architecture/#ndk-service","text":"NDK provides a collection of gRPC services each of which enables custom applications to interact with a certain subsystem on an SR Linux OS delivering a high level of integration and extensibility. With this architecture, NDK agents act as gRPC clients that execute remote procedure calls (RPC) on a system that implements a gRPC server. On SR Linux, ndk_mgr is the application that runs NDK gRPC server. Fig 3. shows how custom agents interact via gRPC with NDK, and NDK in its turn executes the called procedure and communicates with other system applications through IDB and pub/sub interface to return the result of the RPC to a client. Fig 3. gRPC as an Inter Process Communication (IPC) protocol As a result, custom applications are able to communicate with the native SR Linux apps as if they were shipped with SR Linux OS.","title":"NDK Service"},{"location":"ndk/guide/architecture/#proto-files","text":"NDK services, underlying RPCs and messages are defined in .proto files. These files are used to generate language bindings which are essential in building NDK clients as well as serve as the data modeling language for the NDK itself. The source .proto files for NDK are open and published in nokia/srlinux-ndk-protobufs repository. Anyone can clone this repository and explore the NDK gRPC services or build language bindings for the programming language of their choice.","title":"Proto files"},{"location":"ndk/guide/architecture/#documentation","text":"Although the source proto files themselves are human readable, it is easier to browse the NDK services using the generated documentation that we keep in the same nokia/srlinux-ndk-protobufs repo. The HTML document is linked in the readme file that appears when a user selects a tag that matches the NDK release version 1 . The generated documentation provides the developers with a human readable reference of all the services, messages and types that comprise the NDK service.","title":"Documentation"},{"location":"ndk/guide/architecture/#operations-flow","text":"Regardless of a language in which the agents are written, at a high level the following flow of operations applies to all agents when interacting with the NDK service: Fig 4. NDK operations flow Establish gRPC channel with NDK manager and instantiate an NDK client Register the agent with the NDK manager Register notification streams for different types of NDK services (config, lldp, interface, etc.) Start streaming notifications Handle the streamed notifications Update agent's state data if needed Exit gracefully if required To better understand the operations flow that agents use when talking to NDK we will explain each step in a language-abstracted manner, for language specific implementations go to \"Developing with NDK\" chapter.","title":"Operations flow"},{"location":"ndk/guide/architecture/#grpc-channel-and-ndk-manager-client","text":"NDK agents communicate with gRPC based NDK service by invoking RPCs and handling responses. An RPC generally takes in a client request message and returns a response message from the server. To call service methods, a gRPC channel needs to be established prior to communicating with the NDK manager application running on SR Linux 2 . NDK server runs on port 50053 , hence agents which are installed on SR Linux OS use localhost:50053 socket to establish gRPC channel. Once the gRPC channel is setup, an a gRPC client (often called stub ) needs to be created to perform RPCs. Each gRPC service needs to have its own client. In NDK the SdkMgrService service is the first service that agents interact with, hence first users need to create NDK Manager Client (Mgr Client on diagram) that will be able to call RPCs defined for SdkMgrService .","title":"gRPC Channel and NDK Manager Client"},{"location":"ndk/guide/architecture/#agent-registration","text":"Agent must be first registered with SRLinux NDK by calling AgentRegister RPC of SdkMgrService . Initial agent state is created during the registration process. An AgentRegistrationResponse is returned back (omitted in the Fig. 4) with the status of the registration process.","title":"Agent registration"},{"location":"ndk/guide/architecture/#registering-notifications","text":"Agents interact with other services like Network Instance, Config, LLDP, BFD by subscribing to notification updates from these services. Before subscribing to a notification stream of a certain service the subscription stream needs to be created. To create it, a client of SdkMgrService calls NotificationRegister RPC with NotificationRegistrationRequest field Op set to Create and other fields absent. Info NotificationRegistrationRequest message's field Op (for Operation) may have one of the following values: Create creates a subscription stream and returns a StreamId that is used when adding subscriptions with the AddSubscription operation. Delete deletes the existing subscription stream that has a particular SubId . AddSubscription adds a subscription. The stream will now be able to stream notifications of that subscription type (e.g. Intf, NwInst, etc). DeleteSubscription deletes the previously added subscription. When Op field is set to Create , NDK Manager responds with NotificationRegisterResponse message with stream_id field set to some value. This indicates that the stream has been created and subscriptions now can be added to the created stream. To subscribe to a certain service notification updates another call of NotificationRegister RPC is made with the following fields set: stream_id set to an obtained value from the NotificationRegisterResponse Op is set to AddSubscription one of the subscription_types is set according to the desired service notifications. For example, if notifications from the Config service are of interest, then config field of type ConfigSubscriptionRequest is set. NotificationRegisterResponse message follows the request and contains the same stream_id but now also the sub_id field - subscription identifier. At this point agent successfully indicated its desire to receive notifications from certain services, but the notification streams haven't been started yet.","title":"Registering notifications"},{"location":"ndk/guide/architecture/#streaming-notifications","text":"Requesting applications to send notifications is done by interfacing with SdkNotificationService . As this is another gRPC service, it requires its own client - Notification client - to be used to execute RPCs. To initiate streaming of updates based on the agent subscriptions the Notification Client executes NotificationStream RPC which has [ NotificationStreamRequest ][notif_stream_req_doc] message with stream_id field set to the ID of a stream to be used. This RPC returns a stream of NotificationStreamResponse , which makes this RPC of type \"server streaming RPC\". Server streaming RPC A server-streaming RPC is similar to a unary RPC, except that the server returns a stream of messages in response to a client\u2019s request. After sending all its messages, the server\u2019s status details (status code and optional status message) and optional trailing metadata are sent to the client. This completes processing on the server side. The client completes once it has all the server\u2019s messages. NotificationStreamResponse message represents notification stream response that contains one or more notification. The Notification message contains one of the subscription_types notifications, which will be set in accordance to what notifications were subscribed by the agent. In our example, we sent ConfigSubscriptionRequest inside the NotificationRegisterRequest , hence the notifications that we will get back for that stream_id will contain ConfigNotification messages inside Notification of a NotificationStreamResponse .","title":"Streaming notifications"},{"location":"ndk/guide/architecture/#handling-notifications","text":"The agent handles the stream of notifications by analyzing which concrete type of notification was read from the stream. The Server streaming RPC will provide notifications till the last one available, the agent then reads out the incoming notifications and handles the messages contained within them. The handling of notifications is done when the last notification is sent by the server. At this point the agent may perform some work on the received data and, if needed, update the agent's state if it has one.","title":"Handling notifications"},{"location":"ndk/guide/architecture/#updating-agents-state-data","text":"Each agent may keep state and configuration data which is modeled in YANG. When agent needs to set/update its own state data (for example when it made some calculations based on received notifications), it needs to use SdkMgrTelemetryService and a corresponding client. Fig 5. Updating agent's state flow The state that agent intends to have will be available for gNMI telemetry, CLI access and JSON-RPC retrieval, as it essentially becomes part of the SR Linux state. Updating or initializing agent's state with data is done with TelemetryAddOrUpdate RPC that has a request of type TelemetryUpdateRequest that encloses a list of TelemetryInfo messages. Each TelemetryInfo message contains a key field that points to a subtree of agent's YANG model that needs to be updated with the JSON data contained within data field.","title":"Updating agent's state data"},{"location":"ndk/guide/architecture/#exiting-gracefully","text":"When agent needs to stop it's operation and be removed from the SR Linux system, it needs to be Unregistered, by invoking AgentUnRegister RPC of the SdkMgrService . The gRPC connection to the NDK server needs to be closed. When unregistered, the agent's state data will be removed from SR Linux system and will no longer be accessible to any of the management interfaces. For example, here you will find the auto-generated documentation for the latest NDK version at the moment of this writing. \u21a9 ndk_mgr is the name of the applications that implements NDK gRPC server side and runs on SR Linux OS. \u21a9","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/","text":"Developing agents with NDK in Go # This guide explains how NDK service is meant to be consumed when developers writing agents in Go 1 programming language. Note This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter. Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations. In addition to the publicly available protobuf files which define the NDK Service, Nokia also provides generated Go bindings for data access classes of NDK in a nokia/srlinux-ndk-go repo. The github.com/nokia/srlinux-ndk-go/v21/ndk package provided in that repository enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Go package themselves. Establish gRPC channel with NDK manager and instantiate an NDK client # Additional information To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux. This is done by passing the NDK server address - localhost:50053 - to grpc.Dial() as follows: import ( \"google.golang.org/grpc\" ) conn , err := grpc . Dial ( \"localhost:50053\" , grpc . WithInsecure ()) if err != nil { ... } defer conn . Close () Once the gRPC channel is setup, we need to instantiate a client (often called stub ) to perform RPCs. The client is obtained using the NewSdkMgrServiceClient method provided. import \"github.com/nokia/srlinux-ndk-go/v21/ndk\" client := NewSdkMgrServiceClient ( conn ) Register the agent with the NDK manager: # Additional information Agent must be first registered with SRLinux by calling the AgentRegister method available on the returned SdkMgrServiceClient interface. Initial agent state is created during the registration process. Agent's context # Go context is a required parameter for each RPC service method. It provides a means of enforcing deadlines and cancellations but also a way to transmit metadata within the request. To register the agent, metadata must be passed within the ctx context. During registration, SRLinux will be expecting a key-value pair with a key of \"agent_name\" and a value of the agent's name. The agent name is defined in the agent's yml file. Warning Not including this metadata in the agent ctx would result in a agent registration failure. SRLinux would not be able to differentiate between two agents both connected to the same NDK manager. ctx , cancel := context . WithCancel ( context . Background ()) defer cancel () // appending agent's name to the context metadata ctx = metadata . AppendToOutgoingContext ( ctx , \"agent_name\" , \"ndkDemo\" ) Agent registration # AgentRegister method takes in the context ctx that is by now has agent name as its metadata and an AgentRegistrationRequest . AgentRegistrationRequest structure can be passed in with its default values for a basic registration request. import \"github.com/nokia/srlinux-ndk-go/v21/ndk\" r , err := client . AgentRegister ( ctx , & ndk . AgentRegistrationRequest {}) if err != nil { log . Fatalf ( \"agent registration failed: %v\" , err ) } AgentRegister method returns AgentRegistrationResponse and an error. Response can be additionally checked for status and error description. Register notification streams # Additional information Create subscription stream # A subscription stream needs to be created first before any subscription types can be added. SdkMgrServiceClient first creates the subscription stream by executing NotificationRegister method with a NotificationRegisterRequest only field Op set to a value of const NotificationRegisterRequest_Create . This effectively creates a stream which is identified with a StreamID returned inside the NotificationRegisterResponse . StreamId must be associated when subscribing/unsubscribing to certain types of router notifications. req := & ndk . NotificationRegisterRequest { Op : ndk . NotificationRegisterRequest_Create , } resp , err := client . NotificationRegister ( r . ctx , req ) if err != nil { log . Fatalf ( \"Notification Register failed with error: %v\" , err ) } else if resp . GetStatus () == ndk . SdkMgrStatus_kSdkMgrFailed { r . log . Fatalf ( \"Notification Register failed with status %d\" , resp . GetStatus ()) } log . Debugf ( \"Notification Register was successful: StreamID: %d SubscriptionID: %d\" , resp . GetStreamId (), resp . GetSubId ()) } Add notification subscriptions # Once the StreamId is acquired, a client can register notifications of a particular type to be delivered over that stream. Different types of notifications types can be subscribed to by calling the same NotificationRegister method with a NotificationRegisterRequest having Op field set to NotificationRegisterRequest_AddSubscription and certain SubscriptionType selected. In the example below we would like to receive notifications from the Config service, hence we specify NotificationRegisterRequest_Config subscription type. subType := & ndk . NotificationRegisterRequest_Config { // This is unique to each notification type (Config, Intf, etc.). Config : & ndk . ConfigSubscriptionRequest {}, } req := & ndk . NotificationRegisterRequest { StreamId : resp . GetStreamId (), // StreamId is retrieved from the NotificationRegisterResponse Op : ndk . NotificationRegisterRequest_AddSubscription , SubscriptionTypes : subType , } resp , err := r . mgrStub . NotificationRegister ( r . ctx , req ) if err != nil { log . Fatalf ( \"Agent could not subscribe for config notification\" ) } else if resp . GetStatus () == ndk . SdkMgrStatus_kSdkMgrFailed { log . Fatalf ( \"Agent could not subscribe for config notification with status %d\" , resp . GetStatus ()) } log . Infof ( \"Agent was able to subscribe for config notification with status %d\" , resp . GetStatus ()) Streaming notifications # Additional information Actual streaming of notifications is a task for another service - SdkNotificationService . This service requires developers to create its own client, which is done with NewSdkNotificationServiceClient function. The returned SdkNotificationServiceClient interface has a single method NotificationStream that is used to start streaming notifications. NotificationsStream is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request. To tell the server to start streaming notifications that were subscribed to before the NewSdkNotificationServiceClient executes NotificationsStream method where NotificationStreamRequest struct has its StreamId field set to the value that was obtained at subscription stage. req := & ndk . NotificationStreamRequest { StreamId : resp . GetStreamId (), } streamResp , err := notifClient . NotificationStream ( ctx , req ) if err != nil { log . Fatal ( \"Agent failed to create stream client with error: \" , err ) } Handle the streamed notifications # Additional information Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is the pseudocode that illustrates the flow: func HandleNotifications ( stream ndk . SdkNotificationService_NotificationStreamClient ) { for { // loop until stream returns io.EoF notification stream response ( nsr ) := stream . Recv () for notif in nsr . Notification { // nsr.Notification is a slice of `Notification` if notif . GetConfig () is not nil { 1. config notif = notif . GetConfig () 2. handle config notif } else if notif . GetIntf () is not nil { 1. intf notif = notif . GetIntf () 2. handle intf notif } ... // Do this if statement for every notification type the agent is subscribed to } } } NotificationStream method of the SdkNotificationServiceClient interface will return a stream client SdkNotificationService_NotificationStreamClient . SdkNotificationService_NotificationStreamClient contains a Recv() to retrieve notifications one by one. When an end of the stream is found, Rev() will return io.EOF . Recv() returns a *NotificationStreamResponse which contains a slice of Notification . Notification struct has GetXXX() methods defined which retrieve the notification of a specific type. For example: GetConfig returns ConfigNotification . Note ConfigNotification is returned only if Notification struct has a certain subscription type set for its SubscriptionType field. Otherwise, GetConfig returns nil . Once the specific XXXNotification has been extracted using the GetXXX() method, users can access the fields of the notification and process the data contained within the notification using GetKey() and GetData() methods. Exiting gracefully # Agent needs to handle SIGTERM signal that is sent when a user invokes stop command via SR Linux CLI. The following is the required steps to cleanly stop the agent: Remove any agent's state if it was set using TelemetryDelete method of a Telemetry client. Delete notification subscriptions stream NotificationRegister method with Op set to NotificationRegisterRequest_Delete . Invoke use AgentUnRegister() method of a SdkMgrServiceClient interface. Close gRPC channel with the sdk_mgr . Logging # To debug an agent the developers can analyze the log messages that agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at directory /var/log/srlinux/stdout/ . The default SR Linux debug messages is found in the messages directory /var/log/srlinux/buffer/messages and can be useful when something went wrong within the SRLinux system itself (agent registration failed, IDB server warning messages, etc). logrus is a popular structured logger for Go that can log messages of different levels of importance, but developers are free to choose whatever logging package they see fit. Make sure that you have set up the dev environment as explain on this page . Readers are also encouraged to first go through the gRPC basic tutorial to get familiar with the common gRPC workflows when using Go. \u21a9","title":"Go"},{"location":"ndk/guide/dev/go/#developing-agents-with-ndk-in-go","text":"This guide explains how NDK service is meant to be consumed when developers writing agents in Go 1 programming language. Note This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter. Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations. In addition to the publicly available protobuf files which define the NDK Service, Nokia also provides generated Go bindings for data access classes of NDK in a nokia/srlinux-ndk-go repo. The github.com/nokia/srlinux-ndk-go/v21/ndk package provided in that repository enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Go package themselves.","title":"Developing agents with NDK in Go"},{"location":"ndk/guide/dev/go/#establish-grpc-channel-with-ndk-manager-and-instantiate-an-ndk-client","text":"Additional information To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux. This is done by passing the NDK server address - localhost:50053 - to grpc.Dial() as follows: import ( \"google.golang.org/grpc\" ) conn , err := grpc . Dial ( \"localhost:50053\" , grpc . WithInsecure ()) if err != nil { ... } defer conn . Close () Once the gRPC channel is setup, we need to instantiate a client (often called stub ) to perform RPCs. The client is obtained using the NewSdkMgrServiceClient method provided. import \"github.com/nokia/srlinux-ndk-go/v21/ndk\" client := NewSdkMgrServiceClient ( conn )","title":"Establish gRPC channel with NDK manager and instantiate an NDK client"},{"location":"ndk/guide/dev/go/#register-the-agent-with-the-ndk-manager","text":"Additional information Agent must be first registered with SRLinux by calling the AgentRegister method available on the returned SdkMgrServiceClient interface. Initial agent state is created during the registration process.","title":"Register the agent with the NDK manager:"},{"location":"ndk/guide/dev/go/#agents-context","text":"Go context is a required parameter for each RPC service method. It provides a means of enforcing deadlines and cancellations but also a way to transmit metadata within the request. To register the agent, metadata must be passed within the ctx context. During registration, SRLinux will be expecting a key-value pair with a key of \"agent_name\" and a value of the agent's name. The agent name is defined in the agent's yml file. Warning Not including this metadata in the agent ctx would result in a agent registration failure. SRLinux would not be able to differentiate between two agents both connected to the same NDK manager. ctx , cancel := context . WithCancel ( context . Background ()) defer cancel () // appending agent's name to the context metadata ctx = metadata . AppendToOutgoingContext ( ctx , \"agent_name\" , \"ndkDemo\" )","title":"Agent's context"},{"location":"ndk/guide/dev/go/#agent-registration","text":"AgentRegister method takes in the context ctx that is by now has agent name as its metadata and an AgentRegistrationRequest . AgentRegistrationRequest structure can be passed in with its default values for a basic registration request. import \"github.com/nokia/srlinux-ndk-go/v21/ndk\" r , err := client . AgentRegister ( ctx , & ndk . AgentRegistrationRequest {}) if err != nil { log . Fatalf ( \"agent registration failed: %v\" , err ) } AgentRegister method returns AgentRegistrationResponse and an error. Response can be additionally checked for status and error description.","title":"Agent registration"},{"location":"ndk/guide/dev/go/#register-notification-streams","text":"Additional information","title":"Register notification streams"},{"location":"ndk/guide/dev/go/#create-subscription-stream","text":"A subscription stream needs to be created first before any subscription types can be added. SdkMgrServiceClient first creates the subscription stream by executing NotificationRegister method with a NotificationRegisterRequest only field Op set to a value of const NotificationRegisterRequest_Create . This effectively creates a stream which is identified with a StreamID returned inside the NotificationRegisterResponse . StreamId must be associated when subscribing/unsubscribing to certain types of router notifications. req := & ndk . NotificationRegisterRequest { Op : ndk . NotificationRegisterRequest_Create , } resp , err := client . NotificationRegister ( r . ctx , req ) if err != nil { log . Fatalf ( \"Notification Register failed with error: %v\" , err ) } else if resp . GetStatus () == ndk . SdkMgrStatus_kSdkMgrFailed { r . log . Fatalf ( \"Notification Register failed with status %d\" , resp . GetStatus ()) } log . Debugf ( \"Notification Register was successful: StreamID: %d SubscriptionID: %d\" , resp . GetStreamId (), resp . GetSubId ()) }","title":"Create subscription stream"},{"location":"ndk/guide/dev/go/#add-notification-subscriptions","text":"Once the StreamId is acquired, a client can register notifications of a particular type to be delivered over that stream. Different types of notifications types can be subscribed to by calling the same NotificationRegister method with a NotificationRegisterRequest having Op field set to NotificationRegisterRequest_AddSubscription and certain SubscriptionType selected. In the example below we would like to receive notifications from the Config service, hence we specify NotificationRegisterRequest_Config subscription type. subType := & ndk . NotificationRegisterRequest_Config { // This is unique to each notification type (Config, Intf, etc.). Config : & ndk . ConfigSubscriptionRequest {}, } req := & ndk . NotificationRegisterRequest { StreamId : resp . GetStreamId (), // StreamId is retrieved from the NotificationRegisterResponse Op : ndk . NotificationRegisterRequest_AddSubscription , SubscriptionTypes : subType , } resp , err := r . mgrStub . NotificationRegister ( r . ctx , req ) if err != nil { log . Fatalf ( \"Agent could not subscribe for config notification\" ) } else if resp . GetStatus () == ndk . SdkMgrStatus_kSdkMgrFailed { log . Fatalf ( \"Agent could not subscribe for config notification with status %d\" , resp . GetStatus ()) } log . Infof ( \"Agent was able to subscribe for config notification with status %d\" , resp . GetStatus ())","title":"Add notification subscriptions"},{"location":"ndk/guide/dev/go/#streaming-notifications","text":"Additional information Actual streaming of notifications is a task for another service - SdkNotificationService . This service requires developers to create its own client, which is done with NewSdkNotificationServiceClient function. The returned SdkNotificationServiceClient interface has a single method NotificationStream that is used to start streaming notifications. NotificationsStream is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request. To tell the server to start streaming notifications that were subscribed to before the NewSdkNotificationServiceClient executes NotificationsStream method where NotificationStreamRequest struct has its StreamId field set to the value that was obtained at subscription stage. req := & ndk . NotificationStreamRequest { StreamId : resp . GetStreamId (), } streamResp , err := notifClient . NotificationStream ( ctx , req ) if err != nil { log . Fatal ( \"Agent failed to create stream client with error: \" , err ) }","title":"Streaming notifications"},{"location":"ndk/guide/dev/go/#handle-the-streamed-notifications","text":"Additional information Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is the pseudocode that illustrates the flow: func HandleNotifications ( stream ndk . SdkNotificationService_NotificationStreamClient ) { for { // loop until stream returns io.EoF notification stream response ( nsr ) := stream . Recv () for notif in nsr . Notification { // nsr.Notification is a slice of `Notification` if notif . GetConfig () is not nil { 1. config notif = notif . GetConfig () 2. handle config notif } else if notif . GetIntf () is not nil { 1. intf notif = notif . GetIntf () 2. handle intf notif } ... // Do this if statement for every notification type the agent is subscribed to } } } NotificationStream method of the SdkNotificationServiceClient interface will return a stream client SdkNotificationService_NotificationStreamClient . SdkNotificationService_NotificationStreamClient contains a Recv() to retrieve notifications one by one. When an end of the stream is found, Rev() will return io.EOF . Recv() returns a *NotificationStreamResponse which contains a slice of Notification . Notification struct has GetXXX() methods defined which retrieve the notification of a specific type. For example: GetConfig returns ConfigNotification . Note ConfigNotification is returned only if Notification struct has a certain subscription type set for its SubscriptionType field. Otherwise, GetConfig returns nil . Once the specific XXXNotification has been extracted using the GetXXX() method, users can access the fields of the notification and process the data contained within the notification using GetKey() and GetData() methods.","title":"Handle the streamed notifications"},{"location":"ndk/guide/dev/go/#exiting-gracefully","text":"Agent needs to handle SIGTERM signal that is sent when a user invokes stop command via SR Linux CLI. The following is the required steps to cleanly stop the agent: Remove any agent's state if it was set using TelemetryDelete method of a Telemetry client. Delete notification subscriptions stream NotificationRegister method with Op set to NotificationRegisterRequest_Delete . Invoke use AgentUnRegister() method of a SdkMgrServiceClient interface. Close gRPC channel with the sdk_mgr .","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/#logging","text":"To debug an agent the developers can analyze the log messages that agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at directory /var/log/srlinux/stdout/ . The default SR Linux debug messages is found in the messages directory /var/log/srlinux/buffer/messages and can be useful when something went wrong within the SRLinux system itself (agent registration failed, IDB server warning messages, etc). logrus is a popular structured logger for Go that can log messages of different levels of importance, but developers are free to choose whatever logging package they see fit. Make sure that you have set up the dev environment as explain on this page . Readers are also encouraged to first go through the gRPC basic tutorial to get familiar with the common gRPC workflows when using Go. \u21a9","title":"Logging"},{"location":"ndk/guide/env/go/","text":"Go Development Environment # Although every developer's environment is different and is subject to a personal preference, on this page we will provide recommendations for a Go toolchain setup suitable for development and build of NDK applications. Environment components # The toolchain that can be used to develop and build Go based NDK apps consists of the following components: Go programming language - Go compiler, toolchain and standard library Go NDK bindings - generated data access classes for gRPC based NDK service. Goreleaser - Go-focused build & release pipeline runner. Packages nFPM to produce rpm packages that can be used to install NDK agents . Project structure # It is recommended to use Go modules when developing applications with Go. Go modules allow for better dependency management and can be placed outside of the $GOPATH directory. Here is an example project structure that you can use for the NDK agent development: . # Root of a project \u251c\u2500\u2500 app # Contains agent core logic \u251c\u2500\u2500 yang # A directory with agent YANG modules \u251c\u2500\u2500 agent.yml # Agent yml config file \u251c\u2500\u2500 .goreleaser.yml # Goreleaser config file \u251c\u2500\u2500 main.go # Package main that calls agent logic \u251c\u2500\u2500 go.mod # Go mod file \u251c\u2500\u2500 go.sum # Go sum file NDK language bindings # As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Go program the language bindings have to be generated from the source proto files. Nokia provides not only the source proto files for the SR Linux NDK service, but also NDK Go language bindings . With the provided Go bindings, the NDK can be imported in a Go project like that: import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"","title":"Go"},{"location":"ndk/guide/env/go/#go-development-environment","text":"Although every developer's environment is different and is subject to a personal preference, on this page we will provide recommendations for a Go toolchain setup suitable for development and build of NDK applications.","title":"Go Development Environment"},{"location":"ndk/guide/env/go/#environment-components","text":"The toolchain that can be used to develop and build Go based NDK apps consists of the following components: Go programming language - Go compiler, toolchain and standard library Go NDK bindings - generated data access classes for gRPC based NDK service. Goreleaser - Go-focused build & release pipeline runner. Packages nFPM to produce rpm packages that can be used to install NDK agents .","title":"Environment components"},{"location":"ndk/guide/env/go/#project-structure","text":"It is recommended to use Go modules when developing applications with Go. Go modules allow for better dependency management and can be placed outside of the $GOPATH directory. Here is an example project structure that you can use for the NDK agent development: . # Root of a project \u251c\u2500\u2500 app # Contains agent core logic \u251c\u2500\u2500 yang # A directory with agent YANG modules \u251c\u2500\u2500 agent.yml # Agent yml config file \u251c\u2500\u2500 .goreleaser.yml # Goreleaser config file \u251c\u2500\u2500 main.go # Package main that calls agent logic \u251c\u2500\u2500 go.mod # Go mod file \u251c\u2500\u2500 go.sum # Go sum file","title":"Project structure"},{"location":"ndk/guide/env/go/#ndk-language-bindings","text":"As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Go program the language bindings have to be generated from the source proto files. Nokia provides not only the source proto files for the SR Linux NDK service, but also NDK Go language bindings . With the provided Go bindings, the NDK can be imported in a Go project like that: import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"","title":"NDK language bindings"},{"location":"ndk/guide/env/python/","text":"Python Development Environment # Although every developer's environment is different and is subject to a personal preference, on this page we will provide recommendations for a Python toolchain setup suitable for development of NDK applications. Environment components # The toolchain that can be used to develop Python based NDK apps consists of the following components: Python programming language - Python interpreter, toolchain and standard library Python NDK bindings - generated data access classes for gRPC based NDK service. Project structure # Here is an example project structure that you can use for the NDK agent development: . # Root of a project \u251c\u2500\u2500 app # Contains agent core logic \u251c\u2500\u2500 yang # A directory with agent YANG modules \u251c\u2500\u2500 agent.yml # Agent yml config file \u251c\u2500\u2500 main.py # Package main that calls agent logic \u251c\u2500\u2500 requirements.txt # Python packages required by the app logic NDK language bindings # As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Python program the language bindings have to be generated from the source proto files. Nokia provides not only the source proto files for the SR Linux NDK service, but also NDK Python language bindings . With the provided Python bindings, the NDK can be installed with pip # install the specific version (example given for v21.6.2) pip install https://github.com/nokia/srlinux-ndk-py/archive/v21.6.2.zip and imported in a Python project like that: from ndk import appid_service_pb2","title":"Python"},{"location":"ndk/guide/env/python/#python-development-environment","text":"Although every developer's environment is different and is subject to a personal preference, on this page we will provide recommendations for a Python toolchain setup suitable for development of NDK applications.","title":"Python Development Environment"},{"location":"ndk/guide/env/python/#environment-components","text":"The toolchain that can be used to develop Python based NDK apps consists of the following components: Python programming language - Python interpreter, toolchain and standard library Python NDK bindings - generated data access classes for gRPC based NDK service.","title":"Environment components"},{"location":"ndk/guide/env/python/#project-structure","text":"Here is an example project structure that you can use for the NDK agent development: . # Root of a project \u251c\u2500\u2500 app # Contains agent core logic \u251c\u2500\u2500 yang # A directory with agent YANG modules \u251c\u2500\u2500 agent.yml # Agent yml config file \u251c\u2500\u2500 main.py # Package main that calls agent logic \u251c\u2500\u2500 requirements.txt # Python packages required by the app logic","title":"Project structure"},{"location":"ndk/guide/env/python/#ndk-language-bindings","text":"As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Python program the language bindings have to be generated from the source proto files. Nokia provides not only the source proto files for the SR Linux NDK service, but also NDK Python language bindings . With the provided Python bindings, the NDK can be installed with pip # install the specific version (example given for v21.6.2) pip install https://github.com/nokia/srlinux-ndk-py/archive/v21.6.2.zip and imported in a Python project like that: from ndk import appid_service_pb2","title":"NDK language bindings"},{"location":"tutorials/about/","text":"Learning by doing is not only the most effective method, but also an extremely fun one. The hands-on tutorials we provide in this section are designed in such a way that anyone can launch them at absolutely no cost whenever they want it whatever machine they have and run it for as long as required The tutorials use the opensource containerlab project to deploy the lab environment with all the needed components. This ensures that both the tutorial authors and the readers work on exactly the same environment. No more second guessing why the tutorial's outputs differ from yours!","title":"SR Linux tutorials"},{"location":"tutorials/l2evpn/evpn/","text":"Ethernet Virtual Private Network (EVPN), along with Virtual eXtensible LAN (VXLAN), is a technology that allows Layer 2 and Layer 3 traffic to be tunneled across an IP network. The SR Linux EVPN-VXLAN solution enables Layer 2 Broadcast Domains (BDs) in multi-tenant data centers using EVPN for the control plane and VXLAN as the data plane. It includes the following features: EVPN for VXLAN tunnels (Layer 2), extending a BD in overlay multi-tenant DCs EVPN for VXLAN tunnels (Layer 3), allowing inter-subnet-forwarding for unicast traffic within the same tenant infrastructure This tutorial is focused on EVPN for VXLAN tunnels Layer 2. Overview # EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch. VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently. The following figure demonstrates this concept where servers srv1 and srv2 are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain. Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable 1 , we can proceed with the VXLAN based EVPN service configuration. While doing that we will cover the following topics: VXLAN tunnel interface configuration Network instances of type mac-vrf Bridged subinterfaces and BGP EVPN control plane configuration IBGP for EVPN # Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches. For that iBGP configuration we will create a group called iBGP-overlay which will have the peer-as and local-as set to 100 to form an iBGP neighborship. The group will also host the same permissive all routing policy, enabled evpn and disabled ipv4-unicast address families. Then for each leaf we add a new BGP neighbor addressed by the remote system0 interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config: leaf1 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { peer-group iBGP-overlay transport { local-address 10.0.0.1 } } commit now leaf2 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { peer-group iBGP-overlay transport { local-address 10.0.0.2 } } commit now Ensure that the iBGP session is established before proceeding any further: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [0/0/0] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above. Access interfaces # Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches: Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be bridged , this makes the interfaces only attachable to a network instance of mac-vrf type with MAC learning and layer-2 forwarding enabled. The following config is applied to both leaf switches: enter candidate /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } commit now As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers. Tunnel/VXLAN interface # After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric. The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name vxlan<N> where N = 0..255 . A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI). The index can be a number in the range 0-4294967295. The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed). The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of 1..16777215 . The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes. SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets. Note The source IP used in the vxlan-interfaces is the IPv4 address of subinterface system0.0 in the default network-instance. The above information translates to a configuration snippet which is applicable both to leaf1 and leaf2 nodes. enter candidate /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } commit now To verify the tunnel interface configuration: A:leaf2# show tunnel-interface vxlan-interface brief --------------------------------------------------------------------------------- Show report for vxlan-tunnels --------------------------------------------------------------------------------- +------------------+-----------------+---------+-------------+------------------+ | Tunnel Interface | VxLAN Interface | Type | Ingress VNI | Egress source-ip | +==================+=================+=========+=============+==================+ | vxlan1 | vxlan1.1 | bridged | 1 | 10.0.0.2/32 | +------------------+-----------------+---------+-------------+------------------+ --------------------------------------------------------------------------------- Summary 1 tunnel-interfaces, 1 vxlan interfaces 0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip --------------------------------------------------------------------------------- MAC-VRF # Now it is a turn of MAC-VRF to get configured. The network-instance type mac-vrf functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration. By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance: enter candidate /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } } commit now Server interfaces # The servers in our fabric do not have any addresses on their eth1 interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration. By the end of this section, we will have the following addressing scheme complete: To connect to a shell of a server execute docker exec -it <container-name> bash : srv1 docker exec -it clab-evpn01-srv1 bash srv2 docker exec -it clab-evpn01-srv2 bash Within the shell, configure MAC address 2 and IPv4 address for the eth1 interface according to the diagram above, as with this interface the server is connected to the leaf switch. srv1 ip link set address 00:c1:ab:00:00:01 dev eth1 ip addr add 192.168.0.1/24 dev eth1 srv2 ip link set address 00:c1:ab:00:00:02 dev eth1 ip addr add 192.168.0.2/24 dev eth1 Let's try to ping server2 from server1: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 2028ms That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the srv1 interface MAC to get learned by the leaf1 mac-vrf network instance: A:leaf1# show network-instance vrf-1 bridge-table mac-table all ---------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ---------------------------------------------------------------------------------------------------------------------- +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+==========================+===========+========+========+=======+==========================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 242 | 2021-07-13T17:36:23.000Z | +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 1 Total 1 Active ---------------------------------------------------------------------------------------------------------------------- EVPN in MAC-VRF # To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our vrf-1 network-instance. EVPN configuration under the mac-vrf network instance will require two configuration containers: bgp-vpn - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist. bgp-evpn - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to bgp-instance 1 is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI). The following configuration is entered on both leafs: enter candidate /network-instance vrf-1 protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } commit now Once configured, the bgp-vpn instance can be checked to have the RT/RD values set: A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1 ===================================================================== Net Instance : vrf-1 bgp Instance 1 --------------------------------------------------------------------- route-distinguisher: 10.0.0.1:111, auto-derived-from-evi export-route-target: target:100:111, manual import-route-target: target:100:111, manual ===================================================================== VNI to EVI mapping As of release 21.6, SR Linux uses only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI. 5 Final configurations # For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration 6 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks. pastable snippets leaf1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.1 } } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } /interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } commit now leaf2 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.2 } } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } commit now spine1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /network-instance default { interface ethernet-1/1.0 { } interface ethernet-1/2.0 { } interface system0.0 { } protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-as 101 peer-group eBGP-underlay } neighbor 192.168.12.1 { peer-as 102 peer-group eBGP-underlay } } } } /interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } commit now srv1 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv1 bash ip link set address 00 :c1:ab:00:00:01 dev eth1 ip addr add 192 .168.0.1/24 dev eth1 srv2 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv2 bash ip link set address 00 :c1:ab:00:00:02 dev eth1 ip addr add 192 .168.0.2/24 dev eth1 Verification # EVPN IMET routes # When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [1/1/1] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3). The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose: Auto-discovery of the remote VTEPs attached to the same EVI Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated The IMET/RT3 routes can be viewed in summary and detailed modes: RT3 summary A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary ---------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------- Type 3 Inclusive Multicast Ethernet Tag Routes +--------+---------------------+------------+---------------------+---------------------+---------------------+ | Status | Route-distinguisher | Tag-ID | Originator-IP | neighbor | Next-Hop | +========+=====================+============+=====================+=====================+=====================+ | u*> | 10.0.0.2:111 | 0 | 10.0.0.2 | 10.0.0.2 | 10.0.0.2 | +--------+---------------------+------------+---------------------+---------------------+---------------------+ ---------------------------------------------------------------------------------------------------------------- 1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid ---------------------------------------------------------------------------------------------------------------- RT3 detailed A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail ------------------------------------------------------------------------------------- Show report for the EVPN routes in network-instance \"default\" ------------------------------------------------------------------------------------- Route Distinguisher: 10.0.0.2:111 Tag-ID : 0 Originating router : 10.0.0.2 neighbor : 10.0.0.2 Received paths : 1 Path 1: <Best,Valid,Used,> VNI : 1 Route source : neighbor 10.0.0.2 (last modified 2m3s ago) Route preference: No MED, LocalPref is 100 Atomic Aggr : false BGP next-hop : 10.0.0.2 AS Path : i Communities : [target:100:111, bgp-tunnel-encap:VXLAN] RR Attributes : No Originator-ID, Cluster-List is [] Aggregation : None Unknown Attr : None Invalid Reason : None Tie Break Reason: none -------------------------------------------------------------------------------------- Lets capture those routes? Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers . By capturing on the e1-49 interface of the clab-evpn01-leaf1 container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the vrf-1 network instance to trigger the BGP updates to flow and see them in the live capture. Here is the pcap file with the IMET routes advertisements between leaf1 and leaf2 . When the IMET routes from leaf2 are imported for vrf-1 network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 multicast destinations (flooding-list) ------------------------------------------------------------------------------- +--------------+------------+-------------------+----------------------+ | VTEP Address | Egress VNI | Destination-index | Multicast-forwarding | +==============+============+===================+======================+ | 10.0.0.2 | 1 | 160078821962 | BUM | +--------------+------------+-------------------+----------------------+ ------------------------------------------------------------------------------- Summary 1 multicast-destinations ------------------------------------------------------------------------------- This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table. As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes. VXLAN tunnels # After receiving EVPN routes from the remote leafs with VXLAN encapsulation 4 , SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the leaf1 switch. A:leaf1# /show tunnel vxlan-tunnel all ---------------------------------------------------------- Show report for vxlan-tunnels ---------------------------------------------------------- +--------------+--------------+--------------------------+ | VTEP Address | Index | Last Change | +==============+==============+==========================+ | 10.0.0.2 | 160078821947 | 2021-07-13T21:13:50.000Z | +--------------+--------------+--------------------------+ 1 VXLAN tunnels, 1 active, 0 inactive ---------------------------------------------------------- The VXLAN tunnel is built between the vxlan interfaces in the MAC-VRF network instances, which internally use system interfaces of the default network instance as a VTEP: Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index 3 , an entry in the tunnel-table is also created for the tunnel. A:leaf1# /show network-instance default tunnel-table all ------------------------------------------------------------------------------------------------------- Show report for network instance \"default\" tunnel table ------------------------------------------------------------------------------------------------------- +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ | IPv4 Prefix | Owner | Type | Index | Metric | Preference | Fib-prog | Last Update | +=============+===========+=======+=======+========+============+==========+==========================+ | 10.0.0.2/32 | vxlan_mgr | vxlan | 1 | 0 | 0 | Y | 2021-07-13T21:13:43.424Z | +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ ------------------------------------------------------------------------------------------------------- 1 VXLAN tunnels, 1 active, 0 inactive EVPN MAC/IP routes # As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations ------------------------------------------------------------------------------- Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Ethernet Segment Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Summary 0 unicast-destinations, 0 non-es, 0 es 0 MAC addresses, 0 active, 0 non-active This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the vrf-1 , we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs 7 . A:leaf1# show network-instance vrf-1 bridge-table mac-table all ------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ------------------------------------------------------------------------------- Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 0 Total 0 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 0 Total 0 Active ------------------------------------------------------------------------------- Let's try that ping from srv1 towards srv2 once again and see what happens: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. 64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms 64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms 64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2013ms rtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms Much better! The dataplane works and we can check that the MAC table in the vrf-1 network-instance has been populated with local and EVPN-learned MACs: A:leaf1# show network-instance vrf-1 bridge-table mac-table all --------------------------------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 --------------------------------------------------------------------------------------------------------------------------------------------- +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+====================================+===========+===========+========+=======+====================================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 240 | 2021-07-18T14:22:55.000Z | | 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1 | 160078821 | evpn | true | N/A | 2021-07-18T14:22:56.000Z | | | vtep:10.0.0.2 vni:1 | 962 | | | | | +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 1 Total 1 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 2 Total 2 Active --------------------------------------------------------------------------------------------------------------------------------------------- When traffic is exchanged between srv1 and srv2 , the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2) . The MAC/IP routes are imported, and the MACs programmed in the mac-table. The below output shows the MAC/IP EVPN route that leaf1 received from its neighbor. The NLRI information contains the MAC of the srv2 : A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Type 2 MAC-IP Advertisement Routes +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ | Statu | Route- | Tag-ID | MAC-address | IP-address | neighbor | Next-Hop | VNI | ESI | MAC Mobility | | s | distinguisher | | | | | | | | | +=======+================+===========+==================+================+================+================+================+===============================+================+ | u*> | 10.0.0.2:111 | 0 | 00:C1:AB:00:00:0 | 0.0.0.0 | 10.0.0.2 | 10.0.0.2 | 1 | 00:00:00:00:00:00:00:00:00:00 | - | | | | | 2 | | | | | | | +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 MAC-IP Advertisement routes 1 used, 1 valid ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * --------------------------------------------------------------------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations --------------------------------------------------------------------------------------------------------------------------------------------- Destinations --------------------------------------------------------------------------------------------------------------------------------------------- +--------------+------------+-------------------+-----------------------------+ | VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) | +==============+============+===================+=============================+ | 10.0.0.2 | 1 | 160078821962 | 1(1/0) | +--------------+------------+-------------------+-----------------------------+ --------------------------------------------------------------------------------------------------------------------------------------------- Ethernet Segment Destinations --------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------- Summary 1 unicast-destinations, 1 non-es, 0 es 1 MAC addresses, 1 active, 0 non-active ------------------------------------------------------------------------------- packet capture The following pcap was captured a moment before srv1 started to ping srv2 on leaf1 interface e1-49 . It shows how: ARP frames were first exchanged using the multicast destination, next the first ICMP request was sent out by leaf1 again using the BUM destination, since RT2 routes were not received yet and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane. after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes. This concludes the verification steps, as we have a working data plane connectivity between the servers. as was verified before \u21a9 containerlab assigns mac addresses to the interfaces with OUI 00:C1:AB . We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables. \u21a9 If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created. \u21a9 IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference. \u21a9 Per section 5.1.2 of RFC 8365 \u21a9 Easily extracted with doing info <container> where container is routing-policy , network-instance * , interface * , tunnel-interface * \u21a9 We did try to ping from srv1 to srv2 in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again. \u21a9","title":"EVPN configuration"},{"location":"tutorials/l2evpn/evpn/#overview","text":"EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch. VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently. The following figure demonstrates this concept where servers srv1 and srv2 are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain. Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable 1 , we can proceed with the VXLAN based EVPN service configuration. While doing that we will cover the following topics: VXLAN tunnel interface configuration Network instances of type mac-vrf Bridged subinterfaces and BGP EVPN control plane configuration","title":"Overview"},{"location":"tutorials/l2evpn/evpn/#ibgp-for-evpn","text":"Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches. For that iBGP configuration we will create a group called iBGP-overlay which will have the peer-as and local-as set to 100 to form an iBGP neighborship. The group will also host the same permissive all routing policy, enabled evpn and disabled ipv4-unicast address families. Then for each leaf we add a new BGP neighbor addressed by the remote system0 interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config: leaf1 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { peer-group iBGP-overlay transport { local-address 10.0.0.1 } } commit now leaf2 enter candidate /network-instance default protocols bgp group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { peer-group iBGP-overlay transport { local-address 10.0.0.2 } } commit now Ensure that the iBGP session is established before proceeding any further: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [0/0/0] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above.","title":"IBGP for EVPN"},{"location":"tutorials/l2evpn/evpn/#access-interfaces","text":"Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches: Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be bridged , this makes the interfaces only attachable to a network instance of mac-vrf type with MAC learning and layer-2 forwarding enabled. The following config is applied to both leaf switches: enter candidate /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } commit now As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers.","title":"Access interfaces"},{"location":"tutorials/l2evpn/evpn/#tunnelvxlan-interface","text":"After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric. The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name vxlan<N> where N = 0..255 . A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI). The index can be a number in the range 0-4294967295. The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed). The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of 1..16777215 . The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes. SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets. Note The source IP used in the vxlan-interfaces is the IPv4 address of subinterface system0.0 in the default network-instance. The above information translates to a configuration snippet which is applicable both to leaf1 and leaf2 nodes. enter candidate /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } commit now To verify the tunnel interface configuration: A:leaf2# show tunnel-interface vxlan-interface brief --------------------------------------------------------------------------------- Show report for vxlan-tunnels --------------------------------------------------------------------------------- +------------------+-----------------+---------+-------------+------------------+ | Tunnel Interface | VxLAN Interface | Type | Ingress VNI | Egress source-ip | +==================+=================+=========+=============+==================+ | vxlan1 | vxlan1.1 | bridged | 1 | 10.0.0.2/32 | +------------------+-----------------+---------+-------------+------------------+ --------------------------------------------------------------------------------- Summary 1 tunnel-interfaces, 1 vxlan interfaces 0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip ---------------------------------------------------------------------------------","title":"Tunnel/VXLAN interface"},{"location":"tutorials/l2evpn/evpn/#mac-vrf","text":"Now it is a turn of MAC-VRF to get configured. The network-instance type mac-vrf functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration. By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance: enter candidate /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } } commit now","title":"MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#server-interfaces","text":"The servers in our fabric do not have any addresses on their eth1 interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration. By the end of this section, we will have the following addressing scheme complete: To connect to a shell of a server execute docker exec -it <container-name> bash : srv1 docker exec -it clab-evpn01-srv1 bash srv2 docker exec -it clab-evpn01-srv2 bash Within the shell, configure MAC address 2 and IPv4 address for the eth1 interface according to the diagram above, as with this interface the server is connected to the leaf switch. srv1 ip link set address 00:c1:ab:00:00:01 dev eth1 ip addr add 192.168.0.1/24 dev eth1 srv2 ip link set address 00:c1:ab:00:00:02 dev eth1 ip addr add 192.168.0.2/24 dev eth1 Let's try to ping server2 from server1: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 2028ms That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the srv1 interface MAC to get learned by the leaf1 mac-vrf network instance: A:leaf1# show network-instance vrf-1 bridge-table mac-table all ---------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ---------------------------------------------------------------------------------------------------------------------- +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+==========================+===========+========+========+=======+==========================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 242 | 2021-07-13T17:36:23.000Z | +-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 1 Total 1 Active ----------------------------------------------------------------------------------------------------------------------","title":"Server interfaces"},{"location":"tutorials/l2evpn/evpn/#evpn-in-mac-vrf","text":"To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our vrf-1 network-instance. EVPN configuration under the mac-vrf network instance will require two configuration containers: bgp-vpn - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist. bgp-evpn - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to bgp-instance 1 is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI). The following configuration is entered on both leafs: enter candidate /network-instance vrf-1 protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } commit now Once configured, the bgp-vpn instance can be checked to have the RT/RD values set: A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1 ===================================================================== Net Instance : vrf-1 bgp Instance 1 --------------------------------------------------------------------- route-distinguisher: 10.0.0.1:111, auto-derived-from-evi export-route-target: target:100:111, manual import-route-target: target:100:111, manual ===================================================================== VNI to EVI mapping As of release 21.6, SR Linux uses only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI. 5","title":"EVPN in MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#final-configurations","text":"For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration 6 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks. pastable snippets leaf1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.2 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.1 } } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } /interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } commit now leaf2 enter candidate /routing-policy { policy all { default-action { accept { } } } } /tunnel-interface vxlan1 { vxlan-interface 1 { type bridged ingress { vni 1 } } } /network-instance default { interface ethernet-1/49.0 { } interface system0.0 { } protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 ipv4-unicast { admin-state enable } } group iBGP-overlay { export-policy all import-policy all peer-as 100 ipv4-unicast { admin-state disable } evpn { admin-state enable } local-as 100 { } timers { minimum-advertisement-interval 1 } } neighbor 10.0.0.1 { admin-state enable peer-group iBGP-overlay transport { local-address 10.0.0.2 } } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } /network-instance vrf-1 { type mac-vrf admin-state enable interface ethernet-1/1.0 { } vxlan-interface vxlan1.1 { } protocols { bgp-evpn { bgp-instance 1 { admin-state enable vxlan-interface vxlan1.1 evi 111 } } bgp-vpn { bgp-instance 1 { route-target { export-rt target:100:111 import-rt target:100:111 } } } } } /interface ethernet-1/1 { vlan-tagging true subinterface 0 { type bridged admin-state enable vlan { encap { untagged { } } } } } interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } commit now spine1 enter candidate /routing-policy { policy all { default-action { accept { } } } } /network-instance default { interface ethernet-1/1.0 { } interface ethernet-1/2.0 { } interface system0.0 { } protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-as 101 peer-group eBGP-underlay } neighbor 192.168.12.1 { peer-as 102 peer-group eBGP-underlay } } } } /interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } commit now srv1 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv1 bash ip link set address 00 :c1:ab:00:00:01 dev eth1 ip addr add 192 .168.0.1/24 dev eth1 srv2 configuring static MAC and IP on the single interface of a server docker exec -it clab-evpn01-srv2 bash ip link set address 00 :c1:ab:00:00:02 dev eth1 ip addr add 192 .168.0.2/24 dev eth1","title":"Final configurations"},{"location":"tutorials/l2evpn/evpn/#verification","text":"","title":"Verification"},{"location":"tutorials/l2evpn/evpn/#evpn-imet-routes","text":"When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands: A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2 ---------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------- +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Net-Inst | Peer | Group | Flags | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Activ | | | | | | | | | | e/Tx] | +===========+===========+===========+===========+===========+===========+===========+===========+===========+ | default | 10.0.0.2 | iBGP- | S | 100 | establish | 0d:0h:2m: | evpn | [1/1/1] | | | | overlay | | | ed | 9s | | | +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3). The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose: Auto-discovery of the remote VTEPs attached to the same EVI Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated The IMET/RT3 routes can be viewed in summary and detailed modes: RT3 summary A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary ---------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------- Type 3 Inclusive Multicast Ethernet Tag Routes +--------+---------------------+------------+---------------------+---------------------+---------------------+ | Status | Route-distinguisher | Tag-ID | Originator-IP | neighbor | Next-Hop | +========+=====================+============+=====================+=====================+=====================+ | u*> | 10.0.0.2:111 | 0 | 10.0.0.2 | 10.0.0.2 | 10.0.0.2 | +--------+---------------------+------------+---------------------+---------------------+---------------------+ ---------------------------------------------------------------------------------------------------------------- 1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid ---------------------------------------------------------------------------------------------------------------- RT3 detailed A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail ------------------------------------------------------------------------------------- Show report for the EVPN routes in network-instance \"default\" ------------------------------------------------------------------------------------- Route Distinguisher: 10.0.0.2:111 Tag-ID : 0 Originating router : 10.0.0.2 neighbor : 10.0.0.2 Received paths : 1 Path 1: <Best,Valid,Used,> VNI : 1 Route source : neighbor 10.0.0.2 (last modified 2m3s ago) Route preference: No MED, LocalPref is 100 Atomic Aggr : false BGP next-hop : 10.0.0.2 AS Path : i Communities : [target:100:111, bgp-tunnel-encap:VXLAN] RR Attributes : No Originator-ID, Cluster-List is [] Aggregation : None Unknown Attr : None Invalid Reason : None Tie Break Reason: none -------------------------------------------------------------------------------------- Lets capture those routes? Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers . By capturing on the e1-49 interface of the clab-evpn01-leaf1 container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the vrf-1 network instance to trigger the BGP updates to flow and see them in the live capture. Here is the pcap file with the IMET routes advertisements between leaf1 and leaf2 . When the IMET routes from leaf2 are imported for vrf-1 network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 multicast destinations (flooding-list) ------------------------------------------------------------------------------- +--------------+------------+-------------------+----------------------+ | VTEP Address | Egress VNI | Destination-index | Multicast-forwarding | +==============+============+===================+======================+ | 10.0.0.2 | 1 | 160078821962 | BUM | +--------------+------------+-------------------+----------------------+ ------------------------------------------------------------------------------- Summary 1 multicast-destinations ------------------------------------------------------------------------------- This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table. As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes.","title":"EVPN IMET routes"},{"location":"tutorials/l2evpn/evpn/#vxlan-tunnels","text":"After receiving EVPN routes from the remote leafs with VXLAN encapsulation 4 , SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the leaf1 switch. A:leaf1# /show tunnel vxlan-tunnel all ---------------------------------------------------------- Show report for vxlan-tunnels ---------------------------------------------------------- +--------------+--------------+--------------------------+ | VTEP Address | Index | Last Change | +==============+==============+==========================+ | 10.0.0.2 | 160078821947 | 2021-07-13T21:13:50.000Z | +--------------+--------------+--------------------------+ 1 VXLAN tunnels, 1 active, 0 inactive ---------------------------------------------------------- The VXLAN tunnel is built between the vxlan interfaces in the MAC-VRF network instances, which internally use system interfaces of the default network instance as a VTEP: Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index 3 , an entry in the tunnel-table is also created for the tunnel. A:leaf1# /show network-instance default tunnel-table all ------------------------------------------------------------------------------------------------------- Show report for network instance \"default\" tunnel table ------------------------------------------------------------------------------------------------------- +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ | IPv4 Prefix | Owner | Type | Index | Metric | Preference | Fib-prog | Last Update | +=============+===========+=======+=======+========+============+==========+==========================+ | 10.0.0.2/32 | vxlan_mgr | vxlan | 1 | 0 | 0 | Y | 2021-07-13T21:13:43.424Z | +-------------+-----------+-------+-------+--------+------------+----------+--------------------------+ ------------------------------------------------------------------------------------------------------- 1 VXLAN tunnels, 1 active, 0 inactive","title":"VXLAN tunnels"},{"location":"tutorials/l2evpn/evpn/#evpn-macip-routes","text":"As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * ------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations ------------------------------------------------------------------------------- Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Ethernet Segment Destinations ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Summary 0 unicast-destinations, 0 non-es, 0 es 0 MAC addresses, 0 active, 0 non-active This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the vrf-1 , we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs 7 . A:leaf1# show network-instance vrf-1 bridge-table mac-table all ------------------------------------------------------------------------------- Mac-table of network instance vrf-1 ------------------------------------------------------------------------------- Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 0 Total 0 Active Total Evpn Macs : 0 Total 0 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 0 Total 0 Active ------------------------------------------------------------------------------- Let's try that ping from srv1 towards srv2 once again and see what happens: bash-5.0# ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. 64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms 64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms 64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms ^C --- 192.168.0.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2013ms rtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms Much better! The dataplane works and we can check that the MAC table in the vrf-1 network-instance has been populated with local and EVPN-learned MACs: A:leaf1# show network-instance vrf-1 bridge-table mac-table all --------------------------------------------------------------------------------------------------------------------------------------------- Mac-table of network instance vrf-1 --------------------------------------------------------------------------------------------------------------------------------------------- +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ | Address | Destination | Dest | Type | Active | Aging | Last Update | | | | Index | | | | | +===================+====================================+===========+===========+========+=======+====================================+ | 00:C1:AB:00:00:01 | ethernet-1/1.0 | 4 | learnt | true | 240 | 2021-07-18T14:22:55.000Z | | 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1 | 160078821 | evpn | true | N/A | 2021-07-18T14:22:56.000Z | | | vtep:10.0.0.2 vni:1 | 962 | | | | | +-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+ Total Irb Macs : 0 Total 0 Active Total Static Macs : 0 Total 0 Active Total Duplicate Macs : 0 Total 0 Active Total Learnt Macs : 1 Total 1 Active Total Evpn Macs : 1 Total 1 Active Total Evpn static Macs : 0 Total 0 Active Total Irb anycast Macs : 0 Total 0 Active Total Macs : 2 Total 2 Active --------------------------------------------------------------------------------------------------------------------------------------------- When traffic is exchanged between srv1 and srv2 , the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2) . The MAC/IP routes are imported, and the MACs programmed in the mac-table. The below output shows the MAC/IP EVPN route that leaf1 received from its neighbor. The NLRI information contains the MAC of the srv2 : A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Show report for the BGP route table of network-instance \"default\" ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- BGP Router ID: 10.0.0.1 AS: 101 Local AS: 101 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Type 2 MAC-IP Advertisement Routes +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ | Statu | Route- | Tag-ID | MAC-address | IP-address | neighbor | Next-Hop | VNI | ESI | MAC Mobility | | s | distinguisher | | | | | | | | | +=======+================+===========+==================+================+================+================+================+===============================+================+ | u*> | 10.0.0.2:111 | 0 | 00:C1:AB:00:00:0 | 0.0.0.0 | 10.0.0.2 | 10.0.0.2 | 1 | 00:00:00:00:00:00:00:00:00:00 | - | | | | | 2 | | | | | | | +-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 MAC-IP Advertisement routes 1 used, 1 valid ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before: A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination * --------------------------------------------------------------------------------------------------------------------------------------------- Show report for vxlan-interface vxlan1.1 unicast destinations --------------------------------------------------------------------------------------------------------------------------------------------- Destinations --------------------------------------------------------------------------------------------------------------------------------------------- +--------------+------------+-------------------+-----------------------------+ | VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) | +==============+============+===================+=============================+ | 10.0.0.2 | 1 | 160078821962 | 1(1/0) | +--------------+------------+-------------------+-----------------------------+ --------------------------------------------------------------------------------------------------------------------------------------------- Ethernet Segment Destinations --------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------- Summary 1 unicast-destinations, 1 non-es, 0 es 1 MAC addresses, 1 active, 0 non-active ------------------------------------------------------------------------------- packet capture The following pcap was captured a moment before srv1 started to ping srv2 on leaf1 interface e1-49 . It shows how: ARP frames were first exchanged using the multicast destination, next the first ICMP request was sent out by leaf1 again using the BUM destination, since RT2 routes were not received yet and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane. after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes. This concludes the verification steps, as we have a working data plane connectivity between the servers. as was verified before \u21a9 containerlab assigns mac addresses to the interfaces with OUI 00:C1:AB . We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables. \u21a9 If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created. \u21a9 IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference. \u21a9 Per section 5.1.2 of RFC 8365 \u21a9 Easily extracted with doing info <container> where container is routing-policy , network-instance * , interface * , tunnel-interface * \u21a9 We did try to ping from srv1 to srv2 in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again. \u21a9","title":"EVPN MAC/IP routes"},{"location":"tutorials/l2evpn/fabric/","text":"Prior to configuring EVPN based overlay, a routing protocol needs to be deployed in the fabric to advertise the reachability of all the leaf VXLAN Termination End Point (VTEP) addresses throughout the IP fabric. With SR Linux, the following routing protocols can be used in the underlay: ISIS OSPF EBGP We will use a BGP based fabric design as described in RFC7938 due to its simplicity, scalability, and ease of multi-vendor interoperability. Leaf-Spine interfaces # Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the 192.168.xx.0/30 network to address the links, we will implement the following underlay addressing design: On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity. We begin with connecting to the CLI of our nodes via SSH 1 : # connecting to leaf1 ssh admin@clab-evpn01-leaf1 Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration. Let's witness the step by step process of an interface configuration on a leaf1 switch with providing the paste-ables snippets for the rest of the nodes Enter the candidate configuration mode to make edits to the configuration Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# enter candidate The prompt will indicate the changed active mode --{ candidate shared default }--[ ]-- A:leaf1# Enter into the interface configuration context --{ candidate shared default }--[ ]-- A:leaf1# interface ethernet-1/49 Create a subinterface under the parent interface to configure IPv4 address on it --{ * candidate shared default }--[ interface ethernet-1/49 ]-- A:leaf1# subinterface 0 --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]-- A:leaf1# ipv4 address 192.168.11.1/30 Apply the configuration changes by issuing a commit now command. The changes will be written to the running configuration. --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. Below you will find the relevant configuration snippets 2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode. leaf1 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } leaf2 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } spine1 interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } Once those snippets are committed to the running configuration with commit now command, we can ensure that the changes have been applied by showing the interface status: --{ + running }--[ ]-- A:spine1# show interface ethernet-1/1 ==================================================== ethernet-1/1 is up, speed 10G, type None ethernet-1/1.0 is up Network-instance: Encapsulation : null Type : routed IPv4 addr : 192.168.11.2/30 (static, None) ---------------------------------------------------- ==================================================== At this moment, the configured interfaces can not be used as they are not yet associated with any network instance . Below we are placing the interfaces to the network-instance default that is created automatically by SR Linux. leaf1 & leaf2 --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default interface ethernet-1/49.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. spine1 --{ + candidate shared default }--[ ]-- A:spine1# network-instance default interface ethernet-1/1.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]-- A:spine1# /network-instance default interface ethernet-1/2.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]-- A:spine2# commit now All changes have been committed. Leaving candidate mode. When interfaces are owned by the network-instance default , we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from spine1 to leaf2 : --{ + running }--[ ]-- A:spine1# ping 192.168.12.1 network-instance default Using network instance default PING 192.168.12.1 (192.168.12.1) 56(84) bytes of data. 64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms 64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms 64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms 64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms ^C --- 192.168.12.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3003ms rtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms EBGP # Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol. The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering. Let's turn this diagram with the ASN/Router ID allocation into a working configuration: Here is a breakdown of the steps that are needed to configure EBGP on leaf1 towards spine1 : Add BGP protocol to network-instance Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default protocols bgp Assign Autonomous System Number The ASN is reported to peers when BGP speaker opens a session towards another router. According to the diagram above, leaf1 has ASN 101. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# autonomous-system 101 Assign Router ID This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router. Leaf1 has a router-id of 10.0.0.1. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# router-id 10.0.0.1 Enable AF Enable all address families that should be enabled globally as a default for all peers of the BGP instance. When you later configure individual neighbors or groups, you can override the enabled families at those levels. For the sake of IPv4 loopbacks advertisement, we only need to enable ipv4-unicast address family: --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# ipv4-unicast admin-state enable Create export/import policies The export/import policy is required for an EBGP peer to advertise and install routes. The policy named all that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received 4 . The routing policies are configured at /routing-policy context, so first, we switch to it from the current bgp context: --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# /routing-policy --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# Now that we are in the right context, we can paste the policy definition: --{ +* candidate shared default }--[ routing-policy ]-- A:leaf1# info policy all { default-action { accept { } } } Create peer-group config A peer group should include sessions that have a similar or almost identical configuration. In this example, the peer group is named eBGP-underlay since it will be used to enable underlay routing between the leafs and spines. New groups are administratively enabled by default. First, we come back to the bgp context from the routing-policy context: --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# /network-instance default protocols bgp --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# Now create the peer group. The common group configuration includes the peer-as and export-policy statements. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# group eBGP-underlay --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# peer-as 201 --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# export-policy all --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# import-policy all Configure neighbor Configure the BGP session with spine1 . In this example, spine1 is reachable through the ethernet-1/49.0 subinterface. On this subnet, spine1 has the IPv4 address 192.168.11.2 . In this minimal configuration example, the only required configuration for the neighbor is its association with the group eBGP-underlay that was previously created. New neighbors are administratively enabled by default. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay Commit configuration It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far. Regardless of which context you are currently in, you can see the diff against the baseline config by doing diff / --{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# diff / network-instance default { protocols { + bgp { + autonomous-system 101 + router-id 10.0.0.1 + group eBGP-underlay { + export-policy all + import-policy all + peer-as 201 + } + ipv4-unicast { + admin-state enable + } + neighbor 192.168.11.2 { + peer-group eBGP-underlay + } + } } } + routing-policy { + policy all { + default-action { + accept { + } + } + } + } That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# commit now EBGP configuration on leaf2 and spine1 is almost a twin of the one we did for leaf1 . Here is a copy-paste-able 3 config snippets for all of the nodes: leaf1 network-instance default { protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } routing-policy { policy all { default-action { accept { } } } } leaf2 network-instance default { protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } routing-policy { policy all { default-action { accept { } } } } spine1 Spine configuration is a bit different, in a way that peer-as is specified under the neighbor context, and not the group one. network-instance default { protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-group eBGP-underlay peer-as 101 } neighbor 192.168.12.1 { peer-group eBGP-underlay peer-as 102 } } } } routing-policy { policy all { default-action { accept { } } } } Loopbacks # As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces. In the context of the VXLAN data plane, a special kind of a loopback needs to be created - system0 interface. Info The system0.0 interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes. Configuration of the system0 interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to system0 interfaces will match the Router-ID of a given BGP speaker. leaf1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } /network-instance default { interface system0.0 { } } leaf2 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } /network-instance default { interface system0.0 { } } spine1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } /network-instance default { interface system0.0 { } } Verification # As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The system0 interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that. BGP status # The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on leaf1 : --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp summary ------------------------------------------------------------- BGP is enabled and up in network-instance \"default\" Global AS number : 101 BGP identifier : 10.0.0.1 ------------------------------------------------------------- Total paths : 3 Received routes : 3 Received and active routes: None Total UP peers : 1 Configured peers : 1, 0 are disabled Dynamic peers : None ------------------------------------------------------------- Default preferences BGP Local Preference attribute: 100 EBGP route-table preference : 170 IBGP route-table preference : 170 ------------------------------------------------------------- Wait for FIB install to advertise: True Send rapid withdrawals : disabled ------------------------------------------------------------- Ipv4-unicast AFI/SAFI Received routes : 3 Received and active routes : None Max number of multipaths : 1, 1 Multipath can transit multi AS: True ------------------------------------------------------------- Ipv6-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : 1,1 Multipath can transit multi AS: True ------------------------------------------------------------- EVPN-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : N/A Multipath can transit multi AS: N/A ------------------------------------------------------------- BGP neighbor status # Equally important is the neighbor summary status that we can observe with the following: --{ + running }--[ ]-- A:spine1# show network-instance default protocols bgp neighbor ---------------------------------------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------- +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ | Net-Inst | Peer | Group | Flag | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Active/Tx] | | | | | s | | | | | | +================+=======================+================+======+=========+=============+=============+===========+=======================+ | default | 192.168.11.1 | eBGP-underlay | S | 101 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 49s | ast | | | default | 192.168.12.1 | eBGP-underlay | S | 102 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 9s | ast | | +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ ---------------------------------------------------------------------------------------------------------------------------------------------- Summary: 2 configured neighbors, 2 configured sessions are established,0 disabled peers 0 dynamic peers With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state. Received/Advertised routes # The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - system0 interfaces. In the below output we verify that leaf1 advertises the prefix of system0 ( 10.0.0.1/32 ) interface towards its EBGP spine1 peer: --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou tes ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.11.2, remote AS: 201, local AS: 101 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Origin codes: i=IGP, e=EGP, ?=incomplete +-------------------------------------------------------------------------------------+ | Network Next Hop MED LocPref AsPath Origin | +=====================================================================================+ | 10.0.0.1/32 192.168.11. - 100 [101] i | | 1 | | 192.168.11.0/3 192.168.11. - 100 [101] i | | 0 1 | +-------------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 2 advertised BGP routes ----------------------------------------------------------------------------------------- On the far end of the fabric, leaf2 receives both the leaf1 and spine1 system interface prefixes: --{ + running }--[ ]-- A:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route s ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.12.2, remote AS: 201, local AS: 102 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete +-----------------------------------------------------------------------------------+ | Status Network Next Hop MED LocPref AsPath Origin | +===================================================================================+ | u*> 10.0.0.1/ 192.168.1 - 100 [201, i | | 32 2.2 101] | | u*> 10.0.1.1/ 192.168.1 - 100 [201] i | | 32 2.2 | | u*> 192.168.1 192.168.1 - 100 [201] i | | 1.0/30 2.2 | | * 192.168.1 192.168.1 - 100 [201] i | | 2.0/30 2.2 | +-----------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 4 received BGP routes : 3 used 4 valid ----------------------------------------------------------------------------------------- Route table # The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the default network-instance where we expect them to be: --{ running }--[ ]-- A:leaf1# show network-instance default route-table ipv4-unicast summary ----------------------------------------------------------------------------------------------------------------------------------- IPv4 Unicast route table of network instance default ----------------------------------------------------------------------------------------------------------------------------------- +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ | Prefix | ID | Route Type | Route Owner | Best/Fib- | Metric | Pref | Next-hop | Next-hop | | | | | | status(slot) | | | (Type) | Interface | +=================+=======+============+======================+======================+==========+=========+===========+===========+ | 10.0.0.1/32 | 3 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 10.0.0.2/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 10.0.1.1/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 192.168.11.0/30 | 1 | local | net_inst_mgr | True/success | 0 | 0 | 192.168.1 | ethernet- | | | | | | | | | 1.1 | 1/49.0 | | | | | | | | | (direct) | | | 192.168.11.1/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 192.168.11.3/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None (bro | None | | | | | | | | | adcast) | | | 192.168.12.0/30 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ ----------------------------------------------------------------------------------------------------------------------------------- 7 IPv4 routes total 7 IPv4 prefixes with active routes 0 IPv4 prefixes with active ECMP routes ----------------------------------------------------------------------------------------------------------------------------------- Both leaf2 and spine1 prefixes are found in the route table of network-instance default and the bgp_mgr is the owner of those prefixes, which means that they have been added to the route-table by the BGP app. Dataplane # To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay. For that we will use the ping command with src/dst set to loopback addresses: --{ running }--[ ]-- A:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2 Using network instance default PING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration! Resulting configs # Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with info flat command. Note enter candidate and commit now commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config. leaf1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 101 set / network-instance default protocols bgp router-id 10.0.0.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.2 set / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay commit now leaf2 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.2/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 102 set / network-instance default protocols bgp router-id 10.0.0.2 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.12.2 set / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay commit now spine1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/1 set / interface ethernet-1/1 subinterface 0 set / interface ethernet-1/1 subinterface 0 ipv4 set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30 set / interface ethernet-1/2 set / interface ethernet-1/2 subinterface 0 set / interface ethernet-1/2 subinterface 0 ipv4 set / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.1.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/1.0 set / network-instance default interface ethernet-1/2.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 201 set / network-instance default protocols bgp router-id 10.0.1.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.1 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay set / network-instance default protocols bgp neighbor 192.168.12.1 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay commit now default SR Linux credentials are admin:admin . \u21a9 the snippets were extracted with info interface ethernet-1/x command issued in running mode. \u21a9 you can paste those snippets right after you do enter candidate \u21a9 a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines. \u21a9","title":"Fabric configuration"},{"location":"tutorials/l2evpn/fabric/#leaf-spine-interfaces","text":"Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the 192.168.xx.0/30 network to address the links, we will implement the following underlay addressing design: On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity. We begin with connecting to the CLI of our nodes via SSH 1 : # connecting to leaf1 ssh admin@clab-evpn01-leaf1 Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration. Let's witness the step by step process of an interface configuration on a leaf1 switch with providing the paste-ables snippets for the rest of the nodes Enter the candidate configuration mode to make edits to the configuration Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:leaf1# enter candidate The prompt will indicate the changed active mode --{ candidate shared default }--[ ]-- A:leaf1# Enter into the interface configuration context --{ candidate shared default }--[ ]-- A:leaf1# interface ethernet-1/49 Create a subinterface under the parent interface to configure IPv4 address on it --{ * candidate shared default }--[ interface ethernet-1/49 ]-- A:leaf1# subinterface 0 --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]-- A:leaf1# ipv4 address 192.168.11.1/30 Apply the configuration changes by issuing a commit now command. The changes will be written to the running configuration. --{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. Below you will find the relevant configuration snippets 2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode. leaf1 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.11.1/30 { } } } } leaf2 interface ethernet-1/49 { subinterface 0 { ipv4 { address 192.168.12.1/30 { } } } } spine1 interface ethernet-1/1 { subinterface 0 { ipv4 { address 192.168.11.2/30 { } } } } interface ethernet-1/2 { subinterface 0 { ipv4 { address 192.168.12.2/30 { } } } } Once those snippets are committed to the running configuration with commit now command, we can ensure that the changes have been applied by showing the interface status: --{ + running }--[ ]-- A:spine1# show interface ethernet-1/1 ==================================================== ethernet-1/1 is up, speed 10G, type None ethernet-1/1.0 is up Network-instance: Encapsulation : null Type : routed IPv4 addr : 192.168.11.2/30 (static, None) ---------------------------------------------------- ==================================================== At this moment, the configured interfaces can not be used as they are not yet associated with any network instance . Below we are placing the interfaces to the network-instance default that is created automatically by SR Linux. leaf1 & leaf2 --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default interface ethernet-1/49.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]-- A:leaf1# commit now All changes have been committed. Leaving candidate mode. spine1 --{ + candidate shared default }--[ ]-- A:spine1# network-instance default interface ethernet-1/1.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]-- A:spine1# /network-instance default interface ethernet-1/2.0 --{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]-- A:spine2# commit now All changes have been committed. Leaving candidate mode. When interfaces are owned by the network-instance default , we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from spine1 to leaf2 : --{ + running }--[ ]-- A:spine1# ping 192.168.12.1 network-instance default Using network instance default PING 192.168.12.1 (192.168.12.1) 56(84) bytes of data. 64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms 64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms 64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms 64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms ^C --- 192.168.12.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3003ms rtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms","title":"Leaf-Spine interfaces"},{"location":"tutorials/l2evpn/fabric/#ebgp","text":"Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol. The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering. Let's turn this diagram with the ASN/Router ID allocation into a working configuration: Here is a breakdown of the steps that are needed to configure EBGP on leaf1 towards spine1 : Add BGP protocol to network-instance Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. --{ + candidate shared default }--[ ]-- A:leaf1# network-instance default protocols bgp Assign Autonomous System Number The ASN is reported to peers when BGP speaker opens a session towards another router. According to the diagram above, leaf1 has ASN 101. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# autonomous-system 101 Assign Router ID This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router. Leaf1 has a router-id of 10.0.0.1. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# router-id 10.0.0.1 Enable AF Enable all address families that should be enabled globally as a default for all peers of the BGP instance. When you later configure individual neighbors or groups, you can override the enabled families at those levels. For the sake of IPv4 loopbacks advertisement, we only need to enable ipv4-unicast address family: --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# ipv4-unicast admin-state enable Create export/import policies The export/import policy is required for an EBGP peer to advertise and install routes. The policy named all that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received 4 . The routing policies are configured at /routing-policy context, so first, we switch to it from the current bgp context: --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# /routing-policy --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# Now that we are in the right context, we can paste the policy definition: --{ +* candidate shared default }--[ routing-policy ]-- A:leaf1# info policy all { default-action { accept { } } } Create peer-group config A peer group should include sessions that have a similar or almost identical configuration. In this example, the peer group is named eBGP-underlay since it will be used to enable underlay routing between the leafs and spines. New groups are administratively enabled by default. First, we come back to the bgp context from the routing-policy context: --{ * candidate shared default }--[ routing-policy ]-- A:leaf1# /network-instance default protocols bgp --{ * candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# Now create the peer group. The common group configuration includes the peer-as and export-policy statements. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# group eBGP-underlay --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# peer-as 201 --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# export-policy all --{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# import-policy all Configure neighbor Configure the BGP session with spine1 . In this example, spine1 is reachable through the ethernet-1/49.0 subinterface. On this subnet, spine1 has the IPv4 address 192.168.11.2 . In this minimal configuration example, the only required configuration for the neighbor is its association with the group eBGP-underlay that was previously created. New neighbors are administratively enabled by default. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay Commit configuration It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far. Regardless of which context you are currently in, you can see the diff against the baseline config by doing diff / --{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]-- A:leaf1# diff / network-instance default { protocols { + bgp { + autonomous-system 101 + router-id 10.0.0.1 + group eBGP-underlay { + export-policy all + import-policy all + peer-as 201 + } + ipv4-unicast { + admin-state enable + } + neighbor 192.168.11.2 { + peer-group eBGP-underlay + } + } } } + routing-policy { + policy all { + default-action { + accept { + } + } + } + } That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration. --{ +* candidate shared default }--[ network-instance default protocols bgp ]-- A:leaf1# commit now EBGP configuration on leaf2 and spine1 is almost a twin of the one we did for leaf1 . Here is a copy-paste-able 3 config snippets for all of the nodes: leaf1 network-instance default { protocols { bgp { autonomous-system 101 router-id 10.0.0.1 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.11.2 { peer-group eBGP-underlay } } } } routing-policy { policy all { default-action { accept { } } } } leaf2 network-instance default { protocols { bgp { autonomous-system 102 router-id 10.0.0.2 group eBGP-underlay { export-policy all import-policy all peer-as 201 } ipv4-unicast { admin-state enable } neighbor 192.168.12.2 { peer-group eBGP-underlay } } } } routing-policy { policy all { default-action { accept { } } } } spine1 Spine configuration is a bit different, in a way that peer-as is specified under the neighbor context, and not the group one. network-instance default { protocols { bgp { autonomous-system 201 router-id 10.0.1.1 group eBGP-underlay { export-policy all import-policy all } ipv4-unicast { admin-state enable } neighbor 192.168.11.1 { peer-group eBGP-underlay peer-as 101 } neighbor 192.168.12.1 { peer-group eBGP-underlay peer-as 102 } } } } routing-policy { policy all { default-action { accept { } } } }","title":"EBGP"},{"location":"tutorials/l2evpn/fabric/#loopbacks","text":"As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces. In the context of the VXLAN data plane, a special kind of a loopback needs to be created - system0 interface. Info The system0.0 interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes. Configuration of the system0 interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to system0 interfaces will match the Router-ID of a given BGP speaker. leaf1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.1/32 { } } } } /network-instance default { interface system0.0 { } } leaf2 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.0.2/32 { } } } } /network-instance default { interface system0.0 { } } spine1 /interface system0 { admin-state enable subinterface 0 { ipv4 { address 10.0.1.1/32 { } } } } /network-instance default { interface system0.0 { } }","title":"Loopbacks"},{"location":"tutorials/l2evpn/fabric/#verification","text":"As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The system0 interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that.","title":"Verification"},{"location":"tutorials/l2evpn/fabric/#bgp-status","text":"The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on leaf1 : --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp summary ------------------------------------------------------------- BGP is enabled and up in network-instance \"default\" Global AS number : 101 BGP identifier : 10.0.0.1 ------------------------------------------------------------- Total paths : 3 Received routes : 3 Received and active routes: None Total UP peers : 1 Configured peers : 1, 0 are disabled Dynamic peers : None ------------------------------------------------------------- Default preferences BGP Local Preference attribute: 100 EBGP route-table preference : 170 IBGP route-table preference : 170 ------------------------------------------------------------- Wait for FIB install to advertise: True Send rapid withdrawals : disabled ------------------------------------------------------------- Ipv4-unicast AFI/SAFI Received routes : 3 Received and active routes : None Max number of multipaths : 1, 1 Multipath can transit multi AS: True ------------------------------------------------------------- Ipv6-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : 1,1 Multipath can transit multi AS: True ------------------------------------------------------------- EVPN-unicast AFI/SAFI Received routes : None Received and active routes : None Max number of multipaths : N/A Multipath can transit multi AS: N/A -------------------------------------------------------------","title":"BGP status"},{"location":"tutorials/l2evpn/fabric/#bgp-neighbor-status","text":"Equally important is the neighbor summary status that we can observe with the following: --{ + running }--[ ]-- A:spine1# show network-instance default protocols bgp neighbor ---------------------------------------------------------------------------------------------------------------------------------------------- BGP neighbor summary for network-instance \"default\" Flags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow ---------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------- +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ | Net-Inst | Peer | Group | Flag | Peer-AS | State | Uptime | AFI/SAFI | [Rx/Active/Tx] | | | | | s | | | | | | +================+=======================+================+======+=========+=============+=============+===========+=======================+ | default | 192.168.11.1 | eBGP-underlay | S | 101 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 49s | ast | | | default | 192.168.12.1 | eBGP-underlay | S | 102 | established | 0d:18h:20m: | ipv4-unic | [2/1/4] | | | | | | | | 9s | ast | | +----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+ ---------------------------------------------------------------------------------------------------------------------------------------------- Summary: 2 configured neighbors, 2 configured sessions are established,0 disabled peers 0 dynamic peers With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state.","title":"BGP neighbor status"},{"location":"tutorials/l2evpn/fabric/#receivedadvertised-routes","text":"The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - system0 interfaces. In the below output we verify that leaf1 advertises the prefix of system0 ( 10.0.0.1/32 ) interface towards its EBGP spine1 peer: --{ + running }--[ ]-- A:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou tes ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.11.2, remote AS: 201, local AS: 101 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Origin codes: i=IGP, e=EGP, ?=incomplete +-------------------------------------------------------------------------------------+ | Network Next Hop MED LocPref AsPath Origin | +=====================================================================================+ | 10.0.0.1/32 192.168.11. - 100 [101] i | | 1 | | 192.168.11.0/3 192.168.11. - 100 [101] i | | 0 1 | +-------------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 2 advertised BGP routes ----------------------------------------------------------------------------------------- On the far end of the fabric, leaf2 receives both the leaf1 and spine1 system interface prefixes: --{ + running }--[ ]-- A:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route s ipv4 ----------------------------------------------------------------------------------------- Peer : 192.168.12.2, remote AS: 201, local AS: 102 Type : static Description : None Group : eBGP-underlay ----------------------------------------------------------------------------------------- Status codes: u=used, *=valid, >=best, x=stale Origin codes: i=IGP, e=EGP, ?=incomplete +-----------------------------------------------------------------------------------+ | Status Network Next Hop MED LocPref AsPath Origin | +===================================================================================+ | u*> 10.0.0.1/ 192.168.1 - 100 [201, i | | 32 2.2 101] | | u*> 10.0.1.1/ 192.168.1 - 100 [201] i | | 32 2.2 | | u*> 192.168.1 192.168.1 - 100 [201] i | | 1.0/30 2.2 | | * 192.168.1 192.168.1 - 100 [201] i | | 2.0/30 2.2 | +-----------------------------------------------------------------------------------+ ----------------------------------------------------------------------------------------- 4 received BGP routes : 3 used 4 valid -----------------------------------------------------------------------------------------","title":"Received/Advertised routes"},{"location":"tutorials/l2evpn/fabric/#route-table","text":"The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the default network-instance where we expect them to be: --{ running }--[ ]-- A:leaf1# show network-instance default route-table ipv4-unicast summary ----------------------------------------------------------------------------------------------------------------------------------- IPv4 Unicast route table of network instance default ----------------------------------------------------------------------------------------------------------------------------------- +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ | Prefix | ID | Route Type | Route Owner | Best/Fib- | Metric | Pref | Next-hop | Next-hop | | | | | | status(slot) | | | (Type) | Interface | +=================+=======+============+======================+======================+==========+=========+===========+===========+ | 10.0.0.1/32 | 3 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 10.0.0.2/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 10.0.1.1/32 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | | 192.168.11.0/30 | 1 | local | net_inst_mgr | True/success | 0 | 0 | 192.168.1 | ethernet- | | | | | | | | | 1.1 | 1/49.0 | | | | | | | | | (direct) | | | 192.168.11.1/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None | None | | | | | | | | | (extract) | | | 192.168.11.3/32 | 1 | host | net_inst_mgr | True/success | 0 | 0 | None (bro | None | | | | | | | | | adcast) | | | 192.168.12.0/30 | 0 | bgp | bgp_mgr | True/success | 0 | 170 | 192.168.1 | None | | | | | | | | | 1.2 (indi | | | | | | | | | | rect) | | +-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+ ----------------------------------------------------------------------------------------------------------------------------------- 7 IPv4 routes total 7 IPv4 prefixes with active routes 0 IPv4 prefixes with active ECMP routes ----------------------------------------------------------------------------------------------------------------------------------- Both leaf2 and spine1 prefixes are found in the route table of network-instance default and the bgp_mgr is the owner of those prefixes, which means that they have been added to the route-table by the BGP app.","title":"Route table"},{"location":"tutorials/l2evpn/fabric/#dataplane","text":"To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay. For that we will use the ping command with src/dst set to loopback addresses: --{ running }--[ ]-- A:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2 Using network instance default PING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration!","title":"Dataplane"},{"location":"tutorials/l2evpn/fabric/#resulting-configs","text":"Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with info flat command. Note enter candidate and commit now commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config. leaf1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 101 set / network-instance default protocols bgp router-id 10.0.0.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.2 set / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay commit now leaf2 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/49 set / interface ethernet-1/49 subinterface 0 set / interface ethernet-1/49 subinterface 0 ipv4 set / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.0.2/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/49.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 102 set / network-instance default protocols bgp router-id 10.0.0.2 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp group eBGP-underlay peer-as 201 set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.12.2 set / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay commit now spine1 enter candidate # configuration of the physical interface and its subinterface set / interface ethernet-1/1 set / interface ethernet-1/1 subinterface 0 set / interface ethernet-1/1 subinterface 0 ipv4 set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30 set / interface ethernet-1/2 set / interface ethernet-1/2 subinterface 0 set / interface ethernet-1/2 subinterface 0 ipv4 set / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30 # system interface configuration set / interface system0 set / interface system0 admin-state enable set / interface system0 subinterface 0 set / interface system0 subinterface 0 ipv4 set / interface system0 subinterface 0 ipv4 address 10.0.1.1/32 # associating interfaces with net-ins default set / network-instance default set / network-instance default interface ethernet-1/1.0 set / network-instance default interface ethernet-1/2.0 set / network-instance default interface system0.0 # routing policy set / routing-policy set / routing-policy policy all set / routing-policy policy all default-action set / routing-policy policy all default-action accept # BGP configuration set / network-instance default protocols set / network-instance default protocols bgp set / network-instance default protocols bgp autonomous-system 201 set / network-instance default protocols bgp router-id 10.0.1.1 set / network-instance default protocols bgp group eBGP-underlay set / network-instance default protocols bgp group eBGP-underlay export-policy all set / network-instance default protocols bgp group eBGP-underlay import-policy all set / network-instance default protocols bgp ipv4-unicast set / network-instance default protocols bgp ipv4-unicast admin-state enable set / network-instance default protocols bgp neighbor 192.168.11.1 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101 set / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay set / network-instance default protocols bgp neighbor 192.168.12.1 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102 set / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay commit now default SR Linux credentials are admin:admin . \u21a9 the snippets were extracted with info interface ethernet-1/x command issued in running mode. \u21a9 you can paste those snippets right after you do enter candidate \u21a9 a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines. \u21a9","title":"Resulting configs"},{"location":"tutorials/l2evpn/intro/","text":"Tutorial name L2 EVPN-VXLAN with SR Linux Lab components 3 SR Linux nodes Resource requirements 2vCPU 4 GB Containerlab topology file evpn01.clab.yml Lab name evpn01 Packet captures EVPN IMET routes exchange , RT2 routes exchange with ICMP in datapath Main ref documents RFC 7432 - BGP MPLS-Based Ethernet VPN RFC 8365 - A Network Virtualization Overlay Solution Using Ethernet VPN (EVPN) Nokia 7220 SR Linux Advanced Solutions Guide Nokia 7220 SR Linux EVPN-VXLAN Guide Version information 1 containerlab:0.15.4 , srlinux:21.6.1-250 , docker-ce:20.10.2 Ethernet Virtual Private Network (EVPN) is a standard technology in multi-tenant Data Centers (DCs) and provides a control plane framework for many functions. In this tutorial we will configure a VXLAN based Layer 2 EVPN service 3 in a tiny CLOS fabric and at the same get to know SR Linux better! The DC fabric that we will build for this tutorial consists of the two leaf switches (acting as Top-Of-Rack) and a single spine: The two servers are connected to the leafs via an L2 interface. Service-wise the servers will appear to be on the same L2 network by means of the deployed EVPN Layer 2 service. The tutorial will consist of the following major parts: Fabric configuration - here we will configure the routing protocol in the underlay of a fabric to advertise the Virtual Tunnel Endpoints (VTEP) of the leaf switches. EVPN configuration - this chapter is dedicated to the EVPN service configuration and validation. Lab deployment # To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM: The containerlab file that describes the lab topology is referenced below in full: name : evpn01 topology : kinds : srl : image : ghcr.io/nokia/srlinux linux : image : ghcr.io/hellt/network-multitool nodes : leaf1 : kind : srl type : ixrd2 leaf2 : kind : srl type : ixrd2 spine1 : kind : srl type : ixrd3 srv1 : kind : linux srv2 : kind : linux links : # inter-switch links - endpoints : [ \"leaf1:e1-49\" , \"spine1:e1-1\" ] - endpoints : [ \"leaf2:e1-49\" , \"spine1:e1-2\" ] # server links - endpoints : [ \"srv1:eth1\" , \"leaf1:e1-1\" ] - endpoints : [ \"srv2:eth1\" , \"leaf2:e1-1\" ] Save 2 the contents of this file under evpn01.clab.yml name and you are ready to deploy: $ containerlab deploy -t evpn01.clab.yml INFO[0000] Parsing & checking topology file: evpn01.clab.yml INFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 INFO[0000] Creating root CA INFO[0001] Creating container: srv2 INFO[0001] Creating container: srv1 INFO[0001] Creating container: leaf2 INFO[0001] Creating container: spine1 INFO[0001] Creating container: leaf1 INFO[0002] Creating virtual wire: leaf1:e1-49 <--> spine1:e1-1 INFO[0002] Creating virtual wire: srv2:eth1 <--> leaf2:e1-1 INFO[0002] Creating virtual wire: leaf2:e1-49 <--> spine1:e1-2 INFO[0002] Creating virtual wire: srv1:eth1 <--> leaf1:e1-1 INFO[0003] Writing /etc/hosts file +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | 1 | clab-evpn01-leaf1 | 4b81c65af558 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.7/24 | 2001:172:20:20::7/64 | | 2 | clab-evpn01-leaf2 | de000e791dd6 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.8/24 | 2001:172:20:20::8/64 | | 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.6/24 | 2001:172:20:20::6/64 | | 4 | clab-evpn01-srv1 | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.3/24 | 2001:172:20:20::3/64 | | 5 | clab-evpn01-srv2 | fb722453d715 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.5/24 | 2001:172:20:20::5/64 | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of leaf1 : # default credentials admin:admin ssh admin@clab-evpn01-leaf1 With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey! Note We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough 4 details to survive in the configuration waters we are about to get. the following versions have been used to create this tutorial. The newer versions might work, but if they don't, please pin the version to the mentioned ones. \u21a9 Or download it with curl -LO https://github.com/srl-labs/learn-srlinux/blob/master/labs/evpn01.clab.yml \u21a9 Per RFC 8365 & RFC 7432 \u21a9 For a complete documentation coverage don't hesitate to visit our documentation portal . \u21a9","title":"Introduction"},{"location":"tutorials/l2evpn/intro/#lab-deployment","text":"To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM: The containerlab file that describes the lab topology is referenced below in full: name : evpn01 topology : kinds : srl : image : ghcr.io/nokia/srlinux linux : image : ghcr.io/hellt/network-multitool nodes : leaf1 : kind : srl type : ixrd2 leaf2 : kind : srl type : ixrd2 spine1 : kind : srl type : ixrd3 srv1 : kind : linux srv2 : kind : linux links : # inter-switch links - endpoints : [ \"leaf1:e1-49\" , \"spine1:e1-1\" ] - endpoints : [ \"leaf2:e1-49\" , \"spine1:e1-2\" ] # server links - endpoints : [ \"srv1:eth1\" , \"leaf1:e1-1\" ] - endpoints : [ \"srv2:eth1\" , \"leaf2:e1-1\" ] Save 2 the contents of this file under evpn01.clab.yml name and you are ready to deploy: $ containerlab deploy -t evpn01.clab.yml INFO[0000] Parsing & checking topology file: evpn01.clab.yml INFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 INFO[0000] Creating root CA INFO[0001] Creating container: srv2 INFO[0001] Creating container: srv1 INFO[0001] Creating container: leaf2 INFO[0001] Creating container: spine1 INFO[0001] Creating container: leaf1 INFO[0002] Creating virtual wire: leaf1:e1-49 <--> spine1:e1-1 INFO[0002] Creating virtual wire: srv2:eth1 <--> leaf2:e1-1 INFO[0002] Creating virtual wire: leaf2:e1-49 <--> spine1:e1-2 INFO[0002] Creating virtual wire: srv1:eth1 <--> leaf1:e1-1 INFO[0003] Writing /etc/hosts file +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ | 1 | clab-evpn01-leaf1 | 4b81c65af558 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.7/24 | 2001:172:20:20::7/64 | | 2 | clab-evpn01-leaf2 | de000e791dd6 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.8/24 | 2001:172:20:20::8/64 | | 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux | srl | | running | 172.20.20.6/24 | 2001:172:20:20::6/64 | | 4 | clab-evpn01-srv1 | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.3/24 | 2001:172:20:20::3/64 | | 5 | clab-evpn01-srv2 | fb722453d715 | ghcr.io/hellt/network-multitool | linux | | running | 172.20.20.5/24 | 2001:172:20:20::5/64 | +---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+ A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of leaf1 : # default credentials admin:admin ssh admin@clab-evpn01-leaf1 With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey! Note We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough 4 details to survive in the configuration waters we are about to get. the following versions have been used to create this tutorial. The newer versions might work, but if they don't, please pin the version to the mentioned ones. \u21a9 Or download it with curl -LO https://github.com/srl-labs/learn-srlinux/blob/master/labs/evpn01.clab.yml \u21a9 Per RFC 8365 & RFC 7432 \u21a9 For a complete documentation coverage don't hesitate to visit our documentation portal . \u21a9","title":"Lab deployment"},{"location":"tutorials/l2evpn/summary/","text":"Layer 2 EVPN services with VXLAN dataplane are very common in multi-tenant data centers. In this tutorial we walked through every step that is needed to configure a basic Layer 2 EVPN with VXLAN dataplane service deployed on SR Linux switches: IP fabric config using eBGP in the underlay EVPN service config on leaf switches with the control and data plane verification The highly detailed configuration & verification steps helped us achieve the goal of creating an overlay Layer 2 broadcast domain for the two servers in our topology. So that the high level service diagram transformed into a detailed map of configuration constructs and instances. During the verification phases we collected the following packet captures to prove the control/data plane behavior: Exchange of the IMET/RT3 EVPN routes . IMET/RT3 routes are the starting point in the L2 EVPN-VXLAN services, as they are used to dynamically discover the VXLAN VTEPs participating in the same EVI. Exchange of MAC-IP/RT2 EVPN routes which convey the MAC information of the attached servers. These routes are used to create unicast tunnel destinations that the dataplane frames will use. Info The more advanced EVPN topics listed below will be covered in separate tutorials: EVPN L2 multi-homing MAC mobility MAC duplication and loop protection","title":"Summary"},{"location":"yang/browser/","text":"SR Linux YANG Browser # YANG data models is the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse , search through, and process these data models. To answer these demands, we created a web portal - yang.srlinux.dev - it offers: Fast Path Browser to effectively search through thousands of available YANG paths Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux Source .yang files neatly stored in nokia/srlinux-yang-models repository for programmatic access and code generation The web portal's front page aggregates links to individual releases of YANG models. Select the needed version to open the web view of the YANG tools we offer. The main stage of the YANG Browser view is dedicated to the Path Browser , as it is the most efficient way to search through the model. Additional tools are located in the upper right corner . Let's cover them one by one. Path Browser # As was discussed before, SR Linux is a fully modeled system with its configuration and state data entirely covered with YANG models. Consequently, to access any data for configuration or state, one needs to follow the YANG model. Effectively searching for those YANG-based access paths is key to rapid development and operations. For example, how to tell which path to use to get ipv4 statistics of an interface? With Path Browser, it is possible to search through the entire SR Linux YANG model and extract the paths to the leaves of interest. The Path Browser area is composed of three main elements: search input for entering the query Config/State selector table with results for a given search input Path Browser elements A user types in a search query and the result is rendered immediately in the table with the matched words highlighted. The Config/State selector allows users to select if they want the table to show config, state or all leaves. The state leaf is a leaf that has config false statement 2 . Path structure # The table contains the flattened XPATH-like paths for every leaf of a model sorted alphabetically. Each path is denoted with a State attribute in the first column of a table. Leaves, which represent the state data, will have the true value in the first column 2 . List elements are represented in the paths as list-element[key-name=*] - a format suitable for gNMI subscriptions. Each leaf is provided with the type information. Search capabilities # Snappy search features of the Path Browser make it a joy to use when exploring the model or looking for a specific leaf of interest. Let's imagine we need to solve the simple task of subscribing to interface traffic statistics. How would we know which gNMI path corresponds to the traffic statistics counters? Should we try reading source YANG files? But it is challenging as models have lots of imports and quite some augmentations. A few moments and - you're lost. What about the tree representation of a model generated with pyang ? Searching through something like pyang's tree output is impractical since searching the tree representation can't include more than one search parameter. The search becomes a burden on operators' eyes. Path Browser to the rescue. Its ability to return search requests instantaneously makes interrogating the model a walk in the park. The animation below demos a leaf-searching exercise where a user searches for a state leaf responsible for traffic statistics. First, a user tries a logical search query interface byte , which yields some results, but it is easy to spot that they are not related to the task at hand. Thanks to the embedded highlighting capabilities, the search inputs are detectable in the resulting paths. Next, they try to use interface octets search query hoping that it will yield the right results, and so it does! Tip Every table row denotes a leaf, and when a user hovers a mouse over a certain row, the popup appears with a description of the leaf. Tree Browser # The Path Browser is great to search through the entire model, but because it works on flattened paths, it hides the \"tree\" view of the model. Sometimes the tree representation is the best side to look at the models with a naked eye, as the hierarchy becomes very clear. To not strip our users of the beloved tree view mode, we enhanced the pyang -f jstree output and named this view Tree Browser. Access Tree Browser The tree view of the model offers a step-by-step exploration of the SR Linux model going from the top-level modules all the way down to the leaves. The tree view displays the node's type (leaf/container/etc) as well as the leaf type and the read-only status of a leaf. Tree Browser view Tip Every element of a tree has a description that becomes visible if you hover over the element with a mouse. Tree and Paths # If you feel like everything in the world better be in ASCII, then Tree and Paths menu elements will satisfy the urge. These are the ASCII tree of the SR Linux model 1 and the text flattened paths that are used in the Path Browser. Text version of tree and paths The textual paths can be, for example, fetched with curl and users can sed themselves out doing comprehensive searches or path manipulations. extracted with pyang -f tree \u21a9 refer to https://datatracker.ietf.org/doc/html/rfc6020#section-4.2.3 \u21a9 \u21a9","title":"YANG Browser"},{"location":"yang/browser/#sr-linux-yang-browser","text":"YANG data models is the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse , search through, and process these data models. To answer these demands, we created a web portal - yang.srlinux.dev - it offers: Fast Path Browser to effectively search through thousands of available YANG paths Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux Source .yang files neatly stored in nokia/srlinux-yang-models repository for programmatic access and code generation The web portal's front page aggregates links to individual releases of YANG models. Select the needed version to open the web view of the YANG tools we offer. The main stage of the YANG Browser view is dedicated to the Path Browser , as it is the most efficient way to search through the model. Additional tools are located in the upper right corner . Let's cover them one by one.","title":"SR Linux YANG Browser"},{"location":"yang/browser/#path-browser","text":"As was discussed before, SR Linux is a fully modeled system with its configuration and state data entirely covered with YANG models. Consequently, to access any data for configuration or state, one needs to follow the YANG model. Effectively searching for those YANG-based access paths is key to rapid development and operations. For example, how to tell which path to use to get ipv4 statistics of an interface? With Path Browser, it is possible to search through the entire SR Linux YANG model and extract the paths to the leaves of interest. The Path Browser area is composed of three main elements: search input for entering the query Config/State selector table with results for a given search input Path Browser elements A user types in a search query and the result is rendered immediately in the table with the matched words highlighted. The Config/State selector allows users to select if they want the table to show config, state or all leaves. The state leaf is a leaf that has config false statement 2 .","title":"Path Browser"},{"location":"yang/browser/#path-structure","text":"The table contains the flattened XPATH-like paths for every leaf of a model sorted alphabetically. Each path is denoted with a State attribute in the first column of a table. Leaves, which represent the state data, will have the true value in the first column 2 . List elements are represented in the paths as list-element[key-name=*] - a format suitable for gNMI subscriptions. Each leaf is provided with the type information.","title":"Path structure"},{"location":"yang/browser/#search-capabilities","text":"Snappy search features of the Path Browser make it a joy to use when exploring the model or looking for a specific leaf of interest. Let's imagine we need to solve the simple task of subscribing to interface traffic statistics. How would we know which gNMI path corresponds to the traffic statistics counters? Should we try reading source YANG files? But it is challenging as models have lots of imports and quite some augmentations. A few moments and - you're lost. What about the tree representation of a model generated with pyang ? Searching through something like pyang's tree output is impractical since searching the tree representation can't include more than one search parameter. The search becomes a burden on operators' eyes. Path Browser to the rescue. Its ability to return search requests instantaneously makes interrogating the model a walk in the park. The animation below demos a leaf-searching exercise where a user searches for a state leaf responsible for traffic statistics. First, a user tries a logical search query interface byte , which yields some results, but it is easy to spot that they are not related to the task at hand. Thanks to the embedded highlighting capabilities, the search inputs are detectable in the resulting paths. Next, they try to use interface octets search query hoping that it will yield the right results, and so it does! Tip Every table row denotes a leaf, and when a user hovers a mouse over a certain row, the popup appears with a description of the leaf.","title":"Search capabilities"},{"location":"yang/browser/#tree-browser","text":"The Path Browser is great to search through the entire model, but because it works on flattened paths, it hides the \"tree\" view of the model. Sometimes the tree representation is the best side to look at the models with a naked eye, as the hierarchy becomes very clear. To not strip our users of the beloved tree view mode, we enhanced the pyang -f jstree output and named this view Tree Browser. Access Tree Browser The tree view of the model offers a step-by-step exploration of the SR Linux model going from the top-level modules all the way down to the leaves. The tree view displays the node's type (leaf/container/etc) as well as the leaf type and the read-only status of a leaf. Tree Browser view Tip Every element of a tree has a description that becomes visible if you hover over the element with a mouse.","title":"Tree Browser"},{"location":"yang/browser/#tree-and-paths","text":"If you feel like everything in the world better be in ASCII, then Tree and Paths menu elements will satisfy the urge. These are the ASCII tree of the SR Linux model 1 and the text flattened paths that are used in the Path Browser. Text version of tree and paths The textual paths can be, for example, fetched with curl and users can sed themselves out doing comprehensive searches or path manipulations. extracted with pyang -f tree \u21a9 refer to https://datatracker.ietf.org/doc/html/rfc6020#section-4.2.3 \u21a9 \u21a9","title":"Tree and Paths"},{"location":"yang/yang/","text":"Model-driven (MD) interfaces are becoming essential for robust and modern Network OSes. The changes required to create fully model-driven interfaces can not happen overnight - it is a long and tedious process that requires substantial R&D effort. Traditional Network OSes often had to take an evolutionary route with adding MD interfaces on top of the existing internal infrastructure. SR Linux ground-up support for YANG Unfortunately, bolting on model-driven interfaces while keeping the legacy internal infrastructure layer couldn't fully deliver on the promises of MD interfaces. In reality, those new interfaces had visibility discrepancies 1 , which often led to a situation where users needed to mix and match different interfaces to achieve some configuration goal. Apparently, without adopting a fully modeled universal API, it is impossible to make a uniform set of interfaces offering the same visibility level into the NOS. Nokia SR Linux was ground-up designed with YANG 2 data modeling taking a central role. SR Linux makes extensive use of structured data models with each application regardless if it's being provided by Nokia or written by a user has a YANG model that defines its configuration and state. Both Nokia and customer's apps are modeled in YANG SR Linux exposes the YANG models to the supported management APIs. For example, the command tree in the CLI is derived from the SR Linux YANG models loaded into the system, and a gNMI client uses RPCs to configure an application based on its YANG model. When a configuration is committed, the SR Linux management server validates the YANG models and translates them into protocol buffers for the impart database (IDB). With this design, there is no way around YANG; the data model is defined first for any application SR Linux has, then the CLI, APIs, and show output formats derived from it. SR Linux YANG Models # As YANG models play a central role in SR Linux NOS, it is critical to have unobstructed access. With that in mind, we offer SR Linux users many ways to get ahold of SR Linux YANG models: Download modules from SR Linux NOS itself. The models can be found at /opt/srlinux/models/* location. Fetch modules from nokia/srlinux-yang-models repo. Use SR Linux YANG Browser to consume modules in a human-friendly way SR Linux employs a uniform mapping between a YANG module name and the CLI context, making it easy to correlate modules with CLI contexts. YANG modules and CLI aligned The structure of the Nokia SR Linux native models may look familiar to the OpenConfig standard, where different high-level domains are contained in their modules. Source .yang files are great for YANG-based automation tools such as ygot but are not so easy for a human's eye. For living creatures, we offer a YANG Browser portal. We suggest people use it when they want to consume the models in a non-programmable way. indicated by the blue color on the diagram and explained in detail in NFD25 talk . \u21a9 RFC 6020 and RFC 7950 \u21a9","title":"SR Linux & YANG"},{"location":"yang/yang/#sr-linux-yang-models","text":"As YANG models play a central role in SR Linux NOS, it is critical to have unobstructed access. With that in mind, we offer SR Linux users many ways to get ahold of SR Linux YANG models: Download modules from SR Linux NOS itself. The models can be found at /opt/srlinux/models/* location. Fetch modules from nokia/srlinux-yang-models repo. Use SR Linux YANG Browser to consume modules in a human-friendly way SR Linux employs a uniform mapping between a YANG module name and the CLI context, making it easy to correlate modules with CLI contexts. YANG modules and CLI aligned The structure of the Nokia SR Linux native models may look familiar to the OpenConfig standard, where different high-level domains are contained in their modules. Source .yang files are great for YANG-based automation tools such as ygot but are not so easy for a human's eye. For living creatures, we offer a YANG Browser portal. We suggest people use it when they want to consume the models in a non-programmable way. indicated by the blue color on the diagram and explained in detail in NFD25 talk . \u21a9 RFC 6020 and RFC 7950 \u21a9","title":"SR Linux YANG Models"}]}