{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"alwayson/","text":"<p>It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab.</p> <p>But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.</p>","title":"Always-ON SR Linux Instance"},{"location":"alwayson/#what-is-always-on-sr-linux-for","text":"<p>The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user.</p> <ul> <li> <p>getting to know SR Linux CLI     SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier.     New users can make their first steps by looking at the <code>show</code> commands, exploring the datastores, running <code>info from</code> commands and getting the grips of configuration basics by entering into the configuration mode.</p> </li> <li> <p>YANG browsing     By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers.</p> </li> <li> <p>gNMI exploration     The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux.     gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves.</p> </li> </ul>","title":"What is Always-ON SR Linux for?"},{"location":"alwayson/#connection-details","text":"<p>Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces:</p>    Method Details     SSH address: <code>ssh guest@on.srlinux.dev -p 44268</code>password: <code>n0k1asrlinux</code>for key-based authentication use this key to authenticate the <code>guest</code> user   gNMI1 <pre><code>gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\      capabilities</code></pre>   JSON-RPC2 http://http.on.srlinux.dev","title":"Connection details"},{"location":"alwayson/#gnmi","text":"<p>SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate.</p>","title":"gNMI"},{"location":"alwayson/#guest-user","text":"<p>The <code>guest</code> user has the following settings applied to it:</p> <ol> <li>Read-only mode</li> <li><code>bash</code> and <code>file</code> commands are disabled</li> </ol> <p>Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that <code>guest</code> can't commit them.</p>","title":"Guest user"},{"location":"alwayson/#always-on-sandbox-setup","text":"<p>The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node.</p>","title":"Always-ON sandbox setup"},{"location":"alwayson/#protocols-and-services","text":"<p>We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise.</p> <p>The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services.</p> <p>In the overlay the following services are configured:</p> <ol> <li>Layer 2 EVPN with VXLAN dataplane1 with <code>mac-vrf-100</code> network instance created on SR Linux</li> <li>Layer 3 EVPN with VXLAN dataplane with <code>ip-vrf-200</code> network instance created on SR Linux</li> </ol>   <ol> <li> <p>check this tutorial to understand how this service is configured\u00a0\u21a9\u21a9</p> </li> <li> <p>HTTP service running over port 80\u00a0\u21a9</p> </li> </ol>","title":"Protocols and Services"},{"location":"community/","text":"","title":"Community"},{"location":"community/#discord-server","text":"<p>SR Linux has lots to offer to various groups of engineers...</p> <p>Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS.</p> <p>Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI.</p> <p>Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support.</p> <p>We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community1 is the SR Linux Discord Server which everyone can join!</p> <p> Join SR Linux Discord Server</p>","title":"Discord server"},{"location":"community/#always-on-sr-linux","text":"<p>It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab.</p> <p>But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.</p>   <ol> <li> <p>this is an unofficial community. Engineers to engineers.\u00a0\u21a9</p> </li> </ol>","title":"Always-ON SR Linux"},{"location":"get-started/","text":"<p>SR Linux packs a lot of unique features that the data center networking teams can leverage. Some of the features being truly new to the networking domain. The goal of this portal is to introduce SR Linux to the visitors through the interactive tutorials centered around SR Linux services and capabilities.</p> <p>We believe that learning by doing yields the best results. With that in mind we made SR Linux container image available to everybody without any registration or licensing requirements </p> <p>The public SR Linux container image when powered by containerlab allows us to create easily deployable labs that everyone can launch at their convenience. All that to let you not only read about the features we offer, but to try them live!</p>","title":"Get SR Linux"},{"location":"get-started/#sr-linux-container-image","text":"<p>A single container image that hosts management, control and data plane functions is all you need to get started.</p>","title":"SR Linux container image"},{"location":"get-started/#getting-the-image","text":"<p>To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry. This means that you can pull SR Linux container image exactly the same way as you would pull any other image:</p> <pre><code>docker pull ghcr.io/nokia/srlinux\n</code></pre> <p>When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the <code>nokia/srlinux</code> image has and change the <code>docker pull</code> command accordingly.</p>","title":"Getting the image"},{"location":"get-started/#running-sr-linux","text":"<p>When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces.</p> <p>A system on which you can run SR Linux containers should conform to the following requirements:</p> <ol> <li>Linux OS with a kernel v4+1.</li> <li>Docker container runtime.</li> <li>At least 2 vCPU and 4GB RAM.</li> <li>A user with administrative privileges.</li> </ol> <p>Let's explore the different ways you can launch SR Linux container.</p>","title":"Running SR Linux"},{"location":"get-started/#docker-cli","text":"<p><code>docker</code> CLI offers a quick way to run standalone SR Linux container:</p> <pre><code>docker run -t -d --rm --privileged \\\n  -u $(id -u):$(id -g) \\\n  --name srlinux ghcr.io/nokia/srlinux \\\n  sudo bash /opt/srlinux/bin/sr_linux\n</code></pre> <p>The above command will start the container named <code>srlinux</code> on the host system with a single management interface attached to the default docker network.</p> <p>This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work.</p> <p>For multi-node SR Linux deployments containerlab3 offers a better way.</p>","title":"Docker CLI"},{"location":"get-started/#containerlab","text":"<p>Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle.</p> <p>A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together:</p> <pre><code># file: srlinux.clab.yml\nname: srlinux\n\ntopology:\n  nodes:\n    srl1:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n    srl2:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <p>By copying this file over to your system you can immediately deploy it with containerlab:</p> <p><pre><code>containerlab deploy -t srlinux.clab.yml\n</code></pre> <pre><code>INFO[0000] Parsing &amp; checking topology file: srlinux.clab.yml \nINFO[0000] Creating lab directory: /root/demo/clab-srlinux \nINFO[0000] Creating container: srl1                     \nINFO[0000] Creating container: srl2                     \nINFO[0001] Creating virtual wire: srl1:e1-1 &lt;--&gt; srl2:e1-1 \nINFO[0001] Writing /etc/hosts file                      \n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |         Image         | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlinux-srl1  | 50826b3e3703 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n| 2 | clab-srlinux-srl2  | 4d4494aba320 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n</code></pre></p>","title":"Containerlab"},{"location":"get-started/#deployment-verification","text":"<p>Regardless of the way you spin up SR Linux container it will be visible in the output of the <code>docker ps</code> command. If the deployment process went well and the container did not exit, a user can see it with <code>docker ps</code> command:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                      COMMAND                  CREATED             STATUS             PORTS                    NAMES\n4d4494aba320   ghcr.io/nokia/srlinux      \"/tini -- fixuid -q \u2026\"   32 minutes ago      Up 32 minutes                               clab-learn-01-srl2\n</code></pre> <p>The logs of the running container can be displayed with <code>docker logs &lt;container-name&gt;</code>.</p> <p>In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the <code>docker ps</code> output as this command only shows running containers. Containers which are in the exited status will be part of the <code>docker ps -a</code> output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination.</p>","title":"Deployment verification"},{"location":"get-started/#connecting-to-sr-linux","text":"<p>When SR Linux container is up and running, users can connect to it over different interfaces.</p>","title":"Connecting to SR Linux"},{"location":"get-started/#cli","text":"<p>One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface.</p> <p>To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the <code>sr_cli</code> process inside of it:</p> <p><pre><code># get SR Linux container name -&gt; clab-srl01-srl\n$ docker ps\nCONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS         PORTS                    NAMES\n17a47c58ad59   ghcr.io/nokia/srlinux             \"/tini -- fixuid -q \u2026\"   10 seconds ago   Up 6 seconds                            clab-learn-01-srl1\n</code></pre> <pre><code># start the sr_cli process inside this container to get access to CLI\ndocker exec -it clab-learn-01-srl1 sr_cli\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           \nA:srl1#\n</code></pre></p> <p>The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials <code>admin:admin</code> you can connect to the CLI over the network:</p> <pre><code># containerlab creates local /etc/hosts entries\n# for container names to resolve to their IP\nssh admin@clab-learn-01-srl1\n\nadmin@clab-learn-01-srl1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           \nA:srl1#\n</code></pre>","title":"CLI"},{"location":"get-started/#gnmi","text":"<p>SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400.</p> <p>Using the gNMI client2 users can explore SR Linux' gNMI interface:</p> <pre><code>gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities\ngNMI version: 0.7.0\nsupported models:\n  - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31\n  - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30\n  - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31\n&lt;SNIP&gt;\n</code></pre>   <ol> <li> <p>Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container\u00a0\u21a9</p> </li> <li> <p>for example gnmic \u21a9</p> </li> <li> <p>The labs referenced on this site are deployed with containerlab unless stated otherwise\u00a0\u21a9</p> </li> </ol>","title":"gNMI"},{"location":"kb/cfgmgmt/","text":"<p>SR Linux employs a transaction-based configuration management system. That allows for a number of changes to be made to the configuration with an explicit <code>commit</code> required to apply the changes as a single transaction.</p>","title":"Configuration management"},{"location":"kb/cfgmgmt/#configuration-file","text":"<p>The default location for the configuration file is <code>/etc/opt/srlinux/config.json</code>.</p> <p>If there is no configuration file present, a basic configuration file is auto-generated with the following defaults:</p> <ul> <li>Creation of a management network instance</li> <li>Management interface is added to the mgmt network instance</li> <li>DHCP v4/v6 is enabled on mgmt interface</li> <li>A set of default of logs are created</li> <li>SSH server is enabled</li> <li>Some default IPv4/v6 CPM filters</li> </ul>","title":"Configuration file"},{"location":"kb/cfgmgmt/#configuration-modes","text":"<p>Configuration modes define how the system is running when transactions are performed. Supported modes are the following:</p> <ul> <li>Running: the default mode when logging in and displays displays the currently running or active configuration.</li> <li>State: the running configuration plus the addition of any dynamically added data.  Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc.</li> <li>Candidate: this mode is used to modify configuration. Modifications are not applied until the <code>commit</code> is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes:<ul> <li>Shared: this is the default mode when entering the candidate mode with <code>enter candidate</code> command.  This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied.</li> <li>Exclusive Candidate: When entering candidate mode with <code>enter candidate exclusive</code>, it locks out other users from making changes to the candidate configuration.   You can enter candidate exclusive mode only under the following conditions:  <ul> <li>The current shared candidate configuration has not been modified.</li> <li>There are no other users in candidate shared mode.</li> <li>No other users have entered candidate exclusive mode.</li> </ul> </li> <li>Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed.   When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name <code>private-&lt;username&gt;</code> unless a unique name is defined.</li> </ul> </li> </ul>  <p>Note</p> <p>gNMI &amp; JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.</p>","title":"Configuration modes"},{"location":"kb/cfgmgmt/#setting-the-configuration-mode","text":"<p>After logging in to the CLI, you are initially placed in <code>running</code> mode. The following table provides commands to enter in a specific mode:</p>    Candidate mode Command to enter     Candidate shared <code>enter candidate</code>   Candidate mode for named shared candidate <code>enter candidate name &lt;name&gt;</code>   Candidate private <code>enter candidate private</code>   Candidate mode for named private candidate <code>enter candidate private name &lt;name&gt;</code>   Candidate exclusive <code>enter candidate exclusive</code>   Exclusive mode for named candidate <code>enter candidate exclusive name &lt;name&gt;</code>   Running <code>enter running</code>   State <code>enter state</code>   Show <code>enter show</code>","title":"Setting the configuration mode"},{"location":"kb/cfgmgmt/#committing-configuration","text":"<p>Changes made during a configuration modification session do not take effect until a <code>commit</code> command is issued. Several options are available for <code>commit</code> command, below are the most notable ones:</p>    Option Action     <code>commit now</code> Apply the changes, exit candidate mode, and enter running mode   <code>commit stay</code> Apply the changes and then remain in candidate mode   <code>commit save</code> Apply the changes and automatically save the commit to the startup configuration   <code>commit confirmed</code> Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted","title":"Committing configuration"},{"location":"kb/cfgmgmt/#deleting-configuration","text":"<p>Use the <code>delete</code> command to delete configurations while in candidate mode.</p> <p>The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: <pre><code>--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n            login-banner \"Welcome to SRLinux!\"\n        }\n    }\n\n--{ candidate shared default}--[ ]--\nA:leaf1# delete system banner\n\n--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n        }\n    }\n</code></pre></p>","title":"Deleting configuration"},{"location":"kb/cfgmgmt/#discarding-configuration","text":"<p>You can discard previously applied configurations with the <code>discard</code> command.</p> <ul> <li>To discard the changes and remain in candidate mode with a new candidate session, enter <code>discard stay</code>.</li> <li>To discard the changes, exit candidate mode, and enter running mode, enter <code>discard now</code>.</li> </ul>","title":"Discarding configuration"},{"location":"kb/cfgmgmt/#displaying-configuration-diff","text":"<p>Use the <code>diff</code> command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore.</p> <p>The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination.</p> <p>Global arguments include: <code>baseline</code>, <code>candidate</code>, <code>checkpoint</code>, <code>factory</code>, <code>file</code>, <code>from</code>, <code>rescue</code>, <code>running</code>, and <code>startup</code>.</p> <p>The diff command can be used outside of candidate mode, but only if used with arguments.</p> <p>The following shows a basic <code>diff</code> command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: <pre><code>--{ candidate shared default }--[ ]--\n# interface ethernet-1/1 admin-state disable\n\n--{ * candidate shared default }--[ ]--\n# interface ethernet-1/2 description \"updated\"\n\n--{ * candidate shared default }--[ ]--\n# diff\n    interface ethernet-1/1 {\n+       admin-state disable\n    }\n+   interface ethernet-1/2 {\n+       description updated\n+   }\n</code></pre></p>","title":"Displaying configuration diff"},{"location":"kb/cfgmgmt/#displaying-configuration-details","text":"<p>Use the <code>info</code> command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context.</p> <p>To display the entire configuration, enter <code>info</code> from the root context: <pre><code>--{ candidate shared default}--[ ]--\n# info\n&lt;all the configuration is displayed&gt;\n--{ candidate }--[ ]--\n</code></pre></p> <p>To display the configuration for a specific context, enter info and specify the context: <pre><code>--{ candidate shared default}--[ ]--\n# info system lldp\n    system {\n        lldp {\n            admin-state enable\n            hello-timer 600\n            management-address mgmt0.0 {\n                type [\n                    IPv4\n                ]\n            }\n            interface mgmt0 {\n                admin-state disable\n            }\n        }\n    }\n</code></pre></p> <p>The following <code>info</code> command options are rather useful:</p> <ul> <li><code>as-json</code> - to display JSON-formatted output</li> <li><code>detail</code> - to display values for all parameters, including those not specifically configured</li> <li><code>flat</code> -  to display the output as a series of set statements, omitting indentation for any sub-contexts</li> </ul>","title":"Displaying configuration details"},{"location":"kb/hwtypes/","text":"<p>The SR Linux software supports seven Nokia hardware platforms1:</p> <ul> <li>7250 IXR-6</li> <li>7250 IXR-10</li> <li>7220 IXR-D1</li> <li>7220 IXR-D2</li> <li>7220 IXR-D3</li> <li>7220 IXR-H2</li> <li>7220 IXR-H3</li> </ul> <p>The <code>type</code> field under the node configuration sets the emulated hardware type in the containerlab file:</p> <pre><code># part of the evpn01.clab.yml file\n  nodes:\n    leaf1:\n      kind: srl\n      type: ixrd3 # &lt;- hardware type this node will emulate\n</code></pre> <p>The <code>type</code> field defines the hardware variant that this SR Linux node will emulate. The available <code>type</code> values are:</p>    type value HW platform     ixr6 7250 IXR-6   ixr10 7250 IXR-10   ixrd1 7220 IXR-D1   ixrd2 7220 IXR-D2   ixrd3 7220 IXR-D3   ixrh2 7220 IXR-H2   ixrh3 7220 IXR-H3     <p>Tip</p> <p>Containerlab-launched nodes are started as <code>ixrd2</code> hardware type unless set to a different type in the clab file.</p>    <ol> <li> <p>SR Linux can also run on the whitebox/3rd party switches.\u00a0\u21a9</p> </li> </ol>","title":"Hardware types"},{"location":"kb/ifaces/","text":"<p>On the SR Linux, an interface is any physical or logical port through which packets can be sent to or received from other devices.</p>","title":"Interfaces"},{"location":"kb/ifaces/#loopback","text":"<p>Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named <code>loN</code>, where N is 0 to 255.</p>","title":"Loopback"},{"location":"kb/ifaces/#system","text":"<p>The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces:</p> <ul> <li>The system interface can be bound to the default network-instance only.</li> <li>The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses.</li> <li>The system interface cannot be administratively disabled. Once configured, it is always up.</li> </ul> <p>The SR Linux supports a single system interface named <code>system0</code>. When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels.</p>","title":"System"},{"location":"kb/ifaces/#network","text":"<p>Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic.</p> <p>The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options.</p> <p>In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: <code>ethernet-slot/port</code>. For example, interface <code>ethernet-2/1</code> refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card.</p> <p>On 7220 IXR-D3 systems, the QSFP28 connector ports (ports <code>1/3-1/33</code>) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: <code>ethernet-slot/port/breakout-port</code>.</p> <p>For example, if interface <code>ethernet 1/3</code> is enabled for breakout mode, its breakout ports are named as follows:</p> <ul> <li><code>ethernet 1/3/1</code></li> <li><code>ethernet 1/3/2</code></li> <li><code>ethernet 1/3/3</code></li> <li><code>ethernet 1/3/4</code></li> </ul>","title":"Network"},{"location":"kb/ifaces/#management","text":"<p>Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named <code>mgmt0</code>. The <code>mgmt0</code> interface supports the same functionality and defaults as a network interface, except for the following:</p> <ul> <li>Packets sent and received on the mgmt0 interface are processed completely in software.</li> <li>The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class.</li> <li>The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port.</li> </ul>","title":"Management"},{"location":"kb/ifaces/#integrated-routing-and-bridging-irb","text":"<p>IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface.</p>","title":"Integrated Routing and Bridging (IRB)"},{"location":"kb/ifaces/#subinterfaces","text":"<p>On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface.</p> <p>Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags.</p> <p>While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance.</p> <p>You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface.</p> <p>The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class</p> <p>SR Linux subinterfaces can be specified as type routed or bridged:</p> <ul> <li>Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf.</li> <li>Bridged subinterfaces can be assigned to a network-instance of type mac-vrf.</li> </ul> <p>Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.</p>","title":"Subinterfaces"},{"location":"kb/mgmt/","text":"<p>Nokia SR Linux is equipped with 100% YANG modelled management interfaces. The supported management interfaces (CLI, JSON-RPC, and gNMI) access the common management API layer via a gRPC interface. Since all interfaces act as a client towards a common management API, SR Linux provides complete consistency across all the management interfaces with regards to the capabilities available to each of them.</p>","title":"Management interfaces"},{"location":"kb/mgmt/#sr-linux-cli","text":"<p>The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session.</p> <p>Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces.</p>","title":"SR Linux CLI"},{"location":"kb/mgmt/#features","text":"<ul> <li>Output Modifiers.   Advanced Linux output modifiers <code>grep</code>, <code>more</code>, <code>wc</code>, <code>head</code>, and <code>tail</code> are exposed directly through the SR Linux CLI.</li> <li>Suggestions &amp; List Completions.   As commands are typed suggestions are provided.  Tab can be used to list options available.</li> <li>Output Format.   When displaying info from a given datastore, the output can be formatted in one of three ways:<ul> <li>Text: this is the default out, it is JSON-like but not quite JSON.</li> <li>JSON: the output will be in JSON format.</li> <li>Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful.</li> </ul> </li> <li>Aliases.   An alias is used to map a CLI command to a shorter easier to remember command.  For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long.   An alias could be configured so that a shorter string of text could be used to execute that long CLI command.  Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.</li> </ul>","title":"Features"},{"location":"kb/mgmt/#accessing-the-cli","text":"<p>After the SR Linux device is initialized, you can access the CLI using a console or SSH connection.</p> <p>Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to <code>leaf1</code>:</p> <p><pre><code>ssh admin@clab-quickstart-leaf1\n</code></pre> <pre><code>Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts.\nadmin@clab-quickstart-leaf1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:leaf1#\n</code></pre></p>","title":"Accessing the CLI"},{"location":"kb/mgmt/#prompt","text":"<p>By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: <pre><code>--{ modified? mode_and_session_type }--[ context ]--\nhostname#\n</code></pre></p> <p>Example: <pre><code>--{ * candidate shared }--[ acl ]--\n3-node-srlinux-A#\n</code></pre></p> <p>The CLI prompt is configurable and can be changed within the <code>environment prompt</code> configuration context.</p> <p>In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays:</p> <ul> <li>the current mode and session type</li> <li>whether the configuration has been modified</li> <li>the user name and session ID of the current AAA session</li> <li>and the local time</li> </ul> <p>For example: <pre><code>Current mode: * candidate shared     root (36)   Wed 09:52PM\n</code></pre></p>","title":"Prompt"},{"location":"kb/mgmt/#gnmi","text":"<p>The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system.</p> <p>SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information.</p> <p>Supported gNMI RPCs are:</p> <ul> <li>Get</li> <li>Set</li> <li>Subscribe</li> <li>Capabilities</li> </ul>","title":"gNMI"},{"location":"kb/mgmt/#json-rpc","text":"<p>The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system.</p> <p>If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.</p>","title":"JSON-RPC"},{"location":"kb/netwinstance/","text":"<p>On the SR Linux, you can configure one or more virtual routing instances, known as network instances. Each network instance has its own interfaces, its own protocol instances, its own route table, and its own FIB.</p> <p>When a packet arrives on a subinterface associated with a network instance, it is forwarded according to the FIB of that network instance. Transit packets are normally forwarded out another subinterface of the network instance.</p> <p>SR Linux supports the following types of network instances:</p> <ul> <li>default</li> <li>ip-vrf</li> <li>mac-vrf</li> </ul> <p>The initial startup configuration for SR Linux has a single <code>default</code> network instance.</p> <p>By default, there are no ip-vrf or mac-vrf network instances; these must be created by explicit configuration. The ip-vrf network instances are the building blocks of Layer 3 IP VPN services, and mac-vrf network instances are the building blocks of EVPN services.</p> <p>Within a network instance, you can configure BGP, OSPF, and IS-IS protocol options that apply only to that network instance.</p>","title":"Network instances"},{"location":"ndk/intro/","text":"<p>Nokia SR Linux enables its users to create high-performance applications which run alongside native apps on SR Linux Network OS. These \"on-box custom applications\" can be deeply integrated with the rest of the SR Linux system and thus can perform tasks that are not possible with traditional management interfaces standard for the typical network operating systems.</p>  <p> </p> Custom applications run natively on SR Linux NOS  <p>The on-box applications (which we also refer to as \"agents\") leverage the SR Linux SDK called NetOps Development Kit or NDK for short.</p> <p>Applications developed with SR Linux NDK have a set of unique characteristics which set them aside from the traditional off-box automation solutions:</p> <ol> <li>Native integration with SR Linux system     SR Linux architecture is built so that NDK agents look and feel like any other regular application such as bgp or acl. This seamless integration is achieved on several levels:<ol> <li>System integration: when deployed on SR Linux system, an NDK agent renders itself like any other \"standard\" application. That makes lifecycle management unified between Nokia-provided system apps and custom agents.</li> <li>CLI integration: every NDK agent automatically becomes a part of the global CLI tree, making it possible to configure the agent and query its state the same way as for any other configuration region.</li> <li>Telemetry integration: an NDK agent configuration and state data will automatically become available for Streaming Telemetry consumption.</li> </ol> </li> <li>Programming language-neutral     With SR Linux NDK, the developers are not forced to use any particular language when writing their apps. As NDK is a gRPC service defined with Protocol Buffers, it is possible to use any1 programming language for which protobuf compiler is available. </li> <li>Deep integration with system components     NDK apps are not constrained to only configuration and state management, as often happens with traditional north-bound interfaces. On the contrary, the NDK service exposes additional services that enable deep integration with the SR Linux system, such as listening to RIB/FIB updates or having direct access to the datapath.</li> </ol> <p>With the information outlined in the NDK Developers Guide, you will learn about NDK architecture and how to develop apps with this kit.</p> <p>Please navigate to the Apps Catalog to browse our growing list of NDK apps that Nokia or 3rd parties wrote.</p>   <ol> <li> <p>This in practice covers all popular programming languages: Python, Go, C#, C, C++, Java, JS, etc.\u00a0\u21a9</p> </li> </ol>","title":"NetOps Development Kit"},{"location":"ndk/apps/catalog/","text":"<p>SR Linux NetOps Development Kit (NDK) enables its users to write apps to solve many automation tasks, operational hurdles, or optimization problems.</p> <p>gRPC based service that provides deep integration with Network OS is quite a novel thing for a networking domain, making NDK application examples the second most valuable asset after the NDK documentation. Sometimes the best applications are born after getting inspired by others' work or ideas implemented in different projects.</p> <p>With the App Catalog, we intend to collect references to the noteworthy NDK applications that Nokia engineers or 3rd parties have open-sourced. With that growing catalog of examples, we hope that both new and seasoned NDK users will find something that can inspire them to create their next app.</p>  <p>Disclaimer</p> <p>The examples listed in the App Catalog are not of production quality and should not be used \"as is.\" Visitors of App Catalog should treat those applications/agents as demo examples of what can be achieved with NDK.</p> <p>The applications kept under <code>srl-labs</code> or <code>nokia</code> GitHub organizations are not official Nokia products unless explicitly mentioned.</p>","title":"App Catalog"},{"location":"ndk/apps/catalog/#ndk-agents","text":"","title":"NDK agents"},{"location":"ndk/apps/catalog/#evpn-proxy","text":"<p> \u00b7 <code>jbemmel/srl-evpn-proxy</code></p> <p>SR Linux EVPN Proxy agent that allows bridging EVPN domains with domains that only employ static VXLAN.  Read more</p>","title":"EVPN Proxy"},{"location":"ndk/apps/catalog/#kbutler","text":"<p> \u00b7 <code>brwallis/srlinux-kbutler</code></p> <p>kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service's external IP with a next-hop of the worker node.  Read more</p>","title":"kButler"},{"location":"ndk/apps/catalog/#prometheus-exporter","text":"<p> \u00b7 <code>karimra/srl-prometheus-exporter</code></p> <p>SR Linux Prometheus Exporter agent creates Prometheus scrape-able endpoints on individual switches. This horizontally-scaled telemetry collection model has additional operational enhancements over traditional setups with a central telemetry collector.  Read more</p>","title":"Prometheus Exporter"},{"location":"ndk/apps/evpn-proxy/","text":"Description SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN   Components Nokia SR Linux, Cumulus VX   Programming Language Python   Source Code <code>jbemmel/srl-evpn-proxy</code>   Authors Jeroen van Bemmel","title":"SR Linux EVPN Proxy"},{"location":"ndk/apps/evpn-proxy/#introduction","text":"<p>Most data center designs start small before they evolve. At small scale, it may make sense to manually configure static VXLAN tunnels between leaf switches, as implemented on the 2 virtual lab nodes on the left side. </p> <p></p> <p>There is nothing wrong with such an initial design, but as the fabric grows and the number of leaves reaches a certain threshold, having to touch every switch each time a device is added can get cumbersome and error prone.</p> <p>The internet and most modern large scale data center designs use dynamic control plane protocols and volatile in-memory configuration to configure packet forwarding. BGP is a popular choice, and the Ethernet VPN address family (EVPN RFC8365) can support both L2 and L3 overlay services. However, legacy fabrics continue to support business critical applications, and there is a desire to keep doing so without service interruptions, and with minimal changes.</p> <p>So how can we move to the new dynamic world of EVPN based data center fabrics, while transitioning gradually and smoothly from these static configurations?</p>","title":"Introduction"},{"location":"ndk/apps/evpn-proxy/#evpn-proxy-agent","text":"<p>The <code>evpn-proxy</code> agent developed with NDK can answer the need of gradually transitioning from the static VXLAN dataplane to the EVPN based service. It has a lot of embedded functionality, we will cover the core feature here which is the Static VXLAN &lt;-&gt; EVPN Proxy functionality for point to point tunnels.</p> <p>The agent gets installed on SR Linux NOS and enables the control plane stitching between static VXLAN VTEP and EVPN-enabled service by generating EVPN routes on behalf of a legacy VTEP device.</p> <p></p>","title":"EVPN Proxy Agent"},{"location":"ndk/apps/kbutler/","text":"Description kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node   Components Nokia SR Linux, Kubernetes, MetalLB   Programming Language Go   Source Code <code>brwallis/srlinux-kbutler</code>   Additional resources This agent was demonstrated at NFD 25   Authors Bruce Wallis","title":"kButler - k8s aware agent"},{"location":"ndk/apps/kbutler/#introduction","text":"<p>In the datacenter fabrics where applications run in Kubernetes clusters it is common to see Metallb to be used as a mean to advertise k8s services external IP addresses towards the fabric switches over BGP.</p>  <p> </p> kButler agent demo setup  <p>From the application owner standpoint as long as all the nodes advertise IP addresses of the application-related services things are considered to work as expected. But applications users do not get connected to the apps directly, there is always a network in-between which needs to play in unison with the applications.</p> <p>How can we make sure, that the network state matches the expectations of the applications? The networking folks may have little to no visibility into the application land, thus they may not have the necessary information to say if a network state reflects the applications configuration.</p> <p>Consider the diagram above, and the following state of affairs:</p> <ul> <li>application App1 is scaled to run on all three nodes of a cluster</li> <li>a service is created to make this application available from the outside of the k8s cluster</li> <li>all three nodes advertise the virtual IP of the App1 with its own nexthop via BGP</li> </ul> <p>If all goes well, the Data Center leaf switch will install three routes in its forwarding and will ECMP load balance requests towards the nodes running application pods.</p> <p>But what if the leaf switch has installed only two routes in its FIB? This can be a result of a fat fingering during the BGP configuration, or a less likely event of a resources congestion. In any case, the disparity between the network state and the application can arise.</p> <p>The questions becomes, how can we make the network to be aware of the applications configuration and make sure that those deviations can be easily spotted by the NetOps teams?</p>","title":"Introduction"},{"location":"ndk/apps/kbutler/#kbutler","text":"<p>The kButler NDK agent is designed to demonstrate how data center switches can tap into the application land and correlated the network state with the application configuration.</p> <p>At a high level, the agent does the following:</p> <ul> <li>subscribes to the K8S service API and is specifically interested in any new services being exposed or changes to existing exposed services. Objective is to gain view of which worker nodes host an application which has an associated exposed service</li> <li>subscribes to the SR Linux NDK API listening for any changes to the FIB</li> <li>ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node</li> <li>reports the operational status within SR Linux allowing quick alerts of any K8S service degradation</li> <li>provides contextualized monitoring, alerting and troubleshooting, exposing its data model through all available SR Linux management interfaces</li> </ul>","title":"kButler"},{"location":"ndk/apps/srl-prom-exporter/","text":"Description SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector.   Components Nokia SR Linux, Prometheus   Programming Language Go   Source Code <code>karimra/srl-prometheus-exporter</code>   Authors Karim Radhouani","title":"SR Linux Prometheus Exporter"},{"location":"ndk/apps/srl-prom-exporter/#introduction","text":"<p>Most Streaming Telemetry stacks are built with a telemetry collector1 playing a key part in getting data out of the network elements via gNMI subscriptions. While this deployment model is valid and common it is not the only model that can be used.</p> <p>With SR Linux Prometheus Exporter agent we offer SR Linux users another way to consume Streaming Telemetry in a scaled out fashion.</p>  <p> </p> Classic and agent-enabled telemetry stacks  <p>With Prometheus Exporter agent deployed on SR Linux switches the telemetry deployment model changes from a \"single collector - many targets\" to a \"many collectors - single target\" mode. The collection role is now distributed across the network with Prometheus TSDB scraping metrics endpoints exposed by the agents.</p> <p>Adopting this model has some interesting benefits beyond load sharing the collection task across the network fleet:</p> <ol> <li>\"Removing\" gNMI complexity     As gNMI based collection now happens \"inside\" the switch, the monitoring teams do not need to be exposed to gNMI subscription internals or to worry about managing collectors. This streamlines the telemetry scraping workflows, as now the switches practically behave the same way as any other system that provides telemetry metrics.    </li> <li>Easy way to add/remove subscription     Since SR Linux NDK agents provide seamless integration with all the management interfaces, the subscription handling can be done via CLI/gNMI/JSON-RPC. Users will add them the same way they do any configuration on their switches.     Most common subscriptions come pre-baked into the agent, removing the need to do anything for getting basic statistics out of the switches.  </li> <li>Auto discovery of nodes     Agents can register the prometheus endpoints they expose in Consul, which will enable Prometheus server to auto-discover the new nodes as they come This is your self-organizing telemetry fleet.</li> </ol>","title":"Introduction"},{"location":"ndk/apps/srl-prom-exporter/#agents-operations","text":"<p> </p> Agent's core components and interactions map  <p>The high level operations model of the <code>srl-prometheus-exported</code> consists of the following steps:</p> <ol> <li>Agent maps metric names to gNMI XPATHs.</li> <li>A user can disable/enable metrics via any mgmt interface (CLI, gNMI, JSON-RPC)</li> <li>On each scrape request, agent performs a gNMI subscription with mode <code>ONCE</code> for all paths mapped to metrics with state enable (one subscription per metric).</li> <li>The agent will then transform the subscribe responses into prometheus metrics and send them back in the HTTP GET response body.</li> </ol> <p>The following diagram outlines the core components of the agent.</p> <p>Consult with the repository's readme on how to install and configure this agent.</p>   <ol> <li> <p>collectors such as gnmic and others.\u00a0\u21a9</p> </li> </ol>","title":"Agent's operations"},{"location":"ndk/guide/agent-install-and-ops/","text":"","title":"Agent Installation & Operations"},{"location":"ndk/guide/agent-install-and-ops/#installing-the-agent","text":"<p>The onboarding of an NDK agent onto the SR Linux system is simply a task of copying the agent and its files over to the SR Linux filesystem and placing them in the relevant directories.</p> <p>This table summarizes an agent's components and the recommended locations to use.</p>    Component Filesystem location     Executable file <code>/usr/local/bin/</code>   YANG modules <code>/opt/$agentName/yang</code>   Config file <code>/etc/opt/srlinux/appmgr/$agentName.yml</code>   Other files <code>/opt/$agentName/</code>    <p>The agent installation procedure can be carried out in different ways:</p> <ol> <li>manual copy of files via <code>scp</code> or similar tools</li> <li>automated files delivery via configuration management tools (Ansible, etc.)</li> <li>creating an <code>rpm</code> package for the agent and its files and installing the package on SR Linux</li> </ol> <p>The first two options are easy to execute, but they are a bit more involved as the installers need to maintain the remote paths for the copy commands. When using the <code>rpm</code> option, though, it becomes less cumbersome to install the package. All the installers deal with is a single <code>.rpm</code> file and a copy command. Of course, the build process of the <code>rpm</code> package is still required, and we would like to explain this process in detail.</p>","title":"Installing the agent"},{"location":"ndk/guide/agent-install-and-ops/#rpm-package","text":"<p>One of the easiest ways to create an rpm, deb, or apk package is to use the nFPM tool - a simple, 0-dependencies packager.</p> <p>The only thing that nFPM requires of a user is to create a configuration file with the general instructions on how to build a package, and the rest will be taken care of.</p>","title":"RPM package"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-installation","text":"<p>nFPM offers many installation options for all kinds of operating systems and environments. In the course of this guide, we will use the universal nFPM docker image.</p>","title":"nFPM installation"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-configuration-file","text":"<p>nFPM configuration file is the way of letting nFPM know how to build a package for the software artifacts that users created.</p> <p>The complete list of options the <code>nfpm.yml</code> file can have is documented on the project's site. Here we will have a look at the configuration file that is suitable for a typical NDK application written in Go.</p> <p>The file named <code>ndkDemo.yml</code> with the following contents will instruct nFPM how to build a package:</p> <pre><code>name: \"ndkDemo\"       # name of the go package\narch: \"amd64\"         # architecture you are using \nversion: \"v1.0.0\"     # version of this rpm package\nmaintainer: \"John Doe &lt;john@doe.com&gt;\"\ndescription: Sample NDK agent # description of a package\nvendor: \"JD Corp\"     # optional information about the creator of the package\nlicense: \"BSD 2\"\ncontents:                              # contents to add to the package\n  - src: ./ndkDemo                     # local path of agent binary\n    dst: /usr/local/bin/ndkDemo        # destination path of agent binary\n\n  - src: ./yang                        # local path of agent's YANG directory\n    dst: /opt/ndkDemo/yang             # destination path of agent YANG\n\n  - src: ./ndkDemo.yml                 # local path of agent yml\n    dst: /etc/opt/srlinux/appmgr/      # destination path of agent yml\n</code></pre>","title":"nFPM configuration file"},{"location":"ndk/guide/agent-install-and-ops/#running-nfpm","text":"<p>When nFPM configuration and NDK agent files are present, proceed with building an <code>rpm</code> package.</p> <p>Consider the following file layout:</p> <pre><code>.\n\u251c\u2500\u2500 ndkDemo          # agent binary file\n\u251c\u2500\u2500 ndkDemo.yml      # agent config file\n\u251c\u2500\u2500 nfpm.yml         # nFPM config file\n\u2514\u2500\u2500 yang             # directory with agent YANG modules\n    \u2514\u2500\u2500 ndkDemo.yang\n\n1 directory, 4 files\n</code></pre> <p>With these files present we can build an RPM package using the containerized nFPM image like that:</p> <pre><code>docker run --rm -v $PWD:/tmp -w /tmp goreleaser/nfpm package \\\n    --config /tmp/nfpm.yml \\\n    --target /tmp \\\n    --packager rpm\n</code></pre> <p>This command will create <code>ndkDemo-1.0.0.x86_64.rpm</code> file in the current directory that can be copied over to the SR Linux system for installation.</p>","title":"Running nFPM"},{"location":"ndk/guide/agent-install-and-ops/#installing-rpm","text":"<p>Delivering the available rpm package to a fleet of SR Linux boxes can be done with any configuration management tools. For demo purposes, we will utilize the <code>scp</code> utility:</p> <pre><code># this example copies the rpm via scp command to /tmp dir\nscp ndkDemo-1.0.0.x86_64.rpm admin@&lt;srlinux-mgmt-address&gt;:/tmp\n</code></pre> <p>Once the package has been delivered to the SR Linux system, it is ready to be installed. First, we login to SR Linux CLI and drill down to the Linux shell:</p> <pre><code>ssh admin@&lt;srlinux-address&gt;\n\nadmin@clab-srl-srl's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl# bash\n</code></pre> <p>Once in the bash shell, install the package with <code>yum install</code> or <code>rpm</code>:</p> <pre><code>sudo rpm -U /tmp/ndkDemo-1.0.0.x86_64.rpm\n</code></pre>  <p>Tip</p> <p>To check if the package was installed, issue <code>rpm -qa | grep ndkDemo</code></p> <pre><code>admin@srl ~]$ rpm -qa | grep ndkDemo\nndkDemo-1.0.0-1.x86_64\n</code></pre>  <p>During the package installation, the agent related files are copied over to the relevant paths as stated in the nFPM config file:</p> <pre><code># check the executable location\n[admin@srl ~]$ ls -la /usr/local/bin/ | grep ndkDemo\n-rw-r--r-- 1 root root    12312 Nov  4 11:28 ndkDemo\n\n# check YANG modules dir is present\n[admin@srl ~]$ ls -la /opt/ndkDemo/yang/\ntotal 8\ndrwxr-xr-x 2 root root 4096 Nov  4 12:58 .\ndrwxr-xr-x 3 root root 4096 Nov  4 12:53 ..\n-rw-r--r-- 1 root root    0 Nov  4 11:28 ndkDemo.yang\n\n# check ndkDemo config file is present\n[admin@srl ~]$ ls -la /etc/opt/srlinux/appmgr/\ntotal 16\ndrwxr-xr-x+  2 root    root    4096 Nov  4 12:58 .\ndrwxrwxrwx+ 10 srlinux srlinux 4096 Nov  4 12:53 ..\n-rw-r--r--+  1 root    root       0 Nov  4 11:28 ndkDemo.yml\n</code></pre> <p>All the agent components are available by the paths specified in the nFPM configuration file.</p>  <p>Note</p> <p>To update the SR Linux NDK app, the package has to be removed first <pre><code>sudo yum remove ndkDemo-1.0.0 # using yum\nsudo rpm -e ndkDemo-1.0.0     # using rpm\n</code></pre></p>  <p>Congratulations, the agent has been installed successfully.</p>","title":"Installing RPM"},{"location":"ndk/guide/agent-install-and-ops/#loading-the-agent","text":"<p>SR Linux's Application Manager is in charge of managing the applications lifecycle. App Manager controls both the native apps and customer-written agents.</p> <p>After a user installs the agent on the SR Linux system by copying the relevant files, they need to reload the <code>app_mgr</code> process to detect new applications. App Manager gets to know about the available apps by reading the app configuration files located at the following paths:</p>    Directory Description     <code>/opt/srlinux/appmgr/</code> SR Linux embedded applications   <code>/etc/opt/srlinux/appmgr/</code> User-provided applications    <p>To reload the App Manager:</p> <pre><code>/ tools system app-management application app_mgr reload\n</code></pre> <p>Once reloaded, App Manager will detect the new applications and load them according to their configuration. The users will be able to see their app in the list of applications:</p> <pre><code>/show system application &lt;app-name&gt;\n</code></pre>","title":"Loading the agent"},{"location":"ndk/guide/agent-install-and-ops/#managing-the-agents-lifecycle","text":"<p>An application's lifecycle can be managed via any management interface by using the following knobs from the <code>tools</code> schema.</p> <pre><code>/ tools system app-management application &lt;app-name&gt; &lt;start|stop|reload|restart&gt;\n</code></pre> <p>The commands that can be given to an application are translated to system signals as per the following table:</p>    Command Description     <code>start</code> Executes the application   <code>reload</code> Send <code>SIGHUP</code> signal to the app. This signal can be handled by the app and reload its config and change initialization values if necessary   <code>stop</code> Send <code>SIGTERM</code> signal to the app. The app should handle this signal and exit gracefully   <code>quit</code> Send <code>SIGQUIT</code> signal to the app. Default behavior is to terminate the process and dump core info   <code>kill</code> Send <code>SIGKILL</code> signal to the app. Kills the process without any cleanup","title":"Managing the agent's lifecycle"},{"location":"ndk/guide/agent/","text":"<p>As was explained in the NDK Architecture section, an agent1 is a custom software that can extend SR Linux capabilities by running alongside SR Linux native applications and performing some user-defined tasks.</p> <p>To deeply integrate with the rest of the SR Linux architecture, the agents have to be defined like an application that SR Linux's application manager can take control of. The structure of the agents is the main topic of this chapter.</p> <p>The main three components of an agent:</p> <ol> <li>Agent's executable file</li> <li>YANG module</li> <li>Agent configuration file</li> </ol>","title":"Agent Structure"},{"location":"ndk/guide/agent/#executable-file","text":"<p>An executable file is called when the agent starts running on SR Linux system. It contains the application logic and is typically an executable binary or a script.</p> <p>The application logic handles the agents' configuration that may be provided via any management interface (CLI, gNMI, etc.) and contains the core logic of interfacing with gRPC based NDK services.</p> <p>In the subsequent sections of the Developers Guide, we will cover how to write the logic of an agent and interact with various NDK services.</p> <p>An executable file can be placed at <code>/usr/local/bin</code> directory.</p>","title":"Executable file"},{"location":"ndk/guide/agent/#yang-module","text":"<p>SR Linux is a fully modeled Network OS - any native or custom application that can be configured or can have state is required to have a proper YANG model.</p> <p>The \"cost\" associated with requiring users to write YANG models for their apps pays off immensely as this</p> <ul> <li>enables seamless integration of an agent with all management interfaces: CLI, gNMI, JSON-RPC.     Any agent's configuration knobs that users expressed in YANG will be immediately available in the SR Linux CLI as if it was part of it from the beginning. Yes, with auto-suggestion of the fields as well.</li> <li>provides out-of-the-box Streaming Telemetry (gNMI) support for any config or state data that the agent maintains</li> </ul> <p>And secondly, the YANG modules for custom apps are not that hard to write as their data model is typically relatively small.</p>  <p>Note</p> <p>The YANG module is only needed if a developer wants their agent to be configurable via any management interfaces or keep state.</p>  <p>YANG files related to an agent are typically located by the <code>/opt/$agentName/yang</code> path.</p>","title":"YANG module"},{"location":"ndk/guide/agent/#configuration-file","text":"<p>Due to SR Linux modular architecture, each application, be it an internal app like <code>bgp</code> or a custom NDK agent, needs to have a configuration file. This file contains application parameters read by the Application Manager service to onboard the application onto the system.</p> <p>With an agent's config file, users define properties of an application, for example:</p> <ul> <li>application version</li> <li>location of the executable file</li> <li>YANG modules related to this app</li> <li>lifecycle management policy</li> <li>and others</li> </ul> <p>Custom agents must have their config file present by the <code>/etc/opt/srlinux/appmgr</code> directory. It is a good idea to name the agent's config file after the agent's name; if we have the agent called <code>myCoolAgent</code>, then its config file can be named <code>myCoolAgent.yml</code> and stored by the <code>/etc/opt/srlinux/appmgr</code> path.</p> <p>Through the subsequent chapters of the Developers Guide, we will cover the most important options, but here is a complete list of config file parameters:</p>  Complete list of config files parameters <pre><code># Example configuration file for the applications on sr_linux\n# All valid options are shown and explained\n# The name of the application.\n# This must be unique.\napplication-name:\n    # [Mandatory] The source path where the binary can be found\n    path: /usr/local/bin\n    # [Optional, default='./&lt;application-name&gt;'] The command to launch the application.\n    # Note these replacement rules:\n    #   {slot-num} will be replaced by the slot number the process is running on\n    #   {0}, {1}, ... can be replaced by parameters provided in the launch request (launch-by-request: Yes)\n    launch-command: \"VALUE=2 ./binary_name --log-level debug\"\n    # [Optional, default='&lt;launch-command&gt;'] The command to search for when checking if the application is running.\n    # This will be executed as a prefix search, so if the application was launched using './app-name -loglevel debug'\n    # a search-command './app-name' would work.\n    # Note: same replacement rules as launch-command\n    search-command: \"./binary_name\"\n    # [Optional, default=No] Indicates whether the application needs to be launched automatically\n    never-start: No\n    # [Optional, default=No] Indicates whether the application can be restarted automatically when it crashes.\n    # Applies only when never-start is No (if the app is not started by app_mgr it would not be restarted either).\n    # Applications are only restarted when running app_mgr in restart mode (e.g. sr_linux --restart)\n    never-restart: No\n    # [Optional, default=No] Indicates whether the application will be shown in app manager status\n    never-show: No\n    # [Optional, default=No] Indicates whether the launch of the application is delayed\n    # until any configuration is loaded in the application's YANG modules.\n    wait-for-config: No\n    # [Optional] Indicates the application is run as 'user' including 'root'\n    run-as-user: root\n    # [Optional, default=200] Indicates the order in which the application needs to be launched.\n    # The applications with the lowest value are launched first.\n    # Applications with the same value are launched in an undefined order.\n    # By convention, start-order &gt;= 100 require idb.  1 is reserved for device-mgr, which determines chassis type.\n    start-order: 123\n    # [Optional, default=No] Indicates whether this application is launched via an request (idb only at this point).\n    launch-by-request: No\n    # [Optional, default=No] Indicates whether this application is launched in a net namespace (launch-by-request\n    # must be set to Yes).\n    launch-in-net-namespace: No\n    # [Optional, default=3] Indicates the number of restarts within failure-window which will trigger the system restart\n    failure-threshold: 3\n    # [Optional, default=300] Indicates the window in seconds over which to count restarts towards failure-threshold\n    failure-window: 400\n    # [Optional, default=reboot] Indicates the action taken after 'failure-threshold' failures within 'failure-window'\n    failure-action: 'reboot'\n    # [Optional, default=Nokia] Indicates the author of the application\n    author: 'Nokia'\n    # [Optional, default=\u201d\u201d] The command for app_mgr to run to read the application version\n    version-command: 'snmpd --version'\n    # [Optional The operations that may not be manually performed on this application\n    restricted-operations: ['start', 'stop', 'restart', 'quit', 'kill']\n    # [Optional, default No] app-mgr will wait for app to acknowledge it via oob channel\n    oob-init: No\n    # [Optional] The list of launch restrictions - if of all of the restrictions of an element in the list are met,\n    # then the application is launched.  The restrictions are separated by a ':'.  Valid restrictions are:\n    #   'sim' - running in sim mode (like in container env.)\n    #   'hw' - running on real h/w\n    #   'chassis' - running on a chassis (cpm and imm are running on different processors)\n    #   'imm' - runs on the imm\n    #   'cpm' - runs on the cpm (default)\n    launch-restrictions: ['hw:cpm', 'hw:chassis:imm']\n    yang-modules:\n        # [Mandatory] The names of the YANG modules to load. This is usually the file-name without '.yang'\n        names: [module-name, other-module-name]\n        # [Optional] List of enabled YANG features. Each needs to be qualified (e.g. srl_nokia-common:foo)\n        enabled-features: ['module-name:foo', 'other-module-name:bar']\n        # [Optional] The names of the YANG validation plugins to load.\n        validation-plugins: [plugin-name, other-plugin-name]\n        # [Mandatory] All the source-directories where we should search for:\n        #    - The YANG modules listed here\n        #    - any YANG module included/imported in these modules\n        source-directories: [/path/one, /path/two]\n        # [Optional] The names of the not owned YANG modules to load for commit confirmation purposes.\n        not-owned-names: [module-name, other-module-name]\n# [Optional] Multiple applications can be defined in the same YAML file\nother-application-name:\n    command: \"./other-binary\"\n    path: /other/path\n</code></pre>","title":"Configuration file"},{"location":"ndk/guide/agent/#dependency-and-other-files","text":"<p>Quite often, an agent may require additional files for its operation. It can be a virtual environment for your Python agent or some JSON file that your agent consumes.</p> <p>All those auxiliary files can be saved by the <code>/opt/$agentName/</code> directory.</p>   <ol> <li> <p>terms NDK agent and NDK app are used interchangeably\u00a0\u21a9</p> </li> </ol>","title":"Dependency and other files"},{"location":"ndk/guide/architecture/","text":"<p>SR Linux provides a Software Development Kit (SDK) to assist operators with developing agents that run alongside SR Linux applications. This SDK is named NetOps Development Kit, or NDK for short.</p> <p>NDK allows operators to write applications (a.k.a agents) that deeply integrate with other native SR Linux applications. The deep integration is the courtesy of the NDK gRPC service that enables custom applications to interact with other SR Linux applications via Impart Database (IDB).</p> <p>In Fig. 1, custom NDK applications <code>app-1</code> and <code>app-2</code> interact with other SR Linux subsystems via gRPC-based NDK service.</p>   Fig 1. NDK applications integration  <p>In addition to the traditional tasks of reading and writing configuration, NDK-based applications gain low-level access to the SR Linux system. For example, these apps can install FIB routes or listen to LLDP events.</p>","title":"NDK Architecture"},{"location":"ndk/guide/architecture/#grpc-protocol-buffers","text":"<p>NDK uses gRPC - a high-performance, open-source framework for remote procedure calls.</p> <p>gRPC framework by default uses Protocol buffers as its Interface Definition Language as well as the underlying message exchange format.</p>  <p>Info</p> <p>Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p>  <p>In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types.</p> <p>On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client provides the same methods as the server.</p>   Fig 2. gRPC client-server interactions  <p>Leveraging gRPC and protobufs provides some substantial benefits for NDK users:</p> <ol> <li>Language neutrality: NDK apps can be written in any language for which protobuf compiler exists. Go, Python, C, Java, Ruby, and more languages are supported by Protocol buffers enabling SR Linux users to write apps in the language of their choice.</li> <li>High-performance: protobuf-encoded messaging is an efficient way to exchange data in a client-server environment. Applications that consume high-volume streams of data (for example, route updates) benefit from an efficient and fast message delivery enabled by protobuf.</li> <li>Backwards API compatibility: a protobuf design property of using IDs for data fields makes it possible to evolve API over time without ever breaking backward compatibility. Old clients will still be able to consume data stored in the original fields, whereas new clients will benefit from accessing data stored in the new fields.</li> </ol>","title":"gRPC &amp; Protocol buffers"},{"location":"ndk/guide/architecture/#ndk-service","text":"<p>NDK provides a collection of gRPC services, each of which enables custom applications to interact with a particular subsystem on an SR Linux OS, delivering a high level of integration and extensibility.</p> <p>With this architecture, NDK agents act as gRPC clients that execute remote procedure calls (RPC) on a system that implements a gRPC server.</p> <p>On SR Linux, <code>ndk_mgr</code> is the application that runs the NDK gRPC server. Fig 3. shows how custom agents interact via gRPC with NDK, and NDK executes the remote procedure and communicates with other system applications through IDB and pub/sub interface to return the result of the RPC to a client.</p>   Fig 3. gRPC as an Inter Process Communication (IPC) protocol  <p>As a result, custom applications are able to communicate with the native SR Linux apps as if they were shipped with SR Linux OS.</p>","title":"NDK Service"},{"location":"ndk/guide/architecture/#proto-files","text":"<p>NDK services, underlying RPCs, and messages are defined in <code>.proto</code> files. These files are used to generate language bindings essential for the NDK apps development process and serve as the data modeling language for the NDK itself.</p> <p>The source <code>.proto</code> files for NDK are open and published in <code>nokia/srlinux-ndk-protobufs</code> repository. Anyone can clone this repository and explore the NDK gRPC services or build language bindings for the programming language of their choice.</p>","title":"Proto files"},{"location":"ndk/guide/architecture/#documentation","text":"<p>Although the proto files are human-readable, it is easier to browse the NDK services using the generated documentation that we keep in the same <code>nokia/srlinux-ndk-protobufs</code> repo. The HTML document is provided in the readme file that appears when a user selects a tag that matches the NDK release version1.</p> <p>The generated documentation provides the developers with a human-readable reference of all the services, messages, and types that comprise the NDK service.</p>","title":"Documentation"},{"location":"ndk/guide/architecture/#operations-flow","text":"<p>Regardless of the language in which the agents are written, at a high level, the following flow of operations applies to all agents when interacting with the NDK service:</p>   Fig 4. NDK operations flow  <ol> <li>Establish gRPC channel with NDK manager and instantiate an NDK client</li> <li>Register the agent with the NDK manager</li> <li>Register notification streams for different types of NDK services (config, lldp, interface, etc.)</li> <li>Start streaming notifications</li> <li>Handle the streamed notifications</li> <li>Update agent's state data if needed</li> <li>Exit gracefully if required</li> </ol> <p>To better understand the steps each agent undergoes, we will explain them in a language-neutral manner. For language-specific implementations, read the \"Developing with NDK\" chapter.</p>","title":"Operations flow"},{"location":"ndk/guide/architecture/#grpc-channel-and-ndk-manager-client","text":"<p>NDK agents communicate with gRPC based NDK service by invoking RPCs and handling responses. An RPC generally takes in a client request message and returns a response message from the server.</p> <p>A gRPC channel must be established before communicating with the NDK manager application running on SR Linux2. NDK server runs on port <code>50053</code>; agents which are installed on SR Linux OS use <code>localhost:50053</code> socket to establish the gRPC channel.</p> <p>Once the gRPC channel is set up, a gRPC client (often called stub) needs to be created to perform RPCs. Each gRPC service needs to have its own client. In NDK, the <code>SdkMgrService</code> service is the first service that agents interact with, therefore, users first need to create the NDK Manager Client (Mgr Client on diagram) that will be able to call RPCs defined for <code>SdkMgrService</code>.</p>","title":"gRPC Channel and NDK Manager Client"},{"location":"ndk/guide/architecture/#agent-registration","text":"<p>Agent must be first registered with SRLinux NDK by calling <code>AgentRegister</code> RPC of <code>SdkMgrService</code>. Initial agent state is created during the registration process.</p> <p>An <code>AgentRegistrationResponse</code> is returned (omitted in Fig. 4) with the status of the registration process.</p>","title":"Agent registration"},{"location":"ndk/guide/architecture/#registering-notifications","text":"<p>Agents interact with other services like Network Instance, Config, LLDP, BFD by subscribing to notification updates from these services.</p> <p>Before subscribing to a notification stream of a certain service the subscription stream needs to be created. To create it, a client of <code>SdkMgrService</code> calls <code>NotificationRegister</code> RPC with <code>NotificationRegistrationRequest</code> field <code>Op</code> set to <code>Create</code> and other fields absent.</p>  <p>Info</p> <p><code>NotificationRegistrationRequest</code> message's field <code>Op</code> (for Operation) may have one of the following values:</p> <ul> <li><code>Create</code> creates a subscription stream and returns a <code>StreamId</code> that is used when adding subscriptions with the <code>AddSubscription</code> operation.</li> <li><code>Delete</code> deletes the existing subscription stream that has a particular <code>SubId</code>.</li> <li><code>AddSubscription</code> adds a subscription. The stream will now be able to stream notifications of that subscription type (e.g., Intf, NwInst, etc).</li> <li><code>DeleteSubscription</code> deletes the previously added subscription.</li> </ul>  <p>When <code>Op</code> field is set to <code>Create</code>, NDK Manager responds with <code>NotificationRegisterResponse</code> message with <code>stream_id</code> field set to some value. The stream has been created, and the subscriptions can be added to the created stream.</p> <p>To subscribe to a certain service notification updates another call of <code>NotificationRegister</code> RPC is made with the following fields set:</p> <ul> <li><code>stream_id</code> set to an obtained value from the <code>NotificationRegisterResponse</code></li> <li><code>Op</code> is set to <code>AddSubscription</code></li> <li>one of the <code>subscription_types</code> is set according to the desired service notifications. For example, if notifications from the <code>Config</code> service are of interest, then <code>config</code> field of type <code>ConfigSubscriptionRequest</code> is set.</li> </ul> <p><code>NotificationRegisterResponse</code> message follows the request and contains the same <code>stream_id</code> but now also the <code>sub_id</code> field - subscription identifier. At this point agent successfully indicated its desire to receive notifications from certain services, but the notification streams haven't been started yet.</p>","title":"Registering notifications"},{"location":"ndk/guide/architecture/#streaming-notifications","text":"<p>Requesting applications to send notifications is done by interfacing with <code>SdkNotificationService</code>. As this is another gRPC service, it requires its own client - Notification client.</p> <p>To initiate streaming of updates based on the agent subscriptions the Notification Client executes <code>NotificationStream</code> RPC which has <code>NotificationStreamRequest</code> message with <code>stream_id</code> field set to the ID of a stream to be used. This RPC returns a stream of <code>NotificationStreamResponse</code>, which makes this RPC of type \"server streaming RPC\".</p>  Server-streaming RPC <p>A server-streaming RPC is similar to a unary RPC, except that the server returns a stream of messages in response to a client's request. After sending all its messages, the server's status details (status code and optional status message) and optional trailing metadata are sent to the client. This completes processing on the server side. The client completes once it has all the server's messages.</p>  <p><code>NotificationStreamResponse</code> message represents a notification stream response that contains one or more notifications. The <code>Notification</code> message contains one of the <code>subscription_types</code> notifications, which will be set in accordance to what notifications were subscribed by the agent.</p> <p>In our example, we sent <code>ConfigSubscriptionRequest</code> inside the <code>NotificationRegisterRequest</code>, hence the notifications that we will get back for that <code>stream_id</code> will contain <code>ConfigNotification</code> messages inside <code>Notification</code> of a <code>NotificationStreamResponse</code>.</p>","title":"Streaming notifications"},{"location":"ndk/guide/architecture/#handling-notifications","text":"<p>The agent handles the stream of notifications by analyzing which concrete type of notification was read from the stream. The Server streaming RPC will provide notifications till the last available one; the agent then reads out the incoming notifications and handles the messages contained within them.</p> <p>The handling of notifications is done when the last notification is sent by the server. At this point, the agent may perform some work on the received data and, if needed, update the agent's state if it has one.</p>","title":"Handling notifications"},{"location":"ndk/guide/architecture/#updating-agents-state-data","text":"<p>Each agent may keep state and configuration data modeled in YANG. When an agent needs to set/update its own state data (for example, when it made some calculations based on received notifications), it needs to use <code>SdkMgrTelemetryService</code> and a corresponding client.</p>   Fig 5. Updating agent's state flow  <p>The state that an agent intends to have will be available for gNMI telemetry, CLI access, and JSON-RPC retrieval, as it essentially becomes part of the SR Linux state.</p> <p>Updating or initializing agent's state with data is done with <code>TelemetryAddOrUpdate</code> RPC that has a request of type <code>TelemetryUpdateRequest</code> that encloses a list of <code>TelemetryInfo</code> messages. Each <code>TelemetryInfo</code> message contains a <code>key</code> field that points to a subtree of agent's YANG model that needs to be updated with the JSON data contained within <code>data</code> field.</p>","title":"Updating agent's state data"},{"location":"ndk/guide/architecture/#exiting-gracefully","text":"<p>When an agent needs to stop its operation and be removed from the SR Linux system, it needs to be unregistered by invoking <code>AgentUnRegister</code> RPC of the <code>SdkMgrService</code>. The gRPC connection to the NDK server needs to be closed.</p> <p>When unregistered, the agent's state data will be removed from SR Linux system and will no longer be accessible to any of the management interfaces.</p>   <ol> <li> <p>For example, here you will find the auto-generated documentation for the latest NDK version at the moment of this writing.\u00a0\u21a9</p> </li> <li> <p><code>ndk_mgr</code> is the name of the application that implements NDK gRPC server side and runs on SR Linux OS.\u00a0\u21a9</p> </li> </ol>","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/","text":"<p>This guide explains how to consume the NDK service when developers write the agents in a Go1 programming language.</p>  <p>Note</p> <p>This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter.</p> <p>Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations.</p>  <p>In addition to the publicly available protobuf files, which define the NDK Service, Nokia also provides generated Go bindings for data access classes of NDK in a <code>nokia/srlinux-ndk-go</code> repo.</p> <p>The <code>github.com/nokia/srlinux-ndk-go/v21/ndk</code> package provided in that repository enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Go package themselves.</p>","title":"Developing agents with NDK in Go"},{"location":"ndk/guide/dev/go/#establish-grpc-channel-with-ndk-manager-and-instantiate-an-ndk-client","text":"<p> Additional information</p> <p>To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux.</p> <p>This is done by passing the NDK server address - <code>localhost:50053</code> - to <code>grpc.Dial()</code> as follows:</p> <pre><code>import (\n    \"google.golang.org/grpc\"\n)\n\nconn, err := grpc.Dial(\"localhost:50053\", grpc.WithInsecure())\nif err != nil {\n  ...\n}\ndefer conn.Close()\n</code></pre> <p>Once the gRPC channel is setup, we need to instantiate a client (often called stub) to perform RPCs. The client is obtained using the <code>NewSdkMgrServiceClient</code> method provided.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n\nclient := ndk.NewSdkMgrServiceClient(conn)\n</code></pre>","title":"Establish gRPC channel with NDK manager and instantiate an NDK client"},{"location":"ndk/guide/dev/go/#register-the-agent-with-the-ndk-manager","text":"<p> Additional information</p> <p>Agent must be first registered with SR Linux by calling the <code>AgentRegister</code> method available on the returned <code>SdkMgrServiceClient</code> interface. The initial agent state is created during the registration process.</p>","title":"Register the agent with the NDK manager:"},{"location":"ndk/guide/dev/go/#agents-context","text":"<p>Go context is a required parameter for each RPC service method. Contexts provide the means of enforcing deadlines and cancellations as well as transmitting metadata within the request.</p> <p>During registration, SR Linux will be expecting a key-value pair with the <code>agent_name</code> key and a value of the agent's name passed in the context of an RPC. The agent name is defined in the agent's YAML file.</p>  <p>Warning</p> <p>Not including this metadata in the agent <code>ctx</code> would result in an agent registration failure. SR Linux would not be able to differentiate between two agents both connected to the same NDK manager.</p>  <pre><code>ctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\n// appending agent's name to the context metadata\nctx = metadata.AppendToOutgoingContext(ctx, \"agent_name\", \"ndkDemo\")\n</code></pre>","title":"Agent's context"},{"location":"ndk/guide/dev/go/#agent-registration","text":"<p><code>AgentRegister</code> method takes in the context <code>ctx</code> that is by now has agent name as its metadata and an <code>AgentRegistrationRequest</code>.</p> <p><code>AgentRegistrationRequest</code> structure can be passed in with its default values for a basic registration request.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n\nr, err := client.AgentRegister(ctx, &amp;ndk.AgentRegistrationRequest{})\nif err != nil {\n    log.Fatalf(\"agent registration failed: %v\", err)\n}\n</code></pre> <p><code>AgentRegister</code> method returns <code>AgentRegistrationResponse</code> and an error. Response can be additionally checked for status and error description.</p>","title":"Agent registration"},{"location":"ndk/guide/dev/go/#register-notification-streams","text":"<p> Additional information</p>","title":"Register notification streams"},{"location":"ndk/guide/dev/go/#create-subscription-stream","text":"<p>A subscription stream needs to be created first before any of the subscription types can be added. <code>SdkMgrServiceClient</code> first creates the subscription stream by executing <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> only field <code>Op</code> set to a value of <code>const NotificationRegisterRequest_Create</code>. This effectively creates a stream which is identified with a <code>StreamID</code> returned inside the <code>NotificationRegisterResponse</code>.</p> <p><code>StreamId</code> must be associated when subscribing/unsubscribing to certain types of router notifications.</p> <pre><code>req := &amp;ndk.NotificationRegisterRequest{\n    Op: ndk.NotificationRegisterRequest_Create,\n}\n\nresp, err := client.NotificationRegister(ctx, req)\nif err != nil {\n    log.Fatalf(\"Notification Register failed with error: %v\", err)\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\n    r.log.Fatalf(\"Notification Register failed with status %d\", resp.GetStatus())\n}\n\nlog.Debugf(\"Notification Register was successful: StreamID: %d SubscriptionID: %d\", resp.GetStreamId(), resp.GetSubId())\n}\n</code></pre>","title":"Create subscription stream"},{"location":"ndk/guide/dev/go/#add-notification-subscriptions","text":"<p>Once the <code>StreamId</code> is acquired, a client can register notifications of a particular type to be delivered over that stream.</p> <p>Different types of notifications types can be subscribed to by calling the same <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> having <code>Op</code> field set to <code>NotificationRegisterRequest_AddSubscription</code> and certain <code>SubscriptionType</code> selected.</p> <p>In the example below we would like to receive notifications from the <code>Config</code> service, hence we specify <code>NotificationRegisterRequest_Config</code> subscription type.</p> <pre><code>subType := &amp;ndk.NotificationRegisterRequest_Config{ // This is unique to each notification type (Config, Intf, etc.).\n    Config: &amp;ndk.ConfigSubscriptionRequest{},\n}\nreq := &amp;ndk.NotificationRegisterRequest{\n    StreamId:          resp.GetStreamId(), // StreamId is retrieved from the NotificationRegisterResponse\n    Op:                ndk.NotificationRegisterRequest_AddSubscription,\n    SubscriptionTypes: subType,\n}\nresp, err := r.mgrStub.NotificationRegister(r.ctx, req)\nif err != nil {\n    log.Fatalf(\"Agent could not subscribe for config notification\")\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\n    log.Fatalf(\"Agent could not subscribe for config notification with status  %d\", resp.GetStatus())\n}\nlog.Infof(\"Agent was able to subscribe for config notification with status %d\", resp.GetStatus())\n</code></pre>","title":"Add notification subscriptions"},{"location":"ndk/guide/dev/go/#streaming-notifications","text":"<p> Additional information</p> <p>Actual streaming of notifications is a task for another service - <code>SdkNotificationService</code>. This service requires developers to create its own client, which is done with <code>NewSdkNotificationServiceClient</code> function.</p> <p>The returned <code>SdkNotificationServiceClient</code> interface has a single method <code>NotificationStream</code> that is used to start streaming notifications.</p> <p><code>NotificationsStream</code> is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request.</p> <p>To tell the server to start streaming notifications that were subscribed to before the <code>NewSdkNotificationServiceClient</code> executes <code>NotificationsStream</code> method where <code>NotificationStreamRequest</code> struct has its <code>StreamId</code> field set to the value that was obtained at subscription stage.</p> <pre><code>req := &amp;ndk.NotificationStreamRequest{\n    StreamId: resp.GetStreamId(),\n}\nstreamResp, err := notifClient.NotificationStream(ctx, req)\nif err != nil {\n    log.Fatal(\"Agent failed to create stream client with error: \", err)\n}\n</code></pre>","title":"Streaming notifications"},{"location":"ndk/guide/dev/go/#handle-the-streamed-notifications","text":"<p> Additional information</p> <p>Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is the pseudocode that illustrates the flow:</p> <pre><code>func HandleNotifications(stream ndk.SdkNotificationService_NotificationStreamClient) {\n    for { // loop until stream returns io.EoF\n        notification stream response (nsr) := stream.Recv()\n        for notif in nsr.Notification { // nsr.Notification is a slice of `Notification`\n            if notif.GetConfig() is not nil {\n                1. config notif = notif.GetConfig()\n                2. handle config notif\n            } else if notif.GetIntf() is not nil {\n                1. intf notif = notif.GetIntf()\n                2. handle intf notif\n            } ... // Do this if statement for every notification type the agent is subscribed to\n        }\n    }\n}\n</code></pre> <p><code>NotificationStream</code> method of the <code>SdkNotificationServiceClient</code> interface will return a stream client <code>SdkNotificationService_NotificationStreamClient</code>.</p> <p><code>SdkNotificationService_NotificationStreamClient</code> contains a <code>Recv()</code> to retrieve notifications one by one. At the end of a stream <code>Rev()</code> will return <code>io.EOF</code>.</p> <p><code>Recv()</code> returns a <code>*NotificationStreamResponse</code> which contains a slice of <code>Notification</code>.</p> <p><code>Notification</code> struct has <code>GetXXX()</code> methods defined which retrieve the notification of a specific type. For example: <code>GetConfig</code> returns <code>ConfigNotification</code>.</p>  <p>Note</p> <p><code>ConfigNotification</code> is returned only if <code>Notification</code> struct has a certain subscription type set for its <code>SubscriptionType</code> field. Otherwise, <code>GetConfig</code> returns <code>nil</code>.</p>  <p>Once the specific <code>XXXNotification</code> has been extracted using the <code>GetXXX()</code> method, users can access the fields of the notification and process the data contained within the notification using <code>GetKey()</code> and <code>GetData()</code> methods.</p>","title":"Handle the streamed notifications"},{"location":"ndk/guide/dev/go/#exiting-gracefully","text":"<p>Agent needs to handle SIGTERM signal that is sent when a user invokes <code>stop</code> command via SR Linux CLI. The following is the required steps to cleanly stop the agent:</p> <ol> <li>Remove any agent's state if it was set using <code>TelemetryDelete</code> method of a Telemetry client.</li> <li>Delete notification subscriptions stream <code>NotificationRegister</code> method with <code>Op</code> set to <code>NotificationRegisterRequest_Delete</code>.</li> <li>Invoke use <code>AgentUnRegister()</code> method of a <code>SdkMgrServiceClient</code> interface.</li> <li>Close gRPC channel with the <code>sdk_mgr</code>.</li> </ol>","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/#logging","text":"<p>To debug an agent, the developers can analyze the log messages that the agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at <code>/var/log/srlinux/stdout/</code> directory.</p> <p>The default SR Linux debug messages are found in the messages directory <code>/var/log/srlinux/buffer/messages</code>; check them when something went wrong within the SR Linux system (agent registration failed, IDB server warning messages, etc.).</p> <p>logrus is a popular structured logger for Go that can log messages of different levels of importance, but developers are free to choose whatever logging package they see fit.</p>   <ol> <li> <p>Make sure that you have set up the dev environment as explained on this page. Readers are also encouraged to first go through the gRPC basic tutorial to get familiar with the common gRPC workflows when using Go.\u00a0\u21a9</p> </li> </ol>","title":"Logging"},{"location":"ndk/guide/env/go/","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide recommendations for a Go toolchain setup suitable for the development and build of NDK applications.</p>","title":"Go Development Environment"},{"location":"ndk/guide/env/go/#environment-components","text":"<p>The toolchain that can be used to develop and build Go-based NDK apps consists of the following components:</p> <ol> <li>Go programming language - Go compiler, toolchain, and standard library</li> <li>Go NDK bindings - generated data access classes for gRPC based NDK service.</li> <li>Goreleaser - Go-focused build &amp; release pipeline runner. Packages nFPM to produce rpm packages that can be used to install NDK agents.</li> </ol>","title":"Environment components"},{"location":"ndk/guide/env/go/#project-structure","text":"<p>It is recommended to use Go modules when developing applications with Go. Go modules allow for better dependency management and can be placed outside the <code>$GOPATH</code> directory.</p> <p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 .goreleaser.yml          # Goreleaser config file\n\u251c\u2500\u2500 main.go                  # Package main that calls agent logic\n\u251c\u2500\u2500 go.mod                   # Go mod file\n\u251c\u2500\u2500 go.sum                   # Go sum file\n</code></pre>","title":"Project structure"},{"location":"ndk/guide/env/go/#ndk-language-bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Go program the language bindings have to be generated from the source proto files.</p> <p>Nokia not only provides the proto files for the SR Linux NDK service but also NDK Go language bindings.</p> <p>With the provided Go bindings, the NDK can be imported in a Go project like that:</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n</code></pre>","title":"NDK language bindings"},{"location":"ndk/guide/env/python/","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide some recommendations for a Python toolchain setup suitable for the development of NDK applications.</p>","title":"Python Development Environment"},{"location":"ndk/guide/env/python/#environment-components","text":"<p>The toolchain that can be used to develop Python-based NDK apps consists of the following components:</p> <ol> <li>Python programming language - Python interpreter, toolchain, and standard library. Python2 is not supported.</li> <li>Python NDK bindings - generated data access classes for gRPC based NDK service.</li> </ol>","title":"Environment components"},{"location":"ndk/guide/env/python/#project-structure","text":"<p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 main.py                  # Package main that calls agent logic\n\u251c\u2500\u2500 requirements.txt         # Python packages required by the app logic\n</code></pre>","title":"Project structure"},{"location":"ndk/guide/env/python/#ndk-language-bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. The language bindings have to be generated from the source proto files to use gRPC services in a Python program.</p> <p>Nokia provides both the proto files for the SR Linux NDK service and also NDK Python language bindings.</p> <p>With the provided Python bindings, the NDK can be installed with <code>pip</code></p> <pre><code># it is a good practice to use virtual env\nsudo python3 -m venv /opt/myApp/venv\n\n# activate the newly created venv\nsource /opt/myApp/venv/bin/activate\n\n# update pip/setuptools in the venv\npip3 install -U pip setuptools\n\n# install the NDK using a specific version (example given for v21.6.2)\npip3 install https://github.com/nokia/srlinux-ndk-py/archive/v21.6.2.zip\n</code></pre> <p>Once installed, NDK services are imported in a Python project like that:</p> <pre><code>from ndk import appid_service_pb2 # (1)\n</code></pre> <ol> <li>Example given for <code>appid_service_pb2</code> service but every service is imported the same way.</li> </ol>","title":"NDK language bindings"},{"location":"tutorials/about/","text":"<p>Learning by doing is not only the most effective method, but also an extremely fun one.</p> <p>The hands-on tutorials we provide in this section are designed in such a way that anyone can launch them</p> <ul> <li>at absolutely no cost</li> <li>whenever they want it</li> <li>whatever machine they have</li> <li>and run it for as long as required</li> </ul> <p>The tutorials use the opensource containerlab project to deploy the lab environment with all the needed components. This ensures that both the tutorial authors and the readers work on exactly the same environment. No more second guessing why the tutorial's outputs differ from yours!</p>","title":"SR Linux tutorials"},{"location":"tutorials/l2evpn/evpn/","text":"<p>Ethernet Virtual Private Network (EVPN), along with Virtual eXtensible LAN (VXLAN), is a technology that allows Layer 2 and Layer 3 traffic to be tunneled across an IP network.</p> <p>The SR Linux EVPN-VXLAN solution enables Layer 2 Broadcast Domains (BDs) in multi-tenant data centers using EVPN for the control plane and VXLAN as the data plane. It includes the following features:</p> <ul> <li>EVPN for VXLAN tunnels (Layer 2), extending a BD in overlay multi-tenant DCs</li> <li>EVPN for VXLAN tunnels (Layer 3), allowing inter-subnet-forwarding for unicast traffic within the same tenant infrastructure</li> </ul> <p>This tutorial is focused on EVPN for VXLAN tunnels Layer 2.</p>","title":"EVPN configuration"},{"location":"tutorials/l2evpn/evpn/#overview","text":"<p>EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch.</p> <p>VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently.</p> <p>The following figure demonstrates this concept where servers <code>srv1</code> and <code>srv2</code> are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain.</p>  <p>Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable1, we can proceed with the VXLAN based EVPN service configuration.</p> <p>While doing that we will cover the following topics:</p> <ul> <li>VXLAN tunnel interface configuration</li> <li>Network instances of type <code>mac-vrf</code></li> <li>Bridged subinterfaces</li> <li>and BGP EVPN control plane configuration</li> </ul>","title":"Overview"},{"location":"tutorials/l2evpn/evpn/#ibgp-for-evpn","text":"<p>Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. </p> <p>EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches.</p>  <p>For that iBGP configuration we will create a group called <code>iBGP-overlay</code> which will have the <code>peer-as</code> and <code>local-as</code> set to <code>100</code> to form an iBGP neighborship. The group will also host the same permissive <code>all</code> routing policy, enabled <code>evpn</code> and disabled ipv4-unicast address families.</p> <p>Then for each leaf we add a new BGP neighbor addressed by the remote <code>system0</code> interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config:</p> leaf1 <pre><code>enter candidate\n\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n\n    neighbor 10.0.0.2 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.1\n        }\n    }\ncommit now\n</code></pre>  leaf2 <pre><code>enter candidate\n\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n\n    neighbor 10.0.0.1 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.2\n        }\n    }\ncommit now\n</code></pre>   <p>Ensure that the iBGP session is established before proceeding any further:</p> <p><pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [0/0/0]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above.</p>","title":"IBGP for EVPN"},{"location":"tutorials/l2evpn/evpn/#access-interfaces","text":"<p>Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches:</p>  <p>Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be <code>bridged</code>, this makes the interfaces only attachable to a network instance of <code>mac-vrf</code> type with MAC learning and layer-2 forwarding enabled.</p> <p>The following config is applied to both leaf switches:</p> <pre><code>enter candidate\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <p>As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers.</p>","title":"Access interfaces"},{"location":"tutorials/l2evpn/evpn/#tunnelvxlan-interface","text":"<p>After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric.</p> <p>The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name <code>vxlan&lt;N&gt;</code> where <code>N = 0..255</code>.</p> <p>A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI).</p> <ul> <li>The index can be a number in the range 0-4294967295.</li> <li>The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed).</li> <li>The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of <code>1..16777215</code>.   The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes.   SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets.</li> </ul>  <p>Note</p> <p>The source IP used in the vxlan-interfaces is the IPv4 address of subinterface <code>system0.0</code> in the default network-instance.</p>  <p>The above information translates to a configuration snippet which is applicable both to <code>leaf1</code> and <code>leaf2</code> nodes.</p> <pre><code>enter candidate\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\ncommit now\n</code></pre> <p>To verify the tunnel interface configuration: <pre><code>A:leaf2# show tunnel-interface vxlan-interface brief\n---------------------------------------------------------------------------------\nShow report for vxlan-tunnels\n---------------------------------------------------------------------------------\n+------------------+-----------------+---------+-------------+------------------+\n| Tunnel Interface | VxLAN Interface |  Type   | Ingress VNI | Egress source-ip |\n+==================+=================+=========+=============+==================+\n| vxlan1           | vxlan1.1        | bridged | 1           | 10.0.0.2/32      |\n+------------------+-----------------+---------+-------------+------------------+\n---------------------------------------------------------------------------------\nSummary\n  1 tunnel-interfaces, 1 vxlan interfaces\n  0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip\n---------------------------------------------------------------------------------\n</code></pre></p>","title":"Tunnel/VXLAN interface"},{"location":"tutorials/l2evpn/evpn/#mac-vrf","text":"<p>Now it is a turn of MAC-VRF to get configured.</p> <p>The network-instance type <code>mac-vrf</code> functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration.</p> <p>By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance:</p> <pre><code>enter candidate\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n    }\ncommit now\n</code></pre>","title":"MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#server-interfaces","text":"<p>The servers in our fabric do not have any addresses on their <code>eth1</code> interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration.</p> <p>By the end of this section, we will have the following addressing scheme complete:</p>  <p>To connect to a shell of a server execute <code>docker exec -it &lt;container-name&gt; bash</code>:</p> srv1 <pre><code>docker exec -it clab-evpn01-srv1 bash\n</code></pre>  srv2 <pre><code>docker exec -it clab-evpn01-srv2 bash\n</code></pre>   <p>Within the shell, configure MAC address2 and IPv4 address for the <code>eth1</code> interface according to the diagram above, as with this interface the server is connected to the leaf switch.</p> srv1 <pre><code>ip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre>  srv2 <pre><code>ip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre>   <p>Let's try to ping server2 from server1:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2028ms\n</code></pre> <p>That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the <code>srv1</code> interface MAC to get learned by the <code>leaf1</code> mac-vrf network instance:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n----------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n----------------------------------------------------------------------------------------------------------------------\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\n|      Address      |       Destination        |   Dest    |  Type  | Active | Aging |       Last Update        |\n|                   |                          |   Index   |        |        |       |                          |\n+===================+==========================+===========+========+========+=======+==========================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0           | 4         | learnt | true   | 242   | 2021-07-13T17:36:23.000Z |\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    1 Total    1 Active\n----------------------------------------------------------------------------------------------------------------------\n</code></pre>","title":"Server interfaces"},{"location":"tutorials/l2evpn/evpn/#evpn-in-mac-vrf","text":"<p>To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our <code>vrf-1</code> network-instance.</p> <p>EVPN configuration under the mac-vrf network instance will require two configuration containers:</p> <ul> <li><code>bgp-vpn</code> - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist.</li> <li><code>bgp-evpn</code> - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to <code>bgp-instance 1</code> is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI).</li> </ul> <p>The following configuration is entered on both leafs:</p> <pre><code>enter candidate\n    /network-instance vrf-1\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\ncommit now\n</code></pre> <p>Once configured, the <code>bgp-vpn</code> instance can be checked to have the RT/RD values set:</p> <pre><code>A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1\n=====================================================================\nNet Instance   : vrf-1\n    bgp Instance 1\n---------------------------------------------------------------------\n        route-distinguisher: 10.0.0.1:111, auto-derived-from-evi\n        export-route-target: target:100:111, manual\n        import-route-target: target:100:111, manual\n=====================================================================\n</code></pre>  <p>VNI to EVI mapping</p> <p>As of release 21.6, SR Linux uses only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI.5</p>","title":"EVPN in MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#final-configurations","text":"<p>For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration6 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks.</p>  pastable snippets leaf1 <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 101\n                router-id 10.0.0.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.2 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.1\n                    }\n                }\n                neighbor 192.168.11.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    /interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.1/30 {\n                }\n            }\n        }\n    }\n    /interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>  leaf2 <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 102\n                router-id 10.0.0.2\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.1 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.2\n                    }\n                }\n                neighbor 192.168.12.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.1/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.2/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>  spine1 <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n\n    /network-instance default {\n        interface ethernet-1/1.0 {\n        }\n        interface ethernet-1/2.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 201\n                router-id 10.0.1.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                }\n                ipv4-unicast {\n                    admin-state enable\n                }\n                neighbor 192.168.11.1 {\n                    peer-as 101\n                    peer-group eBGP-underlay\n                }\n                neighbor 192.168.12.1 {\n                    peer-as 102\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.2/30 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.2/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.1.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>  srv1 <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv1 bash\n\nip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre></p>  srv2 <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv2 bash\n\nip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre></p>","title":"Final configurations"},{"location":"tutorials/l2evpn/evpn/#verification","text":"","title":"Verification"},{"location":"tutorials/l2evpn/evpn/#evpn-imet-routes","text":"<p>When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands:</p> <pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [1/1/1]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> <p>The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3).</p> <p>The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose:</p> <ul> <li>Auto-discovery of the remote VTEPs attached to the same EVI</li> <li>Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated</li> </ul> <p>The IMET/RT3 routes can be viewed in summary and detailed modes:</p> RT3 summary <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary\n----------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------\nType 3 Inclusive Multicast Ethernet Tag Routes\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n| Status | Route-distinguisher |   Tag-ID   |    Originator-IP    |      neighbor       |      Next-Hop       |\n+========+=====================+============+=====================+=====================+=====================+\n| u*&gt;    | 10.0.0.2:111        | 0          | 10.0.0.2            | 10.0.0.2            | 10.0.0.2            |\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n----------------------------------------------------------------------------------------------------------------\n1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid\n----------------------------------------------------------------------------------------------------------------\n</code></pre>  RT3 detailed <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail\n-------------------------------------------------------------------------------------\nShow report for the EVPN routes in network-instance  \"default\"\n-------------------------------------------------------------------------------------\nRoute Distinguisher: 10.0.0.2:111\nTag-ID             : 0\nOriginating router : 10.0.0.2\nneighbor           : 10.0.0.2\nReceived paths     : 1\nPath 1: &lt;Best,Valid,Used,&gt;\n    VNI             : 1\n    Route source    : neighbor 10.0.0.2 (last modified 2m3s ago)\n    Route preference: No MED, LocalPref is 100\n    Atomic Aggr     : false\n    BGP next-hop    : 10.0.0.2\n    AS Path         :  i\n    Communities     : [target:100:111, bgp-tunnel-encap:VXLAN]\n    RR Attributes   : No Originator-ID, Cluster-List is []\n    Aggregation     : None\n    Unknown Attr    : None\n    Invalid Reason  : None\n    Tie Break Reason: none\n--------------------------------------------------------------------------------------\n</code></pre>    Lets capture those routes? <p>Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers.</p> <p>By capturing on the <code>e1-49</code> interface of the <code>clab-evpn01-leaf1</code> container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the <code>vrf-1</code> network instance to trigger the BGP updates to flow and see them in the live capture.</p> <p>Here is the pcap file with the IMET routes advertisements between <code>leaf1</code> and <code>leaf2</code>.</p>  <p>When the IMET routes from <code>leaf2</code> are imported for <code>vrf-1</code> network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 multicast destinations (flooding-list)\n-------------------------------------------------------------------------------\n+--------------+------------+-------------------+----------------------+\n| VTEP Address | Egress VNI | Destination-index | Multicast-forwarding |\n+==============+============+===================+======================+\n| 10.0.0.2     | 1          | 160078821962      | BUM                  |\n+--------------+------------+-------------------+----------------------+\n-------------------------------------------------------------------------------\nSummary\n  1 multicast-destinations\n-------------------------------------------------------------------------------\n</code></pre> <p>This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table.</p> <p>As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes.</p>","title":"EVPN IMET routes"},{"location":"tutorials/l2evpn/evpn/#vxlan-tunnels","text":"<p>After receiving EVPN routes from the remote leafs with VXLAN encapsulation4, SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the <code>leaf1</code> switch.</p> <pre><code>A:leaf1# /show tunnel vxlan-tunnel all\n----------------------------------------------------------\nShow report for vxlan-tunnels\n----------------------------------------------------------\n+--------------+--------------+--------------------------+\n| VTEP Address |    Index     |       Last Change        |\n+==============+==============+==========================+\n| 10.0.0.2     | 160078821947 | 2021-07-13T21:13:50.000Z |\n+--------------+--------------+--------------------------+\n1 VXLAN tunnels, 1 active, 0 inactive\n----------------------------------------------------------\n</code></pre> <p>The VXLAN tunnel is built between the <code>vxlan</code> interfaces in the MAC-VRF network instances, which internally use <code>system</code> interfaces of the <code>default</code> network instance as a VTEP:</p>  <p>Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index3, an entry in the tunnel-table is also created for the tunnel.</p> <pre><code>A:leaf1# /show network-instance default tunnel-table all\n-------------------------------------------------------------------------------------------------------\nShow report for network instance \"default\" tunnel table\n-------------------------------------------------------------------------------------------------------\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n| IPv4 Prefix |   Owner   | Type  | Index | Metric | Preference | Fib-prog |       Last Update        |\n+=============+===========+=======+=======+========+============+==========+==========================+\n| 10.0.0.2/32 | vxlan_mgr | vxlan | 1     | 0      | 0          | Y        | 2021-07-13T21:13:43.424Z |\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n-------------------------------------------------------------------------------------------------------\n1 VXLAN tunnels, 1 active, 0 inactive\n</code></pre>","title":"VXLAN tunnels"},{"location":"tutorials/l2evpn/evpn/#evpn-macip-routes","text":"<p>As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n-------------------------------------------------------------------------------\nDestinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nEthernet Segment Destinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nSummary\n  0 unicast-destinations, 0 non-es, 0 es\n  0 MAC addresses, 0 active, 0 non-active\n</code></pre> <p>This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the <code>vrf-1</code>, we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs7. <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n-------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n-------------------------------------------------------------------------------\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    0 Total    0 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    0 Total    0 Active\n-------------------------------------------------------------------------------\n</code></pre></p> <p>Let's try that ping from <code>srv1</code> towards <code>srv2</code> once again and see what happens:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms\n64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2013ms\nrtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms\n</code></pre> <p>Much better! The dataplane works and we can check that the MAC table in the <code>vrf-1</code> network-instance has been populated with local and EVPN-learned MACs:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n---------------------------------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n---------------------------------------------------------------------------------------------------------------------------------------------\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\n|      Address      |            Destination             |   Dest    |   Type    | Active | Aging |            Last Update             |\n|                   |                                    |   Index   |           |        |       |                                    |\n+===================+====================================+===========+===========+========+=======+====================================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0                     | 4         | learnt    | true   | 240   | 2021-07-18T14:22:55.000Z           |\n| 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1           | 160078821 | evpn      | true   | N/A   | 2021-07-18T14:22:56.000Z           |\n|                   | vtep:10.0.0.2 vni:1                | 962       |           |        |       |                                    |\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    1 Total    1 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    2 Total    2 Active\n---------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>When traffic is exchanged between <code>srv1</code> and <code>srv2</code>, the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2). The MAC/IP routes are imported, and the MACs programmed in the mac-table.</p> <p>The below output shows the MAC/IP EVPN route that <code>leaf1</code> received from its neighbor. The NLRI information contains the MAC of the <code>srv2</code>:</p> <pre><code>A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType 2 MAC-IP Advertisement Routes\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n| Statu |     Route-     |  Tag-ID   |   MAC-address    |   IP-address   |    neighbor    |    Next-Hop    |      VNI       |              ESI              |  MAC Mobility  |\n|   s   | distinguisher  |           |                  |                |                |                |                |                               |                |\n+=======+================+===========+==================+================+================+================+================+===============================+================+\n| u*&gt;   | 10.0.0.2:111   | 0         | 00:C1:AB:00:00:0 | 0.0.0.0        | 10.0.0.2       | 10.0.0.2       | 1              | 00:00:00:00:00:00:00:00:00:00 | -              |\n|       |                |           | 2                |                |                |                |                |                               |                |\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n1 MAC-IP Advertisement routes 1 used, 1 valid\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n---------------------------------------------------------------------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\nDestinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n+--------------+------------+-------------------+-----------------------------+\n| VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) |\n+==============+============+===================+=============================+\n| 10.0.0.2     | 1          | 160078821962      | 1(1/0)                      |\n+--------------+------------+-------------------+-----------------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------\nEthernet Segment Destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------\nSummary\n  1 unicast-destinations, 1 non-es, 0 es\n  1 MAC addresses, 1 active, 0 non-active\n-------------------------------------------------------------------------------\n</code></pre>  <p>packet capture</p> <p>The following pcap was captured a moment before <code>srv1</code> started to ping <code>srv2</code> on <code>leaf1</code> interface <code>e1-49</code>.</p> <p>It shows how:</p> <ol> <li>ARP frames were first exchanged using the multicast destination, </li> <li>next the first ICMP request was sent out by <code>leaf1</code> again using the BUM destination, since RT2 routes were not received yet </li> <li>and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane.</li> <li>after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes.</li> </ol>  <p>This concludes the verification steps, as we have a working data plane connectivity between the servers.</p>   <ol> <li> <p>as was verified before \u21a9</p> </li> <li> <p>containerlab assigns mac addresses to the interfaces with OUI <code>00:C1:AB</code>. We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables.\u00a0\u21a9</p> </li> <li> <p>If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created.\u00a0\u21a9</p> </li> <li> <p>IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference.\u00a0\u21a9</p> </li> <li> <p>Per section 5.1.2 of RFC 8365 \u21a9</p> </li> <li> <p>Easily extracted with doing <code>info &lt;container&gt;</code> where <code>container</code> is <code>routing-policy</code>, <code>network-instance *</code>, <code>interface *</code>, <code>tunnel-interface *</code> \u21a9</p> </li> <li> <p>We did try to ping from <code>srv1</code> to <code>srv2</code> in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again.\u00a0\u21a9</p> </li> </ol>","title":"EVPN MAC/IP routes"},{"location":"tutorials/l2evpn/fabric/","text":"<p>Prior to configuring EVPN based overlay, a routing protocol needs to be deployed in the fabric to advertise the reachability of all the leaf VXLAN Termination End Point (VTEP) addresses throughout the IP fabric.\u000b</p> <p>With SR Linux, the following routing protocols can be used in the underlay:</p> <ul> <li>ISIS</li> <li>OSPF</li> <li>EBGP</li> </ul> <p>We will use a BGP based fabric design as described in RFC7938 due to its simplicity, scalability, and ease of multi-vendor interoperability.</p>","title":"Fabric configuration"},{"location":"tutorials/l2evpn/fabric/#leaf-spine-interfaces","text":"<p>Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the <code>192.168.xx.0/30</code> network to address the links, we will implement the following underlay addressing design:</p>  <p>On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity.</p> <p>We begin with connecting to the CLI of our nodes via SSH1:</p> <pre><code># connecting to leaf1\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration.</p> <p>Let's witness the step by step process of an interface configuration on a <code>leaf1</code> switch with providing the paste-ables snippets for the rest of the nodes</p> <ol> <li>Enter the <code>candidate</code> configuration mode to make edits to the configuration     <pre><code>Welcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n\n\n--{ running }--[  ]--\nA:leaf1# enter candidate\n</code></pre></li> <li>The prompt will indicate the changed active mode     <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1#                              \n</code></pre></li> <li>Enter into the interface configuration context     <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1# interface ethernet-1/49      \n</code></pre></li> <li>Create a subinterface under the parent interface to configure IPv4 address on it     <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 ]--\nA:leaf1# subinterface 0                                        \n--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]--\nA:leaf1# ipv4 address 192.168.11.1/30                                         \n</code></pre></li> <li>Apply the configuration changes by issuing a <code>commit now</code> command. The changes will be written to the running configuration.     <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]--\nA:leaf1# commit now                                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre></li> </ol> <p>Below you will find the relevant configuration snippets2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode.</p> leaf1 <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.1/30 {\n            }\n        }\n    }\n}\n</code></pre>  leaf2 <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.1/30 {\n            }\n        }\n    }\n}\n</code></pre>  spine1 <pre><code>interface ethernet-1/1 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.2/30 {\n            }\n        }\n    }\n}\ninterface ethernet-1/2 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.2/30 {\n            }\n        }\n    }\n}\n</code></pre>   <p>Once those snippets are committed to the running configuration with <code>commit now</code> command, we can ensure that the changes have been applied by showing the interface status:</p> <pre><code>--{ + running }--[  ]--                             \nA:spine1# show interface ethernet-1/1               \n====================================================\nethernet-1/1 is up, speed 10G, type None\n  ethernet-1/1.0 is up\n    Network-instance: \n    Encapsulation   : null\n    Type            : routed\n    IPv4 addr    : 192.168.11.2/30 (static, None)\n----------------------------------------------------\n====================================================\n</code></pre> <p>At this moment, the configured interfaces can not be used as they are not yet associated with any network instance. Below we are placing the interfaces to the network-instance <code>default</code> that is created automatically by SR Linux.</p> leaf1 &amp; leaf2 <pre><code>--{ + candidate shared default }--[  ]--                                                   \nA:leaf1# network-instance default interface ethernet-1/49.0                                \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]--\nA:leaf1# commit now                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre>  spine1 <pre><code>--{ + candidate shared default }--[  ]--                                                  \nA:spine1# network-instance default interface ethernet-1/1.0                               \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]--\nA:spine1# /network-instance default interface ethernet-1/2.0                              \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]--\nA:spine2# commit now                                                                      \nAll changes have been committed. Leaving candidate mode.\n</code></pre>   <p>When interfaces are owned by the network-instance <code>default</code>, we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from <code>spine1</code> to <code>leaf2</code>:</p> <pre><code>--{ + running }--[  ]--                                     \nA:spine1# ping 192.168.12.1 network-instance default        \nUsing network instance default\nPING 192.168.12.1 (192.168.12.1) 56(84) bytes of data.\n64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms\n64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms\n64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms\n64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms\n^C\n--- 192.168.12.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\nrtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms\n</code></pre>","title":"Leaf-Spine interfaces"},{"location":"tutorials/l2evpn/fabric/#ebgp","text":"<p>Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol.</p> <p>The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering.</p> <p>Let's turn this diagram with the ASN/Router ID allocation into a working configuration:</p>  <p>Here is a breakdown of the steps that are needed to configure EBGP on <code>leaf1</code> towards <code>spine1</code>:</p> <ol> <li> <p>Add BGP protocol to network-instance     Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. <pre><code>--{ + candidate shared default }--[  ]--       \nA:leaf1# network-instance default protocols bgp\n</code></pre></p> </li> <li> <p>Assign Autonomous System Number     The ASN is reported to peers when BGP speaker opens a session towards another router.     According to the diagram above, <code>leaf1</code> has ASN 101. <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# autonomous-system 101\n</code></pre></p> </li> <li> <p>Assign Router ID     This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router.     Leaf1 has a router-id of 10.0.0.1.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# router-id 10.0.0.1\n</code></pre></p> </li> <li> <p>Enable AF     Enable all address families that should be enabled globally as a default for all peers of the BGP instance.     When you later configure individual neighbors or groups, you can override the enabled families at those levels.     For the sake of IPv4 loopbacks advertisement, we only need to enable <code>ipv4-unicast</code> address family:     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# ipv4-unicast admin-state enable\n</code></pre></p> </li> <li> <p>Create export/import policies     The export/import policy is required for an EBGP peer to advertise and install routes.     The policy named <code>all</code> that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received4.  </p> <p>The routing policies are configured at <code>/routing-policy</code> context, so first, we switch to it from the current <code>bgp</code> context: <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# /routing-policy                                                      \n\n--{ * candidate shared default }--[ routing-policy ]--                        \nA:leaf1#\n</code></pre> Now that we are in the right context, we can paste the policy definition: <pre><code>--{ +* candidate shared default }--[ routing-policy ]--\nA:leaf1# info\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n</code></pre></p> </li> <li> <p>Create peer-group config     A peer group should include sessions that have a similar or almost identical configuration.     In this example, the peer group is named <code>eBGP-underlay</code> since it will be used to enable underlay routing between the leafs and spines.     New groups are administratively enabled by default.</p> <p>First, we come back to the bgp context from the routing-policy context: <pre><code>--{ * candidate shared default }--[ routing-policy ]--\nA:leaf1# /network-instance default protocols bgp      \n\n--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1#\n</code></pre> Now create the peer group. The common group configuration includes the <code>peer-as</code> and <code>export-policy</code> statements. <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# group eBGP-underlay\n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# peer-as 201                                                                               \n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# export-policy all\n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# import-policy all\n</code></pre></p> </li> <li> <p>Configure neighbor     Configure the BGP session with <code>spine1</code>. In this example, <code>spine1</code> is reachable through the <code>ethernet-1/49.0</code> subinterface. On this subnet, <code>spine1</code> has the IPv4 address <code>192.168.11.2</code>.     In this minimal configuration example, the only required configuration for the neighbor is its association with the group <code>eBGP-underlay</code> that was previously created.     New neighbors are administratively enabled by default.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay\n</code></pre></p> </li> <li> <p>Commit configuration     It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far.     Regardless of which context you are currently in, you can see the diff against the baseline config by doing <code>diff /</code> <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# diff /                                                                                   \n    network-instance default {\n        protocols {\n+             bgp {\n+                 autonomous-system 101\n+                 router-id 10.0.0.1\n+                 group eBGP-underlay {\n+                     export-policy all\n+                     import-policy all\n+                     peer-as 201\n+                 }\n+                 ipv4-unicast {\n+                     admin-state enable\n+                 }\n+                 neighbor 192.168.11.2 {\n+                     peer-group eBGP-underlay\n+                 }\n+             }\n        }\n    }\n+     routing-policy {\n+         policy all {\n+             default-action {\n+                 accept {\n+                 }\n+             }\n+         }\n+     }\n</code></pre>     That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# commit now\n</code></pre></p> </li> </ol> <p>EBGP configuration on <code>leaf2</code> and <code>spine1</code> is almost a twin of the one we did for <code>leaf1</code>. Here is a copy-paste-able3 config snippets for all of the nodes:</p> leaf1 <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 101\n            router-id 10.0.0.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre>  leaf2 <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 102\n            router-id 10.0.0.2\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.12.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre>  spine1 <p>Spine configuration is a bit different, in a way that <code>peer-as</code> is specified under the neighbor context, and not the group one. <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 201\n            router-id 10.0.1.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.1 {\n                peer-group eBGP-underlay\n                peer-as 101\n            }\n            neighbor 192.168.12.1 {\n                peer-group eBGP-underlay\n                peer-as 102\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre></p>","title":"EBGP"},{"location":"tutorials/l2evpn/fabric/#loopbacks","text":"<p>As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces.</p> <p>In the context of the VXLAN data plane, a special kind of a loopback needs to be created - <code>system0</code> interface.</p>  <p>Info</p> <p>The <code>system0.0</code> interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes.</p>  <p>Configuration of the <code>system0</code> interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to <code>system0</code> interfaces will match the Router-ID of a given BGP speaker.</p> leaf1 <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>  leaf2 <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.2/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>  spine1 <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.1.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>","title":"Loopbacks"},{"location":"tutorials/l2evpn/fabric/#verification","text":"<p>As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The <code>system0</code> interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that.</p>","title":"Verification"},{"location":"tutorials/l2evpn/fabric/#bgp-status","text":"<p>The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on <code>leaf1</code>:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n-------------------------------------------------------------\nDefault preferences\n  BGP Local Preference attribute: 100\n  EBGP route-table preference   : 170\n  IBGP route-table preference   : 170\n-------------------------------------------------------------\nWait for FIB install to advertise: True\nSend rapid withdrawals           : disabled\n-------------------------------------------------------------\nIpv4-unicast AFI/SAFI\n    Received routes               : 3\n    Received and active routes    : None\n    Max number of multipaths      : 1, 1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nIpv6-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : 1,1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nEVPN-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : N/A\n    Multipath can transit multi AS: N/A\n-------------------------------------------------------------\n</code></pre>","title":"BGP status"},{"location":"tutorials/l2evpn/fabric/#bgp-neighbor-status","text":"<p>Equally important is the neighbor summary status that we can observe with the following:</p> <pre><code>--{ + running }--[  ]--\nA:spine1# show network-instance default protocols bgp neighbor\n----------------------------------------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------------------------------------\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n|    Net-Inst    |         Peer          |     Group      | Flag | Peer-AS |    State    |   Uptime    | AFI/SAFI  |    [Rx/Active/Tx]     |\n|                |                       |                |  s   |         |             |             |           |                       |\n+================+=======================+================+======+=========+=============+=============+===========+=======================+\n| default        | 192.168.11.1          | eBGP-underlay  | S    | 101     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 49s         | ast       |                       |\n| default        | 192.168.12.1          | eBGP-underlay  | S    | 102     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 9s          | ast       |                       |\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------\nSummary:\n2 configured neighbors, 2 configured sessions are established,0 disabled peers\n0 dynamic peers\n</code></pre> <p>With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state.</p>","title":"BGP neighbor status"},{"location":"tutorials/l2evpn/fabric/#receivedadvertised-routes","text":"<p>The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - <code>system0</code> interfaces. In the below output we verify that <code>leaf1</code> advertises the prefix of <code>system0</code> (<code>10.0.0.1/32</code>) interface towards its EBGP <code>spine1</code> peer: <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou\ntes ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.11.2, remote AS: 201, local AS: 101\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-------------------------------------------------------------------------------------+\n|    Network        Next Hop       MED     LocPref           AsPath           Origin  |\n+=====================================================================================+\n| 10.0.0.1/32      192.168.11.      -        100     [101]                       i    |\n|                  1                                                                  |\n| 192.168.11.0/3   192.168.11.      -        100     [101]                       i    |\n| 0                1                                                                  |\n+-------------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n2 advertised BGP routes\n-----------------------------------------------------------------------------------------\n</code></pre></p> <p>On the far end of the fabric, <code>leaf2</code> receives both the <code>leaf1</code> and <code>spine1</code> system interface prefixes:</p> <pre><code>--{ + running }--[  ]--\nA:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route\ns ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.12.2, remote AS: 201, local AS: 102\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-----------------------------------------------------------------------------------+\n|  Status      Network    Next Hop       MED       LocPref     AsPath      Origin   |\n+===================================================================================+\n|    u*&gt;      10.0.0.1/   192.168.1       -          100      [201,           i     |\n|             32          2.2                                 101]                  |\n|    u*&gt;      10.0.1.1/   192.168.1       -          100      [201]           i     |\n|             32          2.2                                                       |\n|    u*&gt;      192.168.1   192.168.1       -          100      [201]           i     |\n|             1.0/30      2.2                                                       |\n|     *       192.168.1   192.168.1       -          100      [201]           i     |\n|             2.0/30      2.2                                                       |\n+-----------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n4 received BGP routes : 3 used 4 valid\n-----------------------------------------------------------------------------------------\n</code></pre>","title":"Received/Advertised routes"},{"location":"tutorials/l2evpn/fabric/#route-table","text":"<p>The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the <code>default</code> network-instance where we expect them to be:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast summary\n-----------------------------------------------------------------------------------------------------------------------------------\nIPv4 Unicast route table of network instance default\n-----------------------------------------------------------------------------------------------------------------------------------\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n|     Prefix      |  ID   | Route Type |     Route Owner      |      Best/Fib-       |  Metric  |  Pref   | Next-hop  | Next-hop  |\n|                 |       |            |                      |     status(slot)     |          |         |  (Type)   | Interface |\n+=================+=======+============+======================+======================+==========+=========+===========+===========+\n| 10.0.0.1/32     | 3     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 10.0.0.2/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 10.0.1.1/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 192.168.11.0/30 | 1     | local      | net_inst_mgr         | True/success         | 0        | 0       | 192.168.1 | ethernet- |\n|                 |       |            |                      |                      |          |         | 1.1       | 1/49.0    |\n|                 |       |            |                      |                      |          |         | (direct)  |           |\n| 192.168.11.1/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 192.168.11.3/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None (bro | None      |\n|                 |       |            |                      |                      |          |         | adcast)   |           |\n| 192.168.12.0/30 | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n-----------------------------------------------------------------------------------------------------------------------------------\n7 IPv4 routes total\n7 IPv4 prefixes with active routes\n0 IPv4 prefixes with active ECMP routes\n-----------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Both <code>leaf2</code> and <code>spine1</code> prefixes are found in the route table of network-instance <code>default</code> and the <code>bgp_mgr</code> is the owner of those prefixes, which means that they have been added to the route-table by the BGP app.</p>","title":"Route table"},{"location":"tutorials/l2evpn/fabric/#dataplane","text":"<p>To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay.</p> <p>For that we will use the <code>ping</code> command with src/dst set to loopback addresses:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2\nUsing network instance default\nPING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data.\n64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms\n64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms\n</code></pre> <p>Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration!</p>","title":"Dataplane"},{"location":"tutorials/l2evpn/fabric/#resulting-configs","text":"<p>Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with <code>info flat</code> command.</p>  <p>Note</p> <p><code>enter candidate</code> and <code>commit now</code> commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config.</p>  leaf1 <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.1/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 101\nset / network-instance default protocols bgp router-id 10.0.0.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.2\nset / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay\n\ncommit now\n</code></pre>  leaf2 <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.2/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 102\nset / network-instance default protocols bgp router-id 10.0.0.2\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.12.2\nset / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay\n\ncommit now\n</code></pre>  spine1 <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/1\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30\nset / interface ethernet-1/2\nset / interface ethernet-1/2 subinterface 0\nset / interface ethernet-1/2 subinterface 0 ipv4\nset / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.1.1/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface ethernet-1/2.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 201\nset / network-instance default protocols bgp router-id 10.0.1.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.1\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay\nset / network-instance default protocols bgp neighbor 192.168.12.1\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay\n\ncommit now\n</code></pre>     <ol> <li> <p>default SR Linux credentials are <code>admin:admin</code>.\u00a0\u21a9</p> </li> <li> <p>the snippets were extracted with <code>info interface ethernet-1/x</code> command issued in running mode.\u00a0\u21a9</p> </li> <li> <p>you can paste those snippets right after you do <code>enter candidate</code> \u21a9</p> </li> <li> <p>a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines.\u00a0\u21a9</p> </li> </ol>","title":"Resulting configs"},{"location":"tutorials/l2evpn/intro/","text":"Tutorial name L2 EVPN-VXLAN with SR Linux   Lab components 3 SR Linux nodes   Resource requirements  2vCPU  4 GB   Containerlab topology file evpn01.clab.yml   Lab name evpn01   Packet captures EVPN IMET routes exchange, RT2 routes exchange with ICMP in datapath   Main ref documents RFC 7432 - BGP MPLS-Based Ethernet VPNRFC 8365 - A Network Virtualization Overlay Solution Using Ethernet VPN (EVPN)Nokia 7220 SR Linux Advanced Solutions GuideNokia 7220 SR Linux EVPN-VXLAN Guide   Version information1 <code>containerlab:0.15.4</code>, <code>srlinux:21.6.1-250</code>, <code>docker-ce:20.10.2</code>    <p>Ethernet Virtual Private Network (EVPN) is a standard technology in multi-tenant Data Centers (DCs) and provides a control plane framework for many functions. In this tutorial we will configure a VXLAN based Layer 2 EVPN service3 in a tiny CLOS fabric and at the same get to know SR Linux better!</p> <p>The DC fabric that we will build for this tutorial consists of the two leaf switches (acting as Top-Of-Rack) and a single spine:</p>  <p>The two servers are connected to the leafs via an L2 interface. Service-wise the servers will appear to be on the same L2 network by means of the deployed EVPN Layer 2 service.</p>  <p>The tutorial will consist of the following major parts:</p> <ul> <li>Fabric configuration - here we will configure the routing protocol in the underlay of a fabric to advertise the Virtual Tunnel Endpoints (VTEP) of the leaf switches.</li> <li>EVPN configuration - this chapter is dedicated to the EVPN service configuration and validation.</li> </ul>","title":"Introduction"},{"location":"tutorials/l2evpn/intro/#lab-deployment","text":"<p>To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM:</p> <p>The containerlab file that describes the lab topology is referenced below in full:</p> <pre><code>name: evpn01\n\ntopology:\n  kinds:\n    srl:\n      image: ghcr.io/nokia/srlinux\n    linux:\n      image: ghcr.io/hellt/network-multitool\n\n  nodes:\n    leaf1:\n      kind: srl\n      type: ixrd2\n    leaf2:\n      kind: srl\n      type: ixrd2\n    spine1:\n      kind: srl\n      type: ixrd3\n    srv1:\n      kind: linux\n    srv2:\n      kind: linux\n\n  links:\n    # inter-switch links\n    - endpoints: [\"leaf1:e1-49\", \"spine1:e1-1\"]\n    - endpoints: [\"leaf2:e1-49\", \"spine1:e1-2\"]\n    # server links\n    - endpoints: [\"srv1:eth1\", \"leaf1:e1-1\"]\n    - endpoints: [\"srv2:eth1\", \"leaf2:e1-1\"]\n</code></pre> <p>Save2 the contents of this file under <code>evpn01.clab.yml</code> name and you are ready to deploy: <pre><code>$ containerlab deploy -t evpn01.clab.yml\nINFO[0000] Parsing &amp; checking topology file: evpn01.clab.yml \nINFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 \nINFO[0000] Creating root CA                             \nINFO[0001] Creating container: srv2                  \nINFO[0001] Creating container: srv1                  \nINFO[0001] Creating container: leaf2                    \nINFO[0001] Creating container: spine1                   \nINFO[0001] Creating container: leaf1                    \nINFO[0002] Creating virtual wire: leaf1:e1-49 &lt;--&gt; spine1:e1-1 \nINFO[0002] Creating virtual wire: srv2:eth1 &lt;--&gt; leaf2:e1-1 \nINFO[0002] Creating virtual wire: leaf2:e1-49 &lt;--&gt; spine1:e1-2 \nINFO[0002] Creating virtual wire: srv1:eth1 &lt;--&gt; leaf1:e1-1 \nINFO[0003] Writing /etc/hosts file                      \n\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |              Image              | Kind  | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| 1 | clab-evpn01-leaf1  | 4b81c65af558 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.7/24 | 2001:172:20:20::7/64 |\n| 2 | clab-evpn01-leaf2  | de000e791dd6 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.8/24 | 2001:172:20:20::8/64 |\n| 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.6/24 | 2001:172:20:20::6/64 |\n| 4 | clab-evpn01-srv1   | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 5 | clab-evpn01-srv2   | fb722453d715 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n</code></pre></p> <p>A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of <code>leaf1</code>:</p> <pre><code># default credentials admin:admin\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey!</p>  <p>Note</p> <p>We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough4 details to survive in the configuration waters we are about to get.</p>    <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work, but if they don't, please pin the version to the mentioned ones.\u00a0\u21a9</p> </li> <li> <p>Or download it with <code>curl -LO https://github.com/srl-labs/learn-srlinux/blob/master/labs/evpn01.clab.yml</code> \u21a9</p> </li> <li> <p>Per RFC 8365 &amp; RFC 7432 \u21a9</p> </li> <li> <p>For a complete documentation coverage don't hesitate to visit our documentation portal.\u00a0\u21a9</p> </li> </ol>","title":"Lab deployment"},{"location":"tutorials/l2evpn/summary/","text":"<p>Layer 2 EVPN services with VXLAN dataplane are very common in multi-tenant data centers. In this tutorial we walked through every step that is needed to configure a basic Layer 2 EVPN with VXLAN dataplane service deployed on SR Linux switches:</p> <ul> <li>IP fabric config using eBGP in the underlay</li> <li>EVPN service config on leaf switches with the control and data plane verification</li> </ul> <p>The highly detailed configuration &amp; verification steps helped us achieve the goal of creating an overlay Layer 2 broadcast domain for the two servers in our topology. So that the high level service diagram transformed into a detailed map of configuration constructs and instances.</p>  <p>During the verification phases we collected the following packet captures to prove the control/data plane behavior:</p> <ul> <li>Exchange of the IMET/RT3 EVPN routes. IMET/RT3 routes are the starting point in the L2 EVPN-VXLAN services, as they are used to dynamically discover the VXLAN VTEPs participating in the same EVI.</li> <li>Exchange of MAC-IP/RT2 EVPN routes which convey the MAC information of the attached servers. These routes are used to create unicast tunnel destinations that the dataplane frames will use.</li> </ul>  <p>Info</p> <p>The more advanced EVPN topics listed below will be covered in separate tutorials:</p> <ul> <li>EVPN L2 multi-homing</li> <li>MAC mobility</li> <li>MAC duplication and loop protection</li> </ul>","title":"Summary"},{"location":"yang/browser/","text":"<p>YANG data models are the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse, search through, and process these data models.</p> <p>To answer these demands, we created a web portal - yang.srlinux.dev - it offers:</p> <ul> <li>Fast Path Browser to effectively search through thousands of available YANG paths</li> <li>Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux</li> <li>Source <code>.yang</code> files neatly stored in <code>nokia/srlinux-yang-models</code> repository for programmatic access and code generation</li> </ul>  <p></p>  <p>The web portal's front page aggregates links to individual releases of YANG models. Select the needed version to open the web view of the YANG tools we offer.</p>  <p></p>  <p>The main stage of the YANG Browser view is dedicated to the Path Browser , as it is the most efficient way to search through the model. Additional tools are located in the upper right corner . Let's cover them one by one.</p>","title":"SR Linux YANG Browser"},{"location":"yang/browser/#path-browser","text":"<p>As was discussed before, SR Linux is a fully modeled system with its configuration and state data entirely covered with YANG models. Consequently, to access any data for configuration or state, one needs to follow the YANG model. Effectively searching for those YANG-based access paths is key to rapid development and operations. For example, how to tell which one to use to get ipv4 statistics of an interface?</p> <p>With Path Browser, it is possible to search through the entire SR Linux YANG model and extract the paths to the leaves of interest. The Path Browser area is composed of three main elements:</p> <ul> <li>search input for entering the query </li> <li>Config/State selector </li> <li>table with results for a given search input </li> </ul>  <p> </p> Path Browser elements  <p>A user types in a search query, and the result is rendered immediately in the table with the matched words highlighted. The Config/State selector allows users to select if they want the table to show config, state, or all leaves. The state leaf is a leaf that has <code>config false</code> statement2.</p>","title":"Path Browser"},{"location":"yang/browser/#path-structure","text":"<p>The table contains the flattened XPATH-like paths3 for every leaf of a model sorted alphabetically.</p> <ul> <li>Each path is denoted with a State attribute in the first column of a table. Leaves, which represent the state data, will have the <code>true</code> value in the first column2.</li> <li>List elements are represented in the paths as <code>list-element[key-name=*]</code> - a format suitable for gNMI subscriptions.</li> <li>Each leaf is provided with the type information.</li> </ul>","title":"Path structure"},{"location":"yang/browser/#search-capabilities","text":"<p>Snappy search features of the Path Browser make it a joy to use when exploring the model or looking for a specific leaf of interest.</p> <p>Let's imagine we need to solve the simple task of subscribing to interface traffic statistics. How would we know which gNMI path corresponds to the traffic statistics counters? Should we try reading source YANG files? But it is challenging as models have lots of imports and quite some augmentations. A few moments and - you're lost. What about the tree representation of a model generated with <code>pyang</code>? Searching through something like pyang's tree output is impractical since searching the tree representation can't include more than one search parameter. The search becomes a burden on operators' eyes.</p> <p>Path Browser to the rescue. Its ability to return search requests instantaneously makes interrogating the model a walk in the park. The animation below demos a leaf-searching exercise where a user searches for a state leaf responsible for traffic statistics.  </p> <p>First, a user tries a logical search query <code>interface byte</code>, which yields some results, but it is easy to spot that they are not related to the task at hand. Thanks to the embedded highlighting capabilities, the search inputs are detectable in the resulting paths.</p> <p>Next, they try to use <code>interface octets</code> search query hoping that it will yield the right results, and so it does!</p>   <p>Tip</p> <p>Every table row denotes a leaf, and when a user hovers a mouse over a row, the popup appears with a description of the leaf.</p>","title":"Search capabilities"},{"location":"yang/browser/#tree-browser","text":"<p>The Path Browser is great to search through the entire model, but because it works on flattened paths, it hides the \"tree\" view of the model. Sometimes the tree representation is the best side to look at the models with a naked eye, as the hierarchy becomes very clear.</p> <p>To not strip our users of the beloved tree view mode, we enhanced the <code>pyang -f jstree</code> output and named this view Tree Browser.</p>  <p> </p> Access Tree Browser  <p>The tree view of the model offers a step-by-step exploration of the SR Linux model going from the top-level modules all the way down to the leaves. The tree view displays the node's type (leaf/container/etc) as well as the leaf type and the read-only status of a leaf.</p>  <p> </p> Tree Browser view   <p>Tip</p> <p>Every element of a tree has a description that becomes visible if you hover over the element with a mouse. </p>","title":"Tree Browser"},{"location":"yang/browser/#tree-and-paths","text":"<p>If you feel like everything in the world better be in ASCII, then Tree and Paths menu elements will satisfy the urge. These are the ASCII tree of the SR Linux model1 and the text flattened paths that are used in the Path Browser.</p>  <p> </p> Text version of tree and paths  <p>The textual paths can be, for example, fetched with curl and users can <code>sed</code> themselves out doing comprehensive searches or path manipulations.</p>   <ol> <li> <p>extracted with <code>pyang -f tree</code> \u21a9</p> </li> <li> <p>refer to https://datatracker.ietf.org/doc/html/rfc6020#section-4.2.3 \u21a9\u21a9</p> </li> <li> <p>paths are generated from the YANG model with gnmic \u21a9</p> </li> </ol>","title":"Tree and Paths"},{"location":"yang/yang/","text":"<p>Model-driven (MD) interfaces are becoming essential for robust and modern Network OSes. The changes required to create fully model-driven interfaces can not happen overnight - it is a long and tedious process that requires substantial R&amp;D effort. Traditional Network OSes often had to take an evolutionary route with adding MD interfaces on top of the existing internal infrastructure.</p>  <p> </p> SR Linux ground-up support for YANG  <p>Unfortunately, bolting on model-driven interfaces while keeping the legacy internal infrastructure layer couldn't fully deliver on the promises of MD interfaces. In reality, those new interfaces had visibility discrepancies1, which often led to a situation where users needed to mix and match different interfaces to achieve some configuration goal. Apparently, without adopting a fully modeled universal API, it is impossible to make a uniform set of interfaces offering the same visibility level into the NOS.</p> <p>Nokia SR Linux was ground-up designed with YANG2 data modeling taking a central role. SR Linux makes extensive use of structured data models with each application regardless if it's being provided by Nokia or written by a user has a YANG model that defines its configuration and state.</p>  <p> </p> Both Nokia and customer's apps are modeled in YANG  <p>SR Linux exposes the YANG models to the supported management APIs. For example, the command tree in the CLI is derived from the SR Linux YANG models loaded into the system, and a gNMI client uses RPCs to configure an application based on its YANG model. When a configuration is committed, the SR Linux management server validates the YANG models and translates them into protocol buffers for the impart database (IDB).</p> <p>With this design, there is no way around YANG; the data model is defined first for any application SR Linux has, then the CLI, APIs, and show output formats derived from it.</p>","title":"SR Linux & YANG"},{"location":"yang/yang/#sr-linux-yang-models","text":"<p>As YANG models play a central role in SR Linux NOS, it is critical to have unobstructed access. With that in mind, we offer SR Linux users many ways to get ahold of SR Linux YANG models:</p> <ol> <li>Download modules from SR Linux NOS itself.     The models can be found at <code>/opt/srlinux/models/*</code> location.</li> <li>Fetch modules from <code>nokia/srlinux-yang-models</code> repo.</li> <li>Use SR Linux YANG Browser to consume modules in a human-friendly way</li> </ol> <p>SR Linux employs a uniform mapping between a YANG module name and the CLI context, making it easy to correlate modules with CLI contexts.</p>   YANG modules and CLI aligned  <p>The structure of the Nokia SR Linux native models may look familiar to the OpenConfig standard, where different high-level domains are contained in their modules.</p> <p>Source <code>.yang</code> files are great for YANG-based automation tools such as ygot but are not so easy for a human's eye. For living creatures, we offer a YANG Browser portal. We suggest people use it when they want to consume the models in a non-programmable way.</p>   <ol> <li> <p>indicated by the blue color on the diagram and explained in detail in NFD25 talk.\u00a0\u21a9</p> </li> <li> <p>RFC 6020 and RFC 7950 \u21a9</p> </li> </ol>","title":"SR Linux YANG Models"}]}