{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"alwayson/","title":"Always-ON SR Linux Instance","text":"<p>It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab.</p> <p>But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.</p>"},{"location":"alwayson/#what-is-always-on-sr-linux-for","title":"What is Always-ON SR Linux for?","text":"<p>The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user.</p> <ul> <li> <p>getting to know SR Linux CLI     SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier.     New users can make their first steps by looking at the <code>show</code> commands, exploring the datastores, running <code>info from</code> commands and getting the grips of configuration basics by entering into the configuration mode.</p> </li> <li> <p>YANG browsing     By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers.</p> </li> <li> <p>gNMI exploration     The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux.     gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves.</p> </li> </ul>"},{"location":"alwayson/#connection-details","title":"Connection details","text":"<p>Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces:</p> Method Details SSH address: <code>ssh guest@on.srlinux.dev -p 44268</code>password: <code>n0k1asrlinux</code>for key-based authentication use this key to authenticate the <code>guest</code> user gNMI<sup>1</sup> <pre><code>gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\      capabilities</code></pre> JSON-RPC<sup>2</sup> http://http.on.srlinux.dev"},{"location":"alwayson/#gnmi","title":"gNMI","text":"<p>SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate.</p>"},{"location":"alwayson/#guest-user","title":"Guest user","text":"<p>The <code>guest</code> user has the following settings applied to it:</p> <ol> <li>Read-only mode</li> <li><code>bash</code> and <code>file</code> commands are disabled</li> </ol> <p>Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that <code>guest</code> can't commit them.</p>"},{"location":"alwayson/#always-on-sandbox-setup","title":"Always-ON sandbox setup","text":"<p>The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node.</p>"},{"location":"alwayson/#protocols-and-services","title":"Protocols and Services","text":"<p>We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise.</p> <p>The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services.</p> <p>In the overlay the following services are configured:</p> <ol> <li>Layer 2 EVPN with VXLAN dataplane<sup>1</sup> with <code>mac-vrf-100</code> network instance created on SR Linux</li> <li>Layer 3 EVPN with VXLAN dataplane with <code>ip-vrf-200</code> network instance created on SR Linux</li> </ol> <ol> <li> <p>check this tutorial to understand how this service is configured\u00a0\u21a9\u21a9</p> </li> <li> <p>HTTP service running over port 80\u00a0\u21a9</p> </li> </ol>"},{"location":"community/","title":"Community","text":""},{"location":"community/#discord-server","title":"Discord server","text":"<p>SR Linux has lots to offer to various groups of engineers...</p> <p>Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS.</p> <p>Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI.</p> <p>Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support.</p> <p>We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community<sup>1</sup> is the SR Linux Discord Server which everyone can join!</p> <p> Join SR Linux Discord Server</p> <ol> <li> <p>this is an unofficial community. Engineers to engineers.\u00a0\u21a9</p> </li> </ol>"},{"location":"get-started/","title":"Get Started","text":"<p>SR Linux packs a lot of unique features that the data center networking teams can leverage. Some of the features are truly new to the networking domain. The goal of this portal is to introduce SR Linux to visitors through interactive tutorials centered around SR Linux services and capabilities.</p> <p>We believe that learning by doing yields the best results. With that in mind we made SR Linux container image available to everybody without any registration or licensing requirements </p> <p>The public SR Linux container image when powered by containerlab allows us to create easily deployable labs that everyone can launch at their convenience. All that to let you not only read about the features we offer, but to try them live!</p>"},{"location":"get-started/#sr-linux-container-image","title":"SR Linux container image","text":"<p>A single container image that hosts management, control and data plane functions is all you need to get started.</p>"},{"location":"get-started/#getting-the-image","title":"Getting the image","text":"<p>To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry. This means that you can pull SR Linux container image exactly the same way as you would pull any other image:</p> <pre><code>docker pull ghcr.io/nokia/srlinux\n</code></pre> <p>When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the <code>nokia/srlinux</code> image has and change the <code>docker pull</code> command accordingly.</p>"},{"location":"get-started/#_1","title":"Get Started","text":""},{"location":"get-started/#running-sr-linux","title":"Running SR Linux","text":"<p>When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces.</p> <p>A system on which you can run SR Linux containers should conform to the following requirements:</p> <ol> <li>Linux OS with a kernel v4.10+<sup>1</sup>.</li> <li>Docker container runtime.</li> <li>At least 2 vCPU and 4GB RAM.</li> <li>A user with administrative privileges.</li> </ol> <p>Let's explore the different ways you can launch SR Linux container.</p>"},{"location":"get-started/#docker-cli","title":"Docker CLI","text":"<p><code>docker</code> CLI offers a quick way to run a standalone SR Linux container:</p> Docker runTopology file <p>Note the presence of topology file mount in the <code>docker run</code> command, it is used to drive a selection of the emulated chassis type. <pre><code>docker run -t -d --rm --privileged \\\n-v $(pwd)/topology.yml:/tmp/topology.yml \\\n-u $(id -u):$(id -g) \\\n--name srlinux ghcr.io/nokia/srlinux \\\nsudo bash /opt/srlinux/bin/sr_linux\n</code></pre></p> <p>The following topology file is for IXR-D3L chassis.</p> <pre><code>chassis_configuration:\n\"chassis_type\": 72\n\"base_mac\": \"1a:b0:00:00:00:00\"\n\"cpm_card_type\": 187\nslot_configuration:\n1:\n\"card_type\": 187\n\"mda_type\": 200\n</code></pre> <p>The above command will start the container named <code>srlinux</code> on the host system with a single management interface attached to the default docker network.</p> <p>Warning</p> <p>To get SSH access for a deployed container, make sure to disable TX Offload on a default docker bridge, otherwise, CRC checksums will be fake and SR Linux will discard those packets.</p> <pre><code>sudo ethtool --offload docker0 tx off\n</code></pre> <p>This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work.</p> <p>For multi-node SR Linux deployments containerlab<sup>3</sup> offers a better way.</p>"},{"location":"get-started/#containerlab","title":"Containerlab","text":"<p>Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle.</p> <p>A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together:</p> <pre><code># file: srlinux.clab.yml\nname: srlinux\ntopology:\nnodes:\nsrl1:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nsrl2:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nlinks:\n- endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <p>By copying this file over to your system you can immediately deploy it with containerlab:</p> <pre><code>containerlab deploy -t srlinux.clab.yml\n</code></pre> <pre><code>INFO[0000] Parsing &amp; checking topology file: srlinux.clab.yml \nINFO[0000] Creating lab directory: /root/demo/clab-srlinux \nINFO[0000] Creating container: srl1                     \nINFO[0000] Creating container: srl2                     \nINFO[0001] Creating virtual wire: srl1:e1-1 &lt;--&gt; srl2:e1-1 \nINFO[0001] Writing /etc/hosts file                      \n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |         Image         | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlinux-srl1  | 50826b3e3703 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n| 2 | clab-srlinux-srl2  | 4d4494aba320 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n</code></pre>"},{"location":"get-started/#deployment-verification","title":"Deployment verification","text":"<p>Regardless of the way you spin up SR Linux container it will be visible in the output of the <code>docker ps</code> command. If the deployment process went well and the container did not exit, a user can see it with <code>docker ps</code> command:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                      COMMAND                  CREATED             STATUS             PORTS                    NAMES\n4d4494aba320   ghcr.io/nokia/srlinux      \"/tini -- fixuid -q \u2026\"   32 minutes ago      Up 32 minutes                               clab-learn-01-srl2\n</code></pre> <p>The logs of the running container can be displayed with <code>docker logs &lt;container-name&gt;</code>.</p> <p>In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the <code>docker ps</code> output as this command only shows running containers. Containers which are in the exited status will be part of the <code>docker ps -a</code> output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination.</p>"},{"location":"get-started/#documentation","title":"Documentation","text":"<p>This portal does not substitute but augments the official SR Linux documentation. You can find official docs using one of the following links:</p> <ol> <li>SR Linux documentation collection - https://documentation.nokia.com/srlinux</li> <li>Short and memorizable URLs with the release version being part of the URL<ol> <li>https://doc.srlinux.dev/22-11 for the main documentation pages of SR Linux 22.11 release.</li> <li>https://doc.srlinux.dev/rn22-11-2 for a direct link to Release Notes.</li> </ol> </li> <li>Network Infrastructure documentation collection - https://bit.ly/iondoc</li> </ol>"},{"location":"get-started/#connecting-to-sr-linux","title":"Connecting to SR Linux","text":"<p>When SR Linux container is up and running, users can connect to it over different interfaces.</p>"},{"location":"get-started/#cli","title":"CLI","text":"<p>One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface.</p> <p>To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the <code>sr_cli</code> process inside of it:</p> <pre><code># get SR Linux container name -&gt; clab-srl01-srl\n$ docker ps\nCONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS         PORTS                    NAMES\n17a47c58ad59   ghcr.io/nokia/srlinux             \"/tini -- fixuid -q \u2026\"   10 seconds ago   Up 6 seconds                            clab-learn-01-srl1\n</code></pre> <pre><code># start the sr_cli process inside this container to get access to CLI\ndocker exec -it clab-learn-01-srl1 sr_cli\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           A:srl1#\n</code></pre> <p>The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials <code>admin:admin</code> you can connect to the CLI over the network:</p> <pre><code># containerlab creates local /etc/hosts entries\n# for container names to resolve to their IP\nssh admin@clab-learn-01-srl1\n\nadmin@clab-learn-01-srl1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           A:srl1#\n</code></pre>"},{"location":"get-started/#gnmi","title":"gNMI","text":"<p>SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400.</p> <p>Using the gNMI client<sup>2</sup> users can explore SR Linux' gNMI interface:</p> <pre><code>gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities\ngNMI version: 0.7.0\nsupported models:\n  - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31\n  - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30\n  - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31\n&lt;SNIP&gt;\n</code></pre> <ol> <li> <p>Centos 7.3+ has a 3.x kernel and won't be able to run SR Linux container images newer than v22.11.\u00a0\u21a9</p> </li> <li> <p>for example gnmic \u21a9</p> </li> <li> <p>The labs referenced on this site are deployed with containerlab unless stated otherwise\u00a0\u21a9</p> </li> </ol>"},{"location":"ansible/","title":"Ansible","text":"<p>Ansible is one of the leading configuration management frameworks both in the application and networking realms. It provides a quick route to network automation by offering a simple<sup>1</sup> domain-specific language (DSL) and a rich collection of modules written for different platforms and services.</p> <p>Nokia provides Ansible users with the <code>nokia.srlinux</code> Ansible collection that stores plugins and modules designed to perform automation tasks against Nokia SR Linux Network OS. The modules in <code>nokia.srlinux</code> collection are designed in a generic way with universal modules enabling configuration operations.</p> <p>In contrast to the supported <code>nokia.srlinux</code> collection, a community collection <code>srllabs.srlinux</code> is in the works where the community memebers work on the network resource modules for SR Linux.</p> <ol> <li> <p>Simplicity of a DSL often goes hand with constraints that a DSL has to impose when compared with a generic programming language. This may lead to complications when advanced data processing or branching control is required. For that reason it is common to hear sentiments that Ansible is easy to start with but may become a problem over time when automation tasks become more complex.\u00a0\u21a9</p> </li> </ol>"},{"location":"ansible/collection/","title":"Ansible Collection For SR Linux","text":"<p>Ansible collection for SR Linux is identified with <code>nokia.srlinux</code> fully qualified collection name and contains the plugins and modules to interface with SR Linux devices.</p> Summary Collection name <code>nokia.srlinux</code> Galaxy URL nokia/srlinux Github repository nokia/srlinux-ansible-collection SR Linux version &gt;=23.3.1<sup>1</sup> Python version &gt;=3.6 <p>Modules contained within this collection fully conform to the idempotence principles of Ansible, as well as provide first-class support for diff and check functionality<sup>2</sup>.</p> <p>Tutorial</p> <p>Besides examples provided for each module of this collection, checkout Ansible with SR Linux tutorial to see how to use this collection in practice.</p>","tags":["ansible"]},{"location":"ansible/collection/#modules","title":"Modules","text":"<p>Nokia SR Linux collection provides the following modules that work via HttpApi connection plugin and therefore rely on SR Linux's JSON-RPC server:</p> <ul> <li><code>nokia.srlinux.get</code> - retrieve configuration and state.</li> <li><code>nokia.srlinux.config</code> - configure SR Linux devices via model driven interface.</li> <li><code>nokia.srlinux.cli</code> - execute CLI commands.</li> <li><code>nokia.srlinux.validate</code> - validate provided configuration.</li> </ul> <p>Architecturally the modules provide a generic way to retrieve data from and configure SR Linux devices in a model-driven way<sup>3</sup>. Because SR Linux is a fully-modelled NOS, users should leverage the YANG Browser to navigate the datastores and understand the data model.</p> <p>Quick examples of how to use the modules. For more information on each module options and return values, please refer to the module's documentation page.</p> getconfigclivalidate <p>Retrieve configuration and state from SR Linux devices.</p> <pre><code>- name: Get system information\nhosts: clab\ngather_facts: false\ntasks:\n- name: Get /system/information container\nnokia.srlinux.get:\npaths:\n- path: /system/information\ndatastore: state\n</code></pre> <p>Configure SR Linux devices.</p> <pre><code>- name: Set leaves\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set system information with values\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\n</code></pre> <p>Execute CLI commands on SR Linux devices.</p> <pre><code>- name: Execute \"show version\" CLI command\nhosts: clab\ngather_facts: false\ntasks:\n- name: Execute \"show version\" CLI command\nnokia.srlinux.cli:\ncommands:\n- show version\n</code></pre> <p>Validate intended configuration.</p> <pre><code>- name: Validate\nhosts: clab\ngather_facts: false\ntasks:\n- name: Validate a valid change set\nnokia.srlinux.validate:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\n</code></pre>","tags":["ansible"]},{"location":"ansible/collection/#installation","title":"Installation","text":"<p>The recommended way to install galaxy collections is via the <code>ansible-galaxy</code> CLI command that ships with the ansible installation.</p> Latest versionSpecific version <pre><code>ansible-galaxy collection install nokia.srlinux\n</code></pre> <pre><code>ansible-galaxy collection install nokia.srlinux:0.1.0 #(1)!\n</code></pre> <ol> <li>Available version are listed in collection's home page.</li> </ol> <p>Collection is installed at the collections path. Default location is <code>~/.ansible/collections/ansible_collections/nokia/slinux</code>.</p>","tags":["ansible"]},{"location":"ansible/collection/#sr-linux-configuration","title":"SR Linux configuration","text":"<p>The factory configuration that SR Linux ships with doesn't have JSON-RPC interface enabled. In order to use the modules of this collection, users should enable and configure the JSON-RPC server.</p>","tags":["ansible"]},{"location":"ansible/collection/#ansible_connection","title":"<code>ansible_connection</code>","text":"<p>Ansible supports a number of connection modes to establish connectivity with devices over multiple connection protocols. SSH, HTTP and Local connections are likely the most popular one.</p> <p>The modules in <code>nokia.srlinux</code> collection interface with SR Linux devices over its JSON-RPC interface, and thus leverage HttpApi connection plugin of Ansible's <code>netcommon</code> collection.</p> <p>To instruct Ansible to use <code>ansible.netcommon.httpapi</code> connection plugin, users should set the <code>ansible_connection</code> variable to <code>ansible.netcommon.httpapi</code>.</p> <p>There are many ways to set this variable, though most common one is to set in the inventory file:</p> <pre><code>[clab]\nclab-ansible-srl  ansible_connection=ansible.netcommon.httpapi ;(1)!\n</code></pre> <ol> <li>Other variables are omitted</li> </ol>","tags":["ansible"]},{"location":"ansible/collection/#ansible_network_os","title":"<code>ansible_network_os</code>","text":"<p>With <code>ansible_network_os</code> variables users select which Network OS the target host runs. For Nokia SR Linux this variable should be set to <code>nokia.srlinux.srlinux</code> value.</p> <p>Most commonly, this variable is set in the inventory file:</p> <pre><code>[clab]\nclab-ansible-srl  ansible_network_os=nokia.srlinux.srlinux ;(1)!\n</code></pre> <ol> <li>Other variables are omitted</li> </ol>","tags":["ansible"]},{"location":"ansible/collection/#authentication","title":"Authentication","text":"<p>Basic HTTP authentication is used by the modules of this collection. Credentials are provided by the common <code>ansible_user</code> and <code>ansible_password</code> variables.</p>","tags":["ansible"]},{"location":"ansible/collection/#tls","title":"TLS","text":"<p>SR Linux JSON-RPC server which the modules of this connection connect to can operate in two modes:</p> <ol> <li>Insecure HTTP mode     No TLS certificates required, connection is not encrypted</li> <li>Secure HTTPS mode     Requires TLS certificate to be generated and configured.</li> </ol> <p>To instruct Ansible to connect to SR Linux device over https protocol users should set the <code>ansible_httpapi_use_ssl</code> variable to <code>true</code>. Again, this can be done on different levels, but most often it is part of the inventory file.</p> <p>When set <code>ansible_httpapi_use_ssl</code> is set to <code>true</code>, Ansible will try to establish the connection over https using <code>443</code> port.</p>","tags":["ansible"]},{"location":"ansible/collection/#certificate-validation","title":"Certificate validation","text":"<p>By default, when operating over https protocol, Ansible will try to validate the remote host's certificate. To disable certificate verification, set <code>ansible_httpapi_validate_certs</code> to <code>false</code>.</p>","tags":["ansible"]},{"location":"ansible/collection/#tls-13-support-and-cipher-suites","title":"TLS 1.3 support and cipher suites","text":"<p>In the recent Python versions (&gt;=3.10) default security settings for TLS have been hardened. More specifically the cipher suites have been restricted to the ones that are considered secure.</p> <p>SR Linux plans to implement TLS 1.3 support in release 23.7, and thus for the time being, users should explicitly set the <code>ansible_httpapi_ciphers</code> variable to the cipher suite that is supported, for example <code>ECDHE-RSA-AES256-SHA</code>.</p>","tags":["ansible"]},{"location":"ansible/collection/#example-hosts-file","title":"Example <code>hosts</code> file","text":"<p>With the above configuration options in mind, the following <code>hosts</code> file demonstrates their usage to connect to the SR Linux device:</p> <pre><code>[clab]\nclab-ansible-srl ansible_connection=ansible.netcommon.httpapi ansible_user=admin ansible_password=NokiaSrl1! ansible_network_os=nokia.srlinux.srlinux ansible_httpapi_ciphers=ECDHE-RSA-AES256-SHA\n</code></pre>","tags":["ansible"]},{"location":"ansible/collection/#lab","title":"Lab","text":"<p>To demonstrate the usage of the modules in this collection, we will use a very simple containerlab topology to deploy a single SR Linux node:</p> <pre><code>name: ansible\ntopology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux:23.3.1\n</code></pre> <p>Save the file as <code>ansible.clab.yml</code> and deploy the lab with <code>containerlab deploy -t ansible.clab.yml</code>.</p> <p>Create the <code>hosts</code> file as shown in the previous section and you're ready to tryout the modules of this collection.</p> <ol> <li> <p><code>nokia.srlinux</code> collection requires SR Linux 23.3.1 or later.\u00a0\u21a9</p> </li> <li> <p>See <code>config</code> module for details.\u00a0\u21a9</p> </li> <li> <p>The modules purposefully don't follow network resource modules pattern, as we wanted to provide a generic way to access the whole configuration and state datastores of the device.\u00a0\u21a9</p> </li> </ol>","tags":["ansible"]},{"location":"ansible/collection/cli/","title":"<code>cli</code> Module","text":"<p>The <code>cli</code> module is used to execute CLI commands on SR Linux devices. It is most often used to get the output of <code>show</code> or to execute CLI plugins that do not belong to the YANG module.</p> PlaybookResponse with default formatResponse with text format <pre><code>- name: Run \"show version\" CLI command\nhosts: clab\ngather_facts: false\ntasks:\n- name: Run \"show version\" CLI command\nnokia.srlinux.cli:\ncommands:\n- show version\nregister: response\n- debug:\nvar: response\n</code></pre> <pre><code>{\n\"changed\": false,\n\"failed\": false,\n\"jsonrpc_req_id\": \"2023-05-01 10:06:52:663255\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n{\n\"basic system info\": {\n\"Architecture\": \"x86_64\",\n\"Build Number\": \"343-gab924f2e64\",\n\"Chassis Type\": \"7250 IXR-6\",\n\"Free Memory\": \"27532316 kB\",\n\"Hostname\": \"srl\",\n\"Last Booted\": \"2023-04-26T20:14:32.259Z\",\n\"Part Number\": \"Sim Part No.\",\n\"Serial Number\": \"Sim Serial No.\",\n\"Software Version\": \"v23.3.1\",\n\"System HW MAC Address\": \"1A:2E:00:FF:00:00\",\n\"Total Memory\": \"36087609 kB\"\n}\n}\n]\n}\n</code></pre> <pre><code>{\n\"response\": {\n\"changed\": false,\n\"failed\": false,\n\"failed_when_result\": false,\n\"jsonrpc_req_id\": \"2023-05-01 10:06:55:082540\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\nHostname             : srl\\nChassis Type         : 7250 IXR-6\\nPart Number          : Sim Part No.\\nSerial Number        : Sim Serial No.\\nSystem HW MAC Address: 1A:2E:00:FF:00:00\\nSoftware Version     : v23.3.1\\nBuild Number         : 343-gab924f2e64\\nArchitecture         : x86_64\\nLast Booted          : 2023-04-26T20:14:32.259Z\\nTotal Memory         : 36087609 kB\\nFree Memory          : 27532316 kB\\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\"\n]\n}\n}\n</code></pre> <p>It is possible to use this module to configure the system by sending over CLI commands that will enter into the candidate mode, make changes and then commit them. However, this is not recommended as it is not idempotent.</p>"},{"location":"ansible/collection/cli/#parameters","title":"Parameters","text":""},{"location":"ansible/collection/cli/#commands","title":"commands","text":"<p>List of commands to execute.</p>"},{"location":"ansible/collection/cli/#output_format","title":"output_format","text":"<p>choice: <code>json</code>, <code>text</code>, <code>table</code></p> <p>Format of the output. Default is <code>json</code>.</p> <p>SR Linux CLI subsystem supports three output formats: <code>json</code>, <code>text</code> and <code>table</code>.</p> <p>The <code>json</code> format is the default format and is used when <code>output_format</code> is not specified.</p> <p>The <code>text</code> format will return the output of the command as a string exactly as it would be displayed on the CLI.</p> <p>The <code>table</code> format will return the output of the command as a string but will apply a table formatting to it. This format is useful when users want to have a table view of <code>info show</code> commands.</p>"},{"location":"ansible/collection/cli/#return-values","title":"Return Values","text":"<p>Common Ansible return values such as <code>changed</code>, <code>failed</code> and common SR Linux values such as <code>jsonrpc_req_id</code> and <code>jsonrpc_version</code> are documented in the Get module.</p>"},{"location":"ansible/collection/cli/#result","title":"result","text":"<p>List of responses for each command using the specified output format.</p>"},{"location":"ansible/collection/config/","title":"<code>config</code> Module","text":"<p>Config module provides a flexible and performant way to configure SR Linux devices using a model-driven HTTP-based JSON-RPC interface.</p> <p>The generic architecture of a module allows configuration changes across the entire SR Linux configuration datastore.</p> Example playbookResponse <pre><code>- name: Set system information\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set system information with values\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\nregister: set_response\n</code></pre> <pre><code>ok: [clab-ansible-srl] =&gt; {\n\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"jsonrpc_req_id\": \"2023-04-28 18:50:19:695364\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"      system {\\n          information {\\n+             contact \\\"Some contact\\\"\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n]\n},\n\"failed\": false,\n\"jsonrpc_req_id\": 3038,\n\"saved\": false\n}\n}\n</code></pre> <p><code>path</code> and <code>value</code> parameters take values based on SR Linux YANG models, enabling fully model-driven configuration operations.</p>"},{"location":"ansible/collection/config/#config-operations","title":"Config operations","text":"<p>The module provides an <code>update</code>, <code>replace</code> and <code>delete</code> operations against SR Linux devices. Moreover, a combination of these operations can be handled by the module enabling rich configuration transactions.</p>"},{"location":"ansible/collection/config/#idempotency","title":"Idempotency","text":"<p>Idempotency principles are fully honored by the module. When users execute a module with configuration operations, the module runs the <code>diff</code> operation to check if the intended configuration set leads to any changes.</p> <p>The <code>diff</code> operation takes <code>update</code>, <code>replace</code> and <code>delete</code> values and applies them to the newly opened candidate configuration. First, the <code>validate</code> function ensures that the provided changeset passes internal validation and then <code>diff</code> function is called to identify if these changes produce any diff output.</p> <p>The output of the <code>diff</code> operation is then returned to the <code>config</code> module. An empty return indicates that the configuration set does not lead to any changes; thus, the <code>config</code> module reports that task as unchanged.</p> <p>A non-empty diff output would mean that the configuration set leads to changes, and therefore the configuration set will be sent<sup>1</sup> to the device to be applied.</p>"},{"location":"ansible/collection/config/#transactions-and-ordering","title":"Transactions and ordering","text":"<p>The generic nature of the module and its ability to contain multiple configuration operations allows its user to create configuration transactions which include a number of <code>update</code>, <code>delete</code> and <code>replace</code> operations.</p> <p>In the example below, the module is stuffed with multiple updates, replaces and deletes. It is important to understand how SR Linux processes such requests.</p> <code>config</code> with multiple operationsResponse <pre><code>- name: Set multiple paths\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set with multiple operations\nnokia.srlinux.config:\nupdate:\n- path: /system/information/location #(1)!\nvalue: Some location\nreplace:\n- path: /system/gnmi-server/trace-options #(1)!\nvalue:\n- request\n- common\ndelete:\n- path: /system/json-rpc-server/network-instance[name=mgmt]/https\nregister: set_response\n</code></pre> <ol> <li>When a path points to a leaf, the value is provided as string or integer.</li> <li>When a path points to a list (aka leaf-list in YANG), the value is provided as a list of scalar values.</li> </ol> <pre><code>{\n\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"jsonrpc_req_id\": \"2023-05-01 08:37:10:798782\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"      system {\\n          gnmi-server {\\n-             trace-options [\\n-                 response\\n-             ]\\n          }\\n          json-rpc-server {\\n              network-instance mgmt {\\n-                 https {\\n-                     admin-state enable\\n-                     tls-profile clab-profile\\n-                 }\\n              }\\n          }\\n          information {\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n]\n},\n\"failed\": false,\n\"jsonrpc_req_id\": \"2023-05-01 08:37:11:009871\",\n\"saved\": false\n}\n}\n</code></pre> <p>Config module uses JSON-RPC interface of SR Linux, and in particular, its <code>Set</code> method. Upon receiving the Set method, SR Linux opens a private named candidate configuration and applies the operations in the  following order:</p> <ol> <li>updates</li> <li>replaces</li> <li>deletes</li> </ol> <p>Note, that the operations that are part of the module's task are not committed independently, they are applied strictly together in the \"all or nothing\" fashion. When we say that the operations are applied in that order, we mean that changes get applied to the candidate configuration in that order. No commit happens just yet.</p> <p>If no errors occur during changes of the candidate configuration, an implicit commit is invoked to apply all the changes in a single transaction. If the commit fails, the configuration is automatically reverted, and the error is returned to the caller. Such functionality provides users with the means to reliably apply multiple changes in a single transaction, without worrying that the devices might be left in a partially configured state.</p>"},{"location":"ansible/collection/config/#check-mode","title":"Check mode","text":"<p>The module fully supports Ansible's check mode. When the module is executed in the check mode, the <code>diff</code> operation indicates whether the task leads to a change in the device's state.</p> <p>Note</p> <p>No changes are applied to the device when running with <code>check</code> mode. It is a safe way to understand if the intended configuration set leads to any changes.</p> <p>As per Ansible's documentation, the <code>check</code> mode can be enabled on a per-playbook or per-task basis. The following example shows how to enable the <code>check</code> mode for the task and the result of the playbook execution.</p> PlaybookResponse <p>The below playbook tests if the intended update operation leads to any changes in the device's state. The device doesn't have the location and contact set, so the diff output indicates that the task leads to a change.</p> <pre><code>- name: Set leaves with check mode\nhosts: clab\ngather_facts: false\ntasks:\n- name: Test check mode\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\ncheck_mode: true\nregister: set_response\n</code></pre> <p>The response object contains the <code>diff</code> output that indicates that the task leads to a change and therefore <code>changed</code> is set to <code>true</code>. Remember that the <code>check</code> mode doesn't apply any changes to the device.</p> <pre><code>{\n\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"prepared\": \"      system {\\n          information {\\n+             contact \\\"Some contact\\\"\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n},\n\"failed\": false,\n\"failed_when_result\": false,\n\"saved\": false\n}\n}\n</code></pre>"},{"location":"ansible/collection/config/#diff-mode","title":"Diff mode","text":"<p>The module supports the <code>diff</code> mode that allows users to display the diff output. This mode can be used both with the <code>check</code> mode and without it. When used with <code>check</code> mode the diff output is displayed but no changes are applied to the device. When used without <code>check</code> mode, the diff output is displayed and the changes are applied to the device.</p> <p>The diff is calculated by the SR Linux device and thus is an accurate representation of the changes that will be applied to the device<sup>2</sup>.</p> PlaybookOutputResponse <p>In this playbook the <code>diff</code> and <code>check</code> modes are both enabled. The task should output the diff of the operation and indicate that the task leads to a change.</p> <pre><code>- name: Test check mode with diff\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\ncheck_mode: true\ndiff: true\nregister: set_response\n</code></pre> <p>As a result, the output contains the diff of the operation and indicates that the task leads to a change.</p> <pre><code>TASK [Test check mode with diff] ***********************************************\n     system {\n          information {\n+             contact \"Some contact\"\n+             location \"Some location\"\n         }\n      }\nchanged: [clab-ansible-srl]\n</code></pre> <p>The response object contains the <code>diff</code> output that indicates that the task leads to a change and therefore <code>changed</code> is set to <code>true</code>. Remember that the <code>check</code> mode doesn't allow the intended changes to be applied.</p> <pre><code>\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"prepared\": \"      system {\\n          information {\\n+             contact \\\"Some contact\\\"\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n},\n\"failed\": false,\n\"failed_when_result\": false,\n\"saved\": false\n}\n</code></pre>"},{"location":"ansible/collection/config/#name-alias","title":"Name alias","text":"<p>The <code>nokia.srlinux.config</code> module has a name alias to align with the historical naming scheme for config modules: <code>nokia.srlinux.srl_config</code>. Users can use any of the names.</p>"},{"location":"ansible/collection/config/#parameters","title":"Parameters","text":""},{"location":"ansible/collection/config/#update","title":"update","text":"<p>The <code>update</code> parameter is a list of update operations that are applied to the candidate configuration in the order they are provided. Each update operation is a dictionary with the <code>path</code> and <code>value</code> keys.</p>"},{"location":"ansible/collection/config/#path","title":"path","text":"<p>required \u00b7 type: string</p> <p>The <code>path</code> parameter is a string that represents a path to the configuration element that needs to be updated. The path is provided in the XPATH-like notation and is relative to the root of the candidate datastore.</p> <p>SR Linux users are encouraged to use YANG Browser to explore the YANG model and identify the paths to the data they want to retrieve.</p>"},{"location":"ansible/collection/config/#value","title":"value","text":"<p>type: any</p> <p>The <code>value</code> parameter is a value that needs to be user for the provided operations. The value type is determined by the type of the configuration element that is being targeted by the path.</p> Path YANG type Value type leaf string/integer/bool leaf-list list list dictionary <p>Check the examples below to see how the value is provided for each of the types.</p>"},{"location":"ansible/collection/config/#replace","title":"replace","text":"<p>The <code>replace</code> parameter is a list of replace operations that are applied to the candidate configuration in the order they are provided. Each replace operation is a dictionary with the <code>path</code> and <code>value</code> keys.</p> <p>Path and value parameters are the same as for the <code>update</code> parameter with the only difference that the <code>value</code> parameter is required for the <code>replace</code> operation.</p>"},{"location":"ansible/collection/config/#delete","title":"delete","text":"<p>The <code>delete</code> parameter is a list of delete operations that are applied to the candidate configuration in the order they are provided. Each delete operation is a dictionary with the <code>path</code> key and no <code>value</code>.</p>"},{"location":"ansible/collection/config/#path_1","title":"path","text":"<p>required \u00b7 type: string</p> <p>The <code>path</code> parameter is a string that represents a path to the configuration element that needs to be deleted. The path is provided in the XPATH-like notation and is relative to the root of the candidate datastore. No value is provided, since delete operation does not require any.</p>"},{"location":"ansible/collection/config/#datastore","title":"datastore","text":"<p>type: string</p> <p>The <code>datastore</code> parameter is a string that represents the name of the datastore that needs to be used for the configuration operations. The default value is <code>candidate</code>.</p> <p>The only other <code>datastore</code> value available is <code>tools</code>, which allows users to invoke operational commands via the <code>tools</code> datastore. See the following example for more details.</p>"},{"location":"ansible/collection/config/#save_when","title":"save_when","text":"<p>choice: <code>always</code>, <code>never</code>, <code>changed</code></p> <p>The <code>save_when</code> parameter allows users to choose when the candidate configuration should be saved to the startup configuration (aka persisted) upon successful commit. The default value is <code>never</code>.</p> <p>By setting the <code>save_when</code> to <code>always</code>, users can ensure that the candidate configuration is always saved to the startup configuration upon successful commit. And by setting the <code>save_when</code> to <code>changed</code>, users can ensure that the candidate configuration is saved to the startup configuration only when the candidate configuration is changed upon successful commit.</p> <p>The <code>save_when</code> has no effect when the <code>check</code> mode is enabled.</p>"},{"location":"ansible/collection/config/#yang_models","title":"yang_models","text":"<p>choice: <code>srl</code>, <code>oc</code></p> <p>The <code>yang_models</code> parameter allows users to choose which YANG models should be used for the configuration operations. The default value is <code>srl</code>. See Get module docs for more details.</p>"},{"location":"ansible/collection/config/#return-values","title":"Return values","text":"<p>The module returns the results in a structured format which contains both standard Ansible fields and SR Linux specific fields.</p> <pre><code>{\n\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"jsonrpc_req_id\": \"2023-04-29 13:02:42:777835\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"      system {\\n          gnmi-server {\\n-             trace-options [\\n-                 response\\n-             ]\\n          }\\n          json-rpc-server {\\n              network-instance mgmt {\\n-                 https {\\n-                     admin-state enable\\n-                     tls-profile clab-profile\\n-                 }\\n              }\\n          }\\n          information {\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n]\n},\n\"failed\": false,\n\"jsonrpc_req_id\": \"2023-04-29 13:02:42:985777\",\n\"saved\": false\n}\n}\n</code></pre>"},{"location":"ansible/collection/config/#changed","title":"changed","text":"<p>type: boolean</p> <p>The <code>changed</code> value indicates if the module has made any changes to the state of a device.</p> <p>When the module is executed without the <code>check</code> mode, the <code>changed</code> value is <code>true</code> if the module has made any changes to the state of a device. Otherwise, the <code>changed</code> value is <code>false</code>.</p> <p>When the module is executed with the <code>check</code> mode, the <code>changed</code> parameter can also become <code>true</code> or <code>false</code>, but in any case, the changes will not be committed to the device.</p>"},{"location":"ansible/collection/config/#failed","title":"failed","text":"<p>type: boolean</p> <p>The <code>failed</code> value indicates if any errors occurred during the execution of the module. The <code>failed</code> value is <code>false</code> when the module completes successfully and <code>true</code> otherwise. See the Error handling section for more details.</p>"},{"location":"ansible/collection/config/#jsonrpc_req_id","title":"jsonrpc_req_id","text":"<p>type: string</p> <p>See Get module.</p>"},{"location":"ansible/collection/config/#jsonrpc_version","title":"jsonrpc_version","text":"<p>type: string</p> <p>See Get module.</p>"},{"location":"ansible/collection/config/#diff","title":"diff","text":"<p>type: dictionary</p> <p>The <code>diff</code> value contains the response of a diff operation that was executed against the device.</p> <p>As explained in the idempotency section, the diff operation is always called to identify if the change is required. The result of that diff RPC is saved in the <code>diff</code> response parameter and contains the full response body object.</p>"},{"location":"ansible/collection/config/#saved","title":"saved","text":"<p>type: boolean</p> <p>The <code>saved</code> value indicates if the candidate configuration was saved to the startup configuration upon successful commit. The <code>saved</code> value is <code>true</code> when the candidate configuration was saved to the startup configuration and <code>false</code> otherwise.</p>"},{"location":"ansible/collection/config/#error-handling","title":"Error handling","text":"<p>The <code>config</code> module sets the <code>failed</code> return value to <code>true</code> when errors occur during the module execution. The error message is returned in the <code>msg</code> field of the return value.</p> <p>Consider the following output when a wrong path is value is used in the <code>update</code> operation:</p> playbookresponse <pre><code>- name: Set wrong value\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set system information with wrong value\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nwrong: Some location\nregister: set_response\n</code></pre> <p>The response will have <code>failed=true</code> and the <code>msg</code> will contain the error message as reported by SR Linux:</p> <pre><code>fatal: [clab-ansible-srl]: FAILED! =&gt; {\"changed\": false, \"id\": \"2023-05-01 08:42:59:116073\", \"method\": \"diff\", \"msg\": \"Schema '/system/information' has no local leaf with the name 'wrong'. Options are [contact, location, protobuf-metadata]: Parse error on line 1: {\\\"wrong\\\": \\\"Some location\\\"}\\nInput:\\n'{\\\"wrong\\\": \\\"Some location\\\"}'\"}\n</code></pre>"},{"location":"ansible/collection/config/#examples","title":"Examples","text":""},{"location":"ansible/collection/config/#adding-an-interface","title":"adding an interface","text":"<p>Let's see step by step how to use the <code>config</code> module to add the <code>ethernet-1/1</code> interface to a device.</p> <p>First, we need to identify the path to the interface configuration and the values that can be set on that path. One way to do that is to leverage YANG Browser's Tree viewer and visually inspect the YANG model. The following screenshot shows the YANG Browser's Tree viewer for SR Linux 23.3.1 release with <code>interfaces</code> model opened:</p> <p></p> <p>As we can see, the interfaces are nested under the <code>/interface[name=*]</code> list. This answers the first question: the path to the interface configuration is <code>/interface[name=ethernet-1/1]</code>.</p> <p>Using the same UI we can see which values can be set on the interface. The following screenshot shows the <code>interface</code> model with <code>subinterface</code> list expanded:</p> screenshot <p></p> <p>Now we know that the <code>description</code> leaf can be set on the interface, and we configure IPv4/6 addresses with subinterfaces. With that info, we can construct the following playbook:</p> <pre><code>- name: Add interface\nhosts: clab\ngather_facts: false\ntasks:\n- name: Add interface\nnokia.srlinux.config:\nupdate:\n- path: /interface[name=ethernet-1/1]\nvalue:\nadmin-state: enable\ndescription: \"interface description set with Ansible\"\nsubinterface:\n- index: 0\nadmin-state: enable\ndescription: \"subinterface description set with Ansible\"\nipv4:\nadmin-state: enable\naddress:\n- ip-prefix: 192.168.0.100/24\nregister: set_response\n- debug:\nvar: set_response\n</code></pre> <p>When constructing the payload we follow a simple rule:</p> <ul> <li>a list in the YANG model is represented as a list in the payload</li> <li>a container in the YANG model is represented as a dictionary in the payload</li> </ul> <p>For example, the <code>subinterface</code> list is represented as a list in the payload, and the <code>ipv4</code> container is represented as a dictionary in the payload.</p>"},{"location":"ansible/collection/config/#multiple-configuration-operations","title":"multiple configuration operations","text":"<p>The following example shows how to use the <code>config</code> module to perform multiple configuration operations in a single task. Note, that you can have multiple <code>update</code>, <code>replace</code>, and <code>delete</code> operations in a single task:</p> PlaybookResponse <pre><code>- name: Set multiple paths\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set with multiple operations\nnokia.srlinux.config:\nupdate:\n- path: /system/information/location\nvalue: Some location\nreplace:\n- path: /system/gnmi-server/trace-options\nvalue:\n- request\n- common\ndelete:\n- path: /system/json-rpc-server/network-instance[name=mgmt]/https\nregister: set_response\n</code></pre> <pre><code>{\n\"set_response\": {\n\"changed\": true,\n\"diff\": {\n\"jsonrpc_req_id\": \"2023-05-01 08:46:38:419093\",\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"      system {\\n          gnmi-server {\\n-             trace-options [\\n-                 response\\n-             ]\\n          }\\n          json-rpc-server {\\n              network-instance mgmt {\\n-                 https {\\n-                     admin-state enable\\n-                     tls-profile clab-profile\\n-                 }\\n              }\\n          }\\n          information {\\n+             location \\\"Some location\\\"\\n          }\\n      }\\n\"\n]\n},\n\"failed\": false,\n\"jsonrpc_req_id\": \"2023-05-01 08:46:38:617927\",\n\"saved\": false\n}\n}\n</code></pre>"},{"location":"ansible/collection/config/#operational-commands-via-tools-datastore","title":"operational commands via <code>tools</code> datastore","text":"<p>SR Linux models operational commands as configuration elements in the <code>tools</code> datastore. This allows users to invoke operational commands via the <code>nokia.srlinux.config</code> module. For example, the following task clears the interface statistics:</p> <pre><code>- name: Clear interface statistics\nnokia.srlinux.config:\ndatastore: tools\nupdate:\n- path: /interface[name=mgmt0]/statistics/clear\ndatastore: tools\n</code></pre>"},{"location":"ansible/collection/config/#openconfig","title":"openconfig","text":"<p>The <code>config</code> module supports OpenConfig models. As with <code>get</code> module, the <code>yang_models</code> parameter must be set to <code>oc</code>. Note, that for the <code>config</code> module, the <code>yang_models</code> parameter is set on a per-task basis.</p> <pre><code>- name: Set OC leaf\nhosts: clab\ngather_facts: false\ntasks:\n- name: Set openconfig leaf\nnokia.srlinux.config:\nupdate:\n- path: /system/config\nvalue:\nmotd-banner: \"hey ansible\"\nyang_models: oc\nregister: set_response\n- debug:\nvar: set_response\n</code></pre>"},{"location":"ansible/collection/config/#source-code","title":"Source code","text":"<p>Config module is implemented in the <code>config.py</code> file.</p> <ol> <li> <p>Effectively, the configuration is sent twice to the device. First, to check if a diff is non-empty, and if not, second time to apply the configuration set. In this case, the diff happens on SR Linux box and is not calculated locally.\u00a0\u21a9</p> </li> <li> <p>The diff returned is the same as the one returned by the <code>diff</code> CLI command.\u00a0\u21a9</p> </li> </ol>"},{"location":"ansible/collection/get/","title":"<code>get</code> Module","text":"<p>Get module is used to retrieve configuration and state from SR Linux devices. Users provide the datastore from which the data is retrieved and the paths to enclosing node or leaf. The module returns the requested data in JSON format.</p> Example playbookResponse <pre><code>- name: Get container\nhosts: clab\ngather_facts: false\ntasks:\n- name: Get /system/information container\nnokia.srlinux.get:\npaths:\n- path: /system/information\ndatastore: state\nyang_models: srl #(1)!\nregister: response\n- debug:\nvar: response\n</code></pre> <ol> <li><code>srl</code> YANG model is used when unspecified, so in this case this parameter could have been omitted. It is provided for demonstration purposes.</li> </ol> <pre><code>ok: [clab-ansible-srl] =&gt; {\n\"response\": {\n\"changed\": false,\n\"failed\": false,\n\"failed_when_result\": false,\n\"jsonrpc_req_id\": 65031,\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n{\n\"current-datetime\": \"2023-04-26T21:29:47.554Z\",\n\"description\": \"SRLinux-v23.3.1-343-gab924f2e64 7250 IXR-6 Copyright (c) 2000-2020 Nokia. Kernel 5.15.0-67-generic #74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023\",\n\"last-booted\": \"2023-04-26T20:14:35.789Z\",\n\"version\": \"v23.3.1-343-gab924f2e64\"\n}\n]\n}\n}\n</code></pre>"},{"location":"ansible/collection/get/#parameters","title":"Parameters","text":""},{"location":"ansible/collection/get/#paths","title":"paths","text":"<p>The <code>paths</code> parameter is a list of dictionaries. Each dictionary contains parameters used to identify the data to be retrieved.</p>"},{"location":"ansible/collection/get/#path","title":"path","text":"<p>required \u00b7 type: string</p> <p>The <code>path</code> parameter is a string that identifies the path to the yang node from which to retrieve the data. The path is provided in the XPATH-like notation and is relative to the root of the datastore.</p> <p>SR Linux users are encouraged to use YANG Browser to explore the YANG model and identify the paths to the data they want to retrieve.</p>"},{"location":"ansible/collection/get/#datastore","title":"datastore","text":"<p>type: string</p> <p>The <code>datastore</code> parameter is a string that identifies the datastore from which to retrieve the data. When omitted, the <code>running</code> datastore is used.</p> <p>The following datastore values are available:</p> <ul> <li><code>running</code> - the current configuration of the system</li> <li><code>candidate</code> - the configuration that is being edited but not yet committed</li> <li><code>baseline</code> - the configuration that was used to \"fork\" candidate from</li> <li><code>tools</code> - the datastore used to store operational commands</li> <li><code>state</code> - the current state of the system</li> </ul>"},{"location":"ansible/collection/get/#yang_models","title":"yang_models","text":"<p>choice: <code>srl</code>, <code>oc</code></p> <p>The <code>yang_models</code> parameter selects which YANG model is used with a specified path. SR Linux Network OS supports the following two values for the <code>yang_models</code> parameter:</p> <ol> <li><code>srl</code> - SR Linux native YANG model</li> <li><code>oc</code> - Openconfig model</li> </ol> <p>The default value for the <code>yang_models</code> parameter is <code>srl</code>, and if the parameter is omitted, the SR Linux native YANG model is assumed. Thus, this parameter is used when users want to retrieve data from the device using Openconfig model.</p>"},{"location":"ansible/collection/get/#return-values","title":"Return values","text":"<p>Module returns the results in a structured format with Ansible common return values and module specific values.</p> <pre><code>{\n\"changed\": false,\n\"failed\": false,\n\"jsonrpc_req_id\": 50861,\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n{\n\"current-datetime\": \"2023-04-27T09:23:07.382Z\",\n\"version\": \"v23.3.1-343-gab924f2e64\"\n}\n]\n}\n</code></pre>"},{"location":"ansible/collection/get/#changed","title":"changed","text":"<p>type: boolean</p> <p>For <code>get</code> module the <code>changed</code> value is always <code>false</code>, as retrieval operations never result in changing state of the device.</p>"},{"location":"ansible/collection/get/#failed","title":"failed","text":"<p>type: boolean</p> <p>The <code>failed</code> value indicates if any errors occurred during the execution of the module. The <code>failed</code> value is <code>false</code> when the module completes successfully and <code>true</code> otherwise. See the Error handling section for more details.</p>"},{"location":"ansible/collection/get/#jsonrpc_req_id","title":"jsonrpc_req_id","text":"<p>type: string</p> <p>The <code>jsonrpc_req_id</code> value is a string identifier that the module uses for the JSON-RPC request. The value is used to match the request with the response when, for instance, checking the JSON-RPC server logs on the device. Its value is set to current UTC time.</p>"},{"location":"ansible/collection/get/#jsonrpc_version","title":"jsonrpc_version","text":"<p>type: string</p> <p>The <code>jsonrpc_version</code> value is a string that indicates the JSON-RPC version used by the module. The value is always <code>2.0</code> and is not user configurable.</p>"},{"location":"ansible/collection/get/#result","title":"result","text":"<p>type: list of any</p> <p>The <code>result</code> value is a list of returned data. The list order matches the order of the <code>paths</code> parameter. Such that the first element of the <code>result</code> list contains the data retrieved from the first element of the <code>paths</code> list.</p> <p>The type of the data returned depends on the <code>path</code> parameter. For instance, if the <code>path</code> parameter points to a leaf, the value of the leaf is returned. If the <code>path</code> parameter points to a container, the container is returned as a dictionary. See examples to see the data returned for different <code>path</code> parameters.</p>"},{"location":"ansible/collection/get/#error-handling","title":"Error handling","text":"<p>The <code>get</code> module sets the <code>failed</code> return value to <code>true</code> when errors occur during the module execution. The error message is returned in the <code>msg</code> field of the return value.</p> <p>Consider the following output when a wrong path is used by the user of a module:</p> playbookresponse <pre><code>- name: Get wrong path\nhosts: clab\ngather_facts: false\ntasks:\n- name: Get wrong path\nnokia.srlinux.get:\npaths:\n- path: /system/informations\ndatastore: state\n</code></pre> <pre><code>fatal: [clab-ansible-srl]: FAILED! =&gt; {\"changed\": false, \"jsonrpc_req_id\": 11505, \"msg\": \"Path not valid - unknown element 'informations'. Options are [features, trace-options, management, configuration, aaa, authentication, boot, lacp, lldp, mtu, name, dhcp-server, event-handler, mpls, gnmi-server, tls, gribi-server, json-rpc-server, bridge-table, license, dns, ntp, clock, ssh-server, ftp-server, snmp, sflow, load-balancing, banner, information, logging, multicast, network-instance, p4rt-server, maintenance, app-management]\"}\n</code></pre>"},{"location":"ansible/collection/get/#examples","title":"Examples","text":""},{"location":"ansible/collection/get/#single-path","title":"Single path","text":"<p>The most simple example of using the <code>get</code> module is to retrieve a single path which may point to any YANG node of a chosen datastore.</p> <p>Consider the example below, where we retrieve the system information from the state datastore providing a path to a YANG container using the <code>/system/information</code> path.</p> TaskResponse <pre><code>- name: Get /system/information container\nnokia.srlinux.get:\npaths:\n- path: /system/information\ndatastore: state\nregister: get_response\n</code></pre> <pre><code>ok: [clab-ansible-srl] =&gt; {\n\"get_response\": {\n\"changed\": false,\n\"failed\": false,\n\"failed_when_result\": false,\n\"jsonrpc_req_id\": 6062,\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n{\n\"current-datetime\": \"2023-04-26T21:23:10.364Z\",\n\"description\": \"SRLinux-v23.3.1-343-gab924f2e64 7250 IXR-6 Copyright (c) 2000-2020 Nokia. Kernel 5.15.0-1036-azure #43-Ubuntu SMP Wed Mar 29 16:11:05 UTC 2023\",\n\"last-booted\": \"2023-04-26T21:22:29.603Z\",\n\"version\": \"v23.3.1-343-gab924f2e64\"\n}\n]\n}\n}\n</code></pre> <p>As explained in the <code>result</code> parameter documentation, the result value contains a list of objects one per requested path. Since we requested the module to retrieve the value of a single path <code>/system/information</code> which points to a YANG container element, the result list contains only a single element with the object containing the state parameters.</p> <p>Accessing the values of a response object is done using dotted notation. For example, to access the description value of the returned object registered in the <code>get_response</code> variable we can use:</p> <pre><code>get_response.result[0].description\n</code></pre>"},{"location":"ansible/collection/get/#multiple-paths","title":"Multiple paths","text":"<p>The <code>get</code> module allows users to retrieve data from multiple paths, datastores and even YANG models, thus providing great flexibility and efficiency.</p> <p>In the following example we retrieve infromation from three paths and different datastores:</p> TaskResponse <pre><code>  name: Get multiple paths\nnokia.srlinux.get:\npaths:\n- path: /system/information\ndatastore: state\n- path: /system/information/version\ndatastore: state\n- path: /system/json-rpc-server\ndatastore: running\n</code></pre> <pre><code>ok: [clab-ansible-srl] =&gt; {\n\"response\": {\n\"changed\": false,\n\"failed\": false,\n\"jsonrpc_req_id\": 23437,\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n{\n\"current-datetime\": \"2023-04-27T10:56:36.670Z\",\n\"description\": \"SRLinux-v23.3.1-343-gab924f2e64 7250 IXR-6 Copyright (c) 2000-2020 Nokia. Kernel 5.15.0-67-generic #74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023\",\n\"last-booted\": \"2023-04-26T20:14:35.789Z\",\n\"version\": \"v23.3.1-343-gab924f2e64\"\n},\n\"v23.3.1-343-gab924f2e64\",\n{\n\"admin-state\": \"enable\",\n\"network-instance\": [\n{\n\"http\": {\n\"admin-state\": \"enable\"\n},\n\"https\": {\n\"admin-state\": \"enable\",\n\"tls-profile\": \"clab-profile\"\n},\n\"name\": \"mgmt\"\n}\n]\n}\n]\n}\n}\n</code></pre> <p>When requesting multiple paths, the returned <code>result</code> list contains as many elements as many paths have been requested. In this example the three elements form the <code>result</code> list.</p> <p>Note, how the second requested path <code>/system/information/version</code> pointed to a YANG leaf, and therefore the 2<sup>nd</sup> element of the <code>result</code> list is just a string of the requested leaf.</p> <p>The 1<sup>st</sup> and 3<sup>rd</sup> elements are json objects, because the paths pointed to a container element. As in the \"single path\" example, users can access the returned data using the dotted notation.</p> Multiple paths with mixing YANG models <p>It is possible to retrieve data from multiple paths using different YANG models. This is achieved by setting the <code>yang_models</code> parameter on a per-path level.</p> <pre><code>- name: Get multiple paths\nnokia.srlinux.get:\npaths:\n- path: /system/state/hostname\ndatastore: state\nyang_models: oc\n- path: /system/information/description\ndatastore: state\nyang_models: srl\n- path: /system/json-rpc-server\ndatastore: running\nyang_models: srl\n</code></pre>"},{"location":"ansible/collection/get/#openconfig","title":"Openconfig","text":"<p>To retrieve data using Openconfig model leverage the <code>yang_models</code> parameter which is set on a per-path level:</p> Task <p>This task requests <code>/system/state/hostname</code> using Openconfig model and <code>/system/information</code> using SR Linux native datastore. Note, that OC and SR Linux models are mixed in the paths, the SR Linux <code>srl</code> model needs to be set explicitly.</p> <pre><code>- name: Get /system/information container\nnokia.srlinux.get:\npaths:\n- path: /system/state/hostname\nyang_models: oc\ndatastore: state\n- path: /system/information\nyang_models: srl\ndatastore: state\nregister: response\n</code></pre> Response <p>Because Openconfig paths pointed to a YANG leaf, the first element in the <code>result</code> list is a string value of the hostname. The 2<sup>nd</sup> element is an object retrieved using SR Linux YANG model.</p> <pre><code>ok: [clab-ansible-srl] =&gt; {\n\"response\": {\n\"changed\": false,\n\"failed\": false,\n\"failed_when_result\": false,\n\"jsonrpc_req_id\": 32499,\n\"jsonrpc_version\": \"2.0\",\n\"result\": [\n\"srl\",\n{\n\"current-datetime\": \"2023-04-26T21:23:46.376Z\",\n\"description\": \"SRLinux-v23.3.1-343-gab924f2e64 7250 IXR-6 Copyright (c) 2000-2020 Nokia. Kernel 5.15.0-1036-azure #43-Ubuntu SMP Wed Mar 29 16:11:05 UTC 2023\",\n\"last-booted\": \"2023-04-26T21:22:29.603Z\",\n\"version\": \"v23.3.1-343-gab924f2e64\"\n}\n]\n}\n}\n</code></pre>"},{"location":"ansible/collection/get/#source-code","title":"Source code","text":"<p>Get module is implemented in the <code>get.py</code> file.</p>"},{"location":"ansible/collection/validate/","title":"<code>validate</code> Module","text":"<p>The <code>validate</code> module is used to validate intended configuration. The module doesn't make any changes to the target device and just ensures that the intended change set passes validation checks performed by SR Linux.</p> <p>Module semantics is similar to the <code>config</code> module with the <code>update</code>, <code>replace</code> and <code>delete</code> parameters carrying the intended change set.</p> <pre><code>- name: Validate\nhosts: clab\ngather_facts: false\ntasks:\n- name: Validate a valid change set\nnokia.srlinux.validate:\nupdate:\n- path: /system/information\nvalue:\nlocation: Some location\ncontact: Some contact\nregister: response\n- debug:\nvar: response\n- name: Validate an invalid change set\nnokia.srlinux.validate:\nupdate:\n- path: /system/information\nvalue:\nwrong: Some location\ncontact: Some contact\n</code></pre>"},{"location":"blog/media/","title":"SR Linux in the Media","text":"<p>We love to talk about SR Linux and all things around it. Here are some of the podcats, screencasts and demos we did in the past.</p>"},{"location":"blog/media/#media","title":"media","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/subscribe/","title":"Subscribe To This Blog","text":"<p>If you want to get notified when a new post is published on this blog - consider subscribing via one of the following channels.</p>"},{"location":"blog/subscribe/#rss","title":"RSS","text":"<p>This blog publishes two RSS feeds:</p> <ul> <li><code>https://learn.srlinux.dev/feed_rss_created.xml</code> - feed is updated whenever a new post is created.</li> <li><code>https://learn.srlinux.dev/feed_rss_updated.xml</code> - feed is updated whenever a new post is created or updated.</li> </ul> <p>Depending on how thoroughly you want to monitor the blog choose between those two feeds.</p>"},{"location":"blog/subscribe/#email","title":"Email","text":"<p>You can get notifications in your mailbox by channeling the RSS feed using one of the RSS-to-Email services.</p> <p>A popular option is a free blogtrottr service, but if it doesn't suit you, there are alternatives like feedrabbit and IFTT.</p> <p>In blogtrottr, all you need to do is to enter <code>https://learn.srlinux.dev</code> in the site input field, type in your email and select the \"Realtime\" delivery option. Then select which feed you want to receive (see rss section for an explanation).</p>"},{"location":"blog/tags/","title":"Tags","text":""},{"location":"blog/tags/#ansible","title":"ansible","text":"<ul> <li>Ansible Collection For SR Linux</li> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>Official Ansible collection for SR Linux</li> </ul>"},{"location":"blog/tags/#backup","title":"backup","text":"<ul> <li>Configuration backup with Event Handler</li> </ul>"},{"location":"blog/tags/#bgp","title":"bgp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#config-management","title":"config management","text":"<ul> <li>Configuration backup with Event Handler</li> </ul>"},{"location":"blog/tags/#demo","title":"demo","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#elk","title":"elk","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#event-handler","title":"event handler","text":"<ul> <li>Configuration backup with Event Handler</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> <li>Event Handler-based Oper Groups tutorial</li> </ul>"},{"location":"blog/tags/#evpn","title":"evpn","text":"<ul> <li>L2 EVPN with SR Linux</li> </ul>"},{"location":"blog/tags/#fss","title":"fss","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#ixp","title":"ixp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#json-rpc","title":"json-rpc","text":"<ul> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>JSON-RPC Management Interface</li> </ul>"},{"location":"blog/tags/#kne","title":"kne","text":"<ul> <li>SR Linux with KNE</li> </ul>"},{"location":"blog/tags/#ldp","title":"ldp","text":"<ul> <li>LDP-based MPLS with SR Linux</li> </ul>"},{"location":"blog/tags/#logging","title":"logging","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#markdown","title":"markdown","text":"<ul> <li>SR Linux Blog Launch</li> </ul>"},{"location":"blog/tags/#media","title":"media","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#mpls","title":"mpls","text":"<ul> <li>LDP-based MPLS with SR Linux</li> </ul>"},{"location":"blog/tags/#ndk","title":"ndk","text":"<ul> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li>NetOps Development Kit</li> </ul>"},{"location":"blog/tags/#nfd","title":"nfd","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#openbgp","title":"openbgp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#openconfig","title":"openconfig","text":"<ul> <li>SR Linux with KNE</li> </ul>"},{"location":"blog/tags/#packet-pushers","title":"packet pushers","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#podcast","title":"podcast","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#presentation","title":"presentation","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#pygments","title":"pygments","text":"<ul> <li>SR Linux Syntax Highlighting with Pygments</li> </ul>"},{"location":"blog/tags/#sr-linux","title":"sr linux","text":"<ul> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>JSON-RPC Management Interface</li> <li>Basic IXP Lab with OpenBGPd Route Server</li> <li>Official Ansible collection for SR Linux</li> <li>SR Linux logging with ELK</li> <li>SR Linux Syntax Highlighting with Pygments</li> </ul>"},{"location":"blog/tags/#syslog","title":"syslog","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#yang","title":"yang","text":"<ul> <li>SR Linux &amp; YANG</li> </ul>"},{"location":"blog/2022/sr-linux-blog-launch/","title":"SR Linux Blog Launch","text":"<p>Openness, extensibility, innovation and community focus make a large part of the Nokia SR Linux core. Mix it up with our engineering background and you get a resource where we share technical content in the engineers-to-engineers fashion.</p> <p>Today we would like to take it one step further and augment the learn.srlinux.dev portal with a community blog section where Nokia engineers and our community members can post content relevant to modern network technologies.</p> <p>This blog post explains how to contribute a blog article to our portal and what visual candies you can use to make your post look awesome.</p> <p>What should I write about? Or should I even start writing?</p> <p>Likely the hardest thing in writing is to start. You may have dozens of doubts preventing you start writing.</p> <p>Is this interesting to anyone? Isn't it too obvious? Is it too short/long?</p> <p>The best advice here might be just to start writing and reiterate as you go. Nothing is perfect, and we welcome all to embrace the joy of writing, which helps to structure your own thoughts and get a firmer grip on the topic.</p> <p>SR Linux appreciates modern network architectures, network automation/orchestration and programmability. Anything that falls under and in-between these domains will make a great blog post.</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#creating-a-blog-post","title":"Creating a blog post","text":"<p>Did you decide to contribute a blog post? That's great. Here is what you need to do.</p> <ol> <li>Create a file under <code>./docs/blog/posts/&lt;year&gt;/&lt;your-filename&gt;.md</code>. A <code>&lt;year&gt;</code> is in the <code>YYYY</code> format and stands for the year the post was authored.     The filename you choose for your post is completely up to you and doesn't affect a URL or title of the blog post.</li> <li>Write  Use the classic markdown syntax and optionally apply our advanced styling for visual dominance.</li> <li>Add a date to the post metadata.</li> <li>Add yourself as a new author if this is your first contribution.</li> <li>Create a new git branch and commit your post.</li> <li>Check how your article looks using the live web server started with the <code>make serve-insiders</code> target<sup>1</sup>.</li> <li>If all looks great, raise a PR with your work so we can review and merge it.</li> <li>Profit!</li> </ol>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#authors","title":"Authors","text":"<p>We want to give credit to the authors. To make yourself known to the community, please add an entry with your information to the <code>.authors.yml</code> file that lists authors. Once added, you can add yourself to the frontmatter of the blog post:</p> <pre><code>---\nauthors:\n- rdodin #(1)!\n---\n</code></pre> <ol> <li><code>rdodin</code> is a key used in the <code>.authors.yml</code> file for a particular authors. Multiple authors can be added to the list of authors</li> </ol>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#styling","title":"Styling","text":"<p>This portal uses the famous mkdocs-material documentation theme. This theme packs a lot of UX improvements on top of classic markdown syntax. Knowing how to use those additional elements can make your post look awesome both from visual and user experience angles.</p> <p>We would like to highlight a few UI elements we use all the time and hope you'll also like them.</p> <p>Tip</p> <p>Check the mkdocs-material reference for a deep dive in the ocean of options and elements mkdocs-material theme provides.</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#tabs","title":"Tabs","text":"<p>Tabs help to visually organize the content and improve readability an awful lot.</p> Content tabs with code blocks<pre><code>=== \"C\"\n\n    ``` c\n    #include &lt;stdio.h&gt;\n\n    int main(void) {\n      printf(\"Hello world!\\n\");\n      return 0;\n    }\n    ```\n\n=== \"C++\"\n\n    ``` c++\n    #include &lt;iostream&gt;\n\n    int main(void) {\n      std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n      return 0;\n    }\n    ```\n</code></pre> CC++ <pre><code>#include &lt;stdio.h&gt;\nint main(void) {\nprintf(\"Hello world!\\n\");\nreturn 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\nint main(void) {\nstd::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#code","title":"Code","text":"<p>Nowadays, code is everywhere. With a few styling aids you can make your code blocks look shart and expressive.</p> <p>A regular code block with a syntax highlighting uses code fences style:</p> Code block<pre><code>``` py\nimport tensorflow as tf\n```\n</code></pre> <pre><code>import tensorflow as tf\n</code></pre>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#title","title":"Title","text":"<p>To add a title to a code block, use the <code>title</code> attribute:</p> Code block with title<pre><code>``` py title=\"bubble_sort.py\"\ndef bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n```\n</code></pre> bubble_sort.py<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#line-numbers","title":"Line numbers","text":"<p>Line numbers can be added to a code block by using the <code>linenums=\"&lt;start&gt;\"</code> option directly after the shortcode, whereas <code>&lt;start&gt;</code> represents the starting line number. A code block can start from a line number other than <code>1</code>, which allows to split large code blocks for readability:</p> Code block with line numbers<pre><code>``` py linenums=\"1\"\ndef bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n```\n</code></pre> <pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#highlighting-specific-lines","title":"Highlighting specific lines","text":"<p>Specific lines can be highlighted by passing the line numbers to the <code>hl_lines</code> argument placed right after the language shortcode. Note that line counts start at <code>1</code>, regardless of the starting line number specified:</p> Code block with highlighted lines<pre><code>``` py hl_lines=\"2 3\"\ndef bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n```\n</code></pre> <pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#annotations","title":"Annotations","text":"<p>Code annotations can be placed anywhere in a code block where a comment for the language of the block can be placed, e.g. for JavaScript in <code>// ...</code> and <code>/* ... */</code>, for YAML in <code># ...</code>, etc.:</p> Code block with annotation<pre><code>``` yaml\ntheme:\n  features:\n    - content.code.annotate # (1)!\n```\n\n1.  :man_raising_hand: I'm a code annotation! I can contain `code`, __formatted\n    text__, images, ... basically anything that can be written in Markdown.\n</code></pre> <pre><code>theme:\nfeatures:\n- content.code.annotate # (1)!\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#admonitions","title":"Admonitions","text":"<p>Admonitions is a great way to emphasize a piece of information. Check the original documentation for all the customizations available for admonitions.</p> Admonition<pre><code>!!! note\n\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n</code></pre> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#icons","title":"Icons","text":"<p>Our doc theme includes a gazillion of icons and emojis which are super easy to use. Use the search tool to find the icon code-block and paste it in your post.</p> Emoji<pre><code>:smile: \n</code></pre> <p></p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#footnotes","title":"Footnotes","text":"<p>Footnotes are a great way to add supplemental or additional information to a specific word, phrase or sentence without interrupting the flow of a document. Material for MkDocs provides the ability to define, reference and render footnotes.</p> <p>A footnote reference must be enclosed in square brackets and must start with a caret <code>^</code>, directly followed by an arbitrary identifier, which is similar to the standard Markdown link syntax.</p> Text with footnote references<pre><code>Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2]\n</code></pre> <p>Lorem ipsum<sup>1</sup> dolor sit amet, consectetur adipiscing elit.<sup>2</sup></p> <p>The footnote content must be declared with the same identifier as the reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink to the footnote reference is automatically added.</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#on-a-single-line","title":"on a single line","text":"<p>Short footnotes can be written on the same line:</p> Footnote<pre><code>[^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n</code></pre> <p> Jump to footnote</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#on-multiple-lines","title":"on multiple lines","text":"<p>Paragraphs can be written on the next line and must be indented by four spaces:</p> Footnote<pre><code>[^2]:\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n</code></pre> <p> Jump to footnote</p>","tags":["markdown"]},{"location":"blog/2022/sr-linux-blog-launch/#subscribing","title":"Subscribing","text":"<p>Get notified when a new post is published using one of the subscription options offered.</p> <ol> <li> <p>Our community members who don't have access to the mkdocs-material-insiders version will have to skip this step until the blog feature becomes available in the community version of the mkdocs-material project.\u00a0\u21a9\u21a9</p> </li> <li> <p>Example footnote\u00a0\u21a9</p> </li> </ol>","tags":["markdown"]},{"location":"blog/2022/configuration-backup-with-event-handler/","title":"Configuration backup with Event Handler","text":"<p>The year is 2023. You wake up to a subtle 'ping' from your phone \ud83d\udcf1, signifying there is a message from someone important. Still half asleep you reach out and grab it, glancing at the screen. It says: \"The network went out - you're fired!\"</p> <p>Rollback to the present day, where I can confidentially share that this is exactly the kind of scenario that our engineers had in mind when they designed the new Event Handler feature for SR Linux. Because you can never have enough flexibility to add just the right amount of automation, configuring things properly and - critically - keeping track of changes to the configuration (and whichever #!$!! person made them)</p> <p> \"Backup config\" event handler instance config</p> <p>To make things more practical, take a look at this Python script which uses the Event Handler mechanism to scp a backup of the config to any destination of your choice, whenever something or someone commits a change.</p> backup_config.py<pre><code>import json, time\n# main entry function for event handler\ndef event_handler_main(in_json_str):\n# parse input json string passed by event handler\nin_json = json.loads(in_json_str)\npaths = in_json[\"paths\"]\noptions = in_json[\"options\"]\ndebug = options.get(\"debug\") == \"true\"\nif debug:\nprint( in_json_str )\ntarget = options.get(\"target\", None)\nif target:\ntimestamp = None\nfor p in paths:\nif p['path']==\"system configuration last-change\":\ntimestamp = p['value']\nbreak\n# elif p['path'] starts with \"system aaa authentication session\" ...\nif not timestamp:\nt = time.gmtime() # in UTC\ntimestamp = '{:04d}-{:02d}-{:02d}_{:02d}:{:02d}:{:02d}_UTC'.format(t[0], t[1], t[2], t[3], t[4], t[5])\nresponse = { \"actions\": [\n{ \"run-script\": {\n\"cmdline\": f\"ip netns exec srbase-mgmt /usr/bin/scp /etc/opt/srlinux/config.json {target}/config-{timestamp}.json\"\n}\n}\n] }\nreturn json.dumps(response)\nprint( \"Error: no 'target' defined\" )\nreturn { \"actions\": [] }\n</code></pre> <p>The script should be fairly self-explanatory: It gets the <code>target</code> from the configuration and the timestamp of the change, and then invokes a standard Linux <code>scp</code> command (making sure it runs in the correct network namespace). Although it does not currently do anything with the username, those skilled in the art will appreciate that this could easily be added.</p> <p>Question</p> <p>Anyone stopped here for a moment thinking \"Why not use Git?\". Valid question, and a reasonable enhancement to the backup function presented here.</p> <p>The reason plain <code>scp</code> has been used in this example is because <code>scp</code> is shipped with the linux subsystem of SR Linux, and <code>git</code> is not. When <code>git</code> becomes available on SR Linux, we may update this example with a <code>git</code>-friendly backup option.</p> <p>The above is just a quick starting point of course - you may want to make it more elaborate and (for example) have the system send you a text for approval, with automatic rollback in case you don't approve within a certain amount of time (configurable). Or maybe you're thinking to add some Blockchain logic there, creating indisputable proof that things happened the way you say they did. Go for it!<sup>1</sup></p> <p>My point is simple: You need a truly open system. A platform that allows you to configure and automate things the way you like to do them, unrestricted by vendor imposed limitations or poor product design choices. Because if not, one day you may find yourself waking up to that 'ping'. Root cause? You didn't pick that truly open network platform when you had the chance...</p> <p>Disclaimer</p> <p>Events and people referenced in this story are fictional. Any resemblance to existing persons or events is completely accidental. And we both know this would never happen to you, right? Because you always make the right choices </p> <ol> <li> <p>Micro Python backend that powers the Event Handler framework doesn't allow you to install extra packages. But you can use external APIs via standard lib HTTP client and integrate with them to build advanced pipelines. Another option is to leverage the <code>run-script</code> action and call an external binary/script that can leverage external dependencies.\u00a0\u21a9</p> </li> </ol>","tags":["event handler","config management","backup"]},{"location":"blog/2022/using-ansible-with-sr-linuxs-json-rpc-interface/","title":"Using Ansible with SR Linux's JSON-RPC Interface","text":"<p>A few days after we fleshed out our  JSON-RPC Basics tutorial, and we are releasing another one. While basics tutorial is essential to read to understand how the interface works, the <code>curl</code> utility we used in the examples there is not something you would like to automate your network with.</p> <p>Quite a lot of network ops teams we've been talking to used Ansible to manage their infra, and they wanted to keep using it for network automation as well. While this is a questionable tactic, we still can give you the \"fishing rod\".</p> <p>Please welcome -  Using Ansible with SR Linux's JSON-RPC Interface tutorial, which puts our JSON-RPC interface to work under Ansible command through a set of task-oriented exercises.</p>","tags":["sr linux","json-rpc","ansible"]},{"location":"blog/2022/json-rpc-management-interface/","title":"JSON-RPC Management Interface","text":"<p>Nokia SR Linux Network OS architecture has been built on strong principles of model-driven APIs and interfaces. Not a single thing in SR Linux datastores can get away without having a matching YANG module describing it.</p> <p>The ground-up model-driven approach allowed us to build management interfaces that don't have shortness of sight as every interface, in essence, uses the common API layer presented by the management server. One of such interfaces - JSON-RPC - that SR Linux offers has been in the shade of a cool-kid gNMI, though JSON-RPC has lots to offer.</p> <p>We are glad to present you with an in-depth tutorial on SR Linux's JSON-RPC interface -  JSON-RPC Basics.</p> <p>In this tutorial, we explain the JSON-RPC capabilities and provide practical examples for every method this interface offers. Be it retrieval of state, model-driven configuration using JSON, or pushing CLI-styled commands - JSON-RPC has you covered.</p>","tags":["sr linux","json-rpc"]},{"location":"blog/2022/material-youtube-devops-approaches-for-enhanced-netops-with-nokia-data-center-fabric-solution/","title":"DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution","text":"<p> NFD 29</p> <p>This presentation provides a quick review and update of the Nokia Data Center Fabric solution, and a review of how DevOps approaches may be adapted for data center fabric NetOps. This is followed by a demo featuring how Nokia SR Linux supports network application warm restart.</p> <p>Participants:  Bruce Wallis</p>","tags":["media","nfd","demo","presentation"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/","title":"Basic IXP Lab with OpenBGPd Route Server","text":"<p>Almost every Internet eXchange Point (IXP) leverages a Router Server (RS) to simplify peering between members of the exchange who exercise an open policy peering. A Route Server is a software component connected to the IXP network which acts as a BGP speaker with whom members peer to receive BGP updates from each other.</p> <p>Nowadays, IXPs predominantly use BIRD routing daemon as a Route Server, but for diversity and sustainability reasons Route Server Support Foundation initiated a program to introduce other software solutions, like OpenBGPd, to the IXP market.</p> <p>While OpenBGPd is not a new kid on the block of software BGP implementations, it is less known in the IXP domain (compared to BIRD). Lots of IXPs are interested in introducing OpenBGPd as a second Route Server in their networks and this lab opens the doors to explore \"OpenBGPd as a Route Server\" use case.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#lab-summary","title":"Lab summary","text":"<p>This blog posts is based on a lab example that builds a simple IXP network with a route server and two IXP members.</p> Summary Lab name Basic IXP Lab with OpenBGPd Route Server Lab components Nokia SR Linux, Arista cEOS and OpenBGPd nodes Resource requirements  2 vCPU  6 GB Lab hellt/obgpd-lab Version information <code>containerlab:0.32.4</code>, <code>srlinux:22.6.4</code>, <code>cEOS:4.28.0F</code>, <code>openbgpd:7.7</code> Authors Roman Dodin","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#prerequisites","title":"Prerequisites","text":"<p>The lab leverages the Containerlab project to spin up a topology of network elements and couple it with containerized software such as openbgpd. A one-click installation gets containerlab installed on any Linux system.</p> Containerlab installation via installation-script<pre><code>bash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre> <p>Since containerlab uses containers as the nodes of a lab, Docker engine has to be installed on the host system.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#lab-topology","title":"Lab topology","text":"<p>The goal of this lab is to give users a hands-on experience with OpenBGPd by providing a lab that mimics a trivialized IXP setup with two members exchanging their routers via a Route Server.</p> <p></p> <p>The setup consists of two routers - Nokia SR Linux and Arista EOS - acting as members connected to a common IXP LAN where a Route Server (OpenBGPd) is present.</p> <p>Members of the exchange establish the BGP peering sessions with a Route Server over a common LAN segment and announce their <code>/32</code> networks.</p> <p></p> <p>The OpenBGPd-based Route Server is configured to announce the routes it receives to connected peers and thus enables a core Route Server functionality.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#obtaining-container-images","title":"Obtaining container images","text":"<p>This lab features three different products:</p> <ul> <li>Nokia SR Linux - IXP member role</li> <li>Arista EOS - IXP member role</li> <li>OpenBGPd - Route Server role</li> </ul> <p>For containerlab to be able to start up the lab, the relevant container images need to be available.</p> Image How to get it? Nokia SR Linux Nokia SR Linux container image is freely available for anyone to use and can be pulled as easy as <code>docker pull ghcr.io/nokia/srlinux:22.6.4</code> Arista EOS Arista EOS containerized version is called cEOS and can be obtained by registering on Arista website and downloading an archive with container image. Follow the instructions provided on containerlab website to get it. OpenBGPd OpenBGPd has a publicly available container pushed to a registry. Pull it with <code>docker pull quay.io/openbgpd/openbgpd:7.7</code>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#containerlab-toplogy-file","title":"Containerlab toplogy file","text":"<p>Courtesy of containerlab, the whole lab topology is captured in a declarative fashion via the <code>obgpd.clab.yml</code> file.</p> <p>Let's cover the key components of the topology.</p> <p>Tip</p> <p>Consult with containerlab documentation to learn more about containerlab topology syntax.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#defining-members-of-the-ixp","title":"Defining members of the IXP","text":"<p>Let's start first by looking at the way we define the members of the IXP:</p> Defining SR Linux and cEOS nodes<pre><code>topology:\nnodes:\n# -- snip --\nsrlinux: #(1)!\nkind: nokia_srlinux\nimage: ghcr.io/nokia/srlinux:22.6.4\nstartup-config: srlinux.cfg\nceos:\nkind: arista_ceos\nimage: ceos:4.28.0F #(2)!\nstartup-config: ceos.cfg\n</code></pre> <ol> <li>This is a node <code>name</code>. It can be any string. For simplicity, we name Nokia SR Linux node <code>srlinux</code>, and Arista EOS node as <code>ceos</code>.</li> <li>Make sure that you download and import cEOS image before starting the lab.</li> </ol> <p>Our two IXP members named in the topology file <code>srlinux</code> and <code>ceos</code> accordingly are defined using the respective kinds <code>nokia_srlinux</code> and <code>arista_ceos</code>. Kinds make containerlab aware of particularities each Network OS has in terms of boot procedures and supported features.</p> <p>To make our member nodes come up with their interfaces and protocols configured as per the lab topology design, we utilize containerlab's startup-config feature. It allows us to provide a configuration file for both nodes that gets applied right after the nodes finish booting.</p> <p>Note</p> <p>The startup configuration files for Nokia SR Linux and Arista EOS contain CLI commands that move the system to a state where BGP peering with the Route Server is fully configured.</p> <p>If you want to configure the systems from the ground up, remove the <code>startup-config</code> block from the topology file and the nodes will boot with the default configuration.</p> <p>Now to the Route Server. Containerlab allows users to combine regular containers with Network OS nodes. These regular containers are identified with the <code>linux</code> kind in the topology, and this is exactly how we define the OpenBGPd node:</p> Defining a Route Server node<pre><code>topology:\nnodes:\n# -- snip --\nopenbgpd:\nkind: linux\nimage: quay.io/openbgpd/openbgpd:7.7 #(1)!\nbinds:\n- openbgpd.conf:/etc/bgpd/bgpd.conf\nexec:\n- \"ip address add dev eth1 192.168.0.3/24\"\n</code></pre> <ol> <li>OpenBGPd team maintains a public container image which we can use right away in the topology file.</li> </ol> <p>To provide OpenBGPd with a configuration file we leverage the <code>binds</code> property that works exactly like bind mount in Docker. You specify a source file path<sup>1</sup> and the corresponding path inside the container process. For <code>openbgpd</code> node we take the <code>openbgpd.conf</code> file and mount it inside the container by the <code>/etc/bgpd/bgpd.conf</code> path. This will make OpenBGPd to read this config when the process starts.</p> <p>One last step left for the <code>openbgpd</code> node: to configure the <code>eth1</code> interface that connects the Route Server to the IXP LAN. Links setup is covered in details in the next chapter, but, for now, just keep in mind that we can configure interfaces of a container using the <code>exec</code> option and <code>ip</code> utility.</p> <p>This is what we have defined so far:</p> <p></p> Logical view of the topology with IXP members defined <p>We have one last piece missing, and that is an IXP LAN network to which all our members should be connected.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#ixp-lan","title":"IXP LAN","text":"<p>To keep things simple, in this lab the IXP's underlay network is just an abstract L2 network as we don't want to get into the weeds of IXP network implementation<sup>2</sup>. And what provides the most trivial Layer 2 segment on a Linux system? Yes, a Linux bridge. If we have a bridge, we can connect our members and a Route Server to it, thus providing the needed L2 connectivity between all parties.</p> <p>First, let's create a bridge interface named <code>ixp</code> on a Linux host:</p> <pre><code>ip link add name ixp type bridge &amp;&amp; ip link set dev ixp up\n</code></pre> <p>In containerlab, a special node of <code>kind: bridge</code> must be part of a topology so that other elements of the lab can be connected to it.</p> Defining a bridge in the topo file<pre><code>topology:\nnodes:\n# --snip--\nixp:\nkind: bridge\n</code></pre> <p>Note</p> <p>Node name for the bridge needs to match the name of the bridge interface you created on your host. It is <code>ixp</code> in our case.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#adding-links","title":"Adding links","text":"<p>And we get to the final part which is defining the links between the nodes of our lab.</p> <p>Following the topology design, our task is to connect every node of our lab to the IXP LAN, which is a bridge network we created a moment ago. To achieve that, we create the <code>links</code> section in our topology file where we wire-up all the elements together:</p> Defining links<pre><code>topology:\n# --snip--\nlinks:\n- endpoints: [\"srlinux:e1-1\", \"ixp:srl1\"]\n- endpoints: [\"ceos:eth1\", \"ixp:ceos1\"]\n- endpoints: [\"openbgpd:eth1\", \"ixp:obgp1\"]\n</code></pre> <p>This block instructs containerlab to create veth paris between the defined endpoints. For example, <code>endpoints: [\"srlinux:e1-1\", \"ixp:srl1\"]</code> tells containerlab to create a veth pair between the nodes <code>srlinux</code> and <code>ixp</code>, where <code>ixp</code> is a Linux bridge. Containerlab will place the veth pair into the relevant namespaces and will name veth interface endpoints according to the names provided by the user in this string array.</p> A wire between srlinux and IXP bridge<pre><code>       srlinux                      ixp bridge\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502               \u2502             \u2502               \u2502\n  \u2502               \u2502             \u2502               \u2502\n  \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2524             \u251c\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n  \u2502        \u2502 e1-1\u2502\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2502 srl1  \u2502      \u2502\n  \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2524             \u251c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n  \u2502               \u2502             \u2502               \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <p>The interface name specified for the bridge-side is placed in the hosts network namespace and has no special meaning.</p> <p>Read more on how links are modelled if you want to know all the details or get an additional explanation.</p> <p>With all the links defined, our lab logical view becomes complete!</p> <p></p> Logical view of the topology with IXP members and links defined","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#lab-deployment","title":"Lab deployment","text":"<p>At this point everything is ready for the lab to be deployed:</p> <ol> <li>Container images are pulled</li> <li>Linux bridge named <code>ixp</code> is created</li> </ol> <p>First, clone the lab:</p> <pre><code>git clone https://github.com/hellt/openbgpd-lab.git &amp;&amp; cd openbgpd-lab\n</code></pre> <p>And deploy!</p> <pre><code>containerlab deploy\n</code></pre> <p>At the end of the deployment process, which should take around 30 seconds, you will be greeted by the summary table with details about deployed nodes:</p> <pre><code>INFO[0000] Containerlab v0.32.4 started\n--snip--\nINFO[0036] Adding containerlab host entries to /etc/hosts file \n+---+---------------------+--------------+-------------------------------+-------------+---------+----------------+----------------------+\n| # |        Name         | Container ID |             Image             |    Kind     |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+---------------------+--------------+-------------------------------+-------------+---------+----------------+----------------------+\n| 1 | clab-obgpd-ceos     | b102810a4e9a | ceos:4.28.0F                  | arista_ceos | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n| 2 | clab-obgpd-openbgpd | ad3724a00562 | quay.io/openbgpd/openbgpd:7.7 | linux       | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 3 | clab-obgpd-srlinux  | 2ece012bc12e | ghcr.io/nokia/srlinux:22.6.4  | srl         | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n+---+---------------------+--------------+-------------------------------+-------------+---------+----------------+----------------------+\n</code></pre> <p>Pro tip</p> <p>Use <code>clab</code> instead of <code>containerlab</code> to save on typing.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#connecting-to-the-nodes","title":"Connecting to the nodes","text":"<p>When the lab is deployed, you can connect to every node and configure and verify the lab' status and protocols operation. For nodes that run SSH server, you can connect with SSH client using the names provided in the summary table:</p> <pre><code>ssh admin@clab-obgpd-srlinux #(1)!\n</code></pre> <ol> <li>Default credentials for both SR Linux and cEOS are <code>admin:admin</code>     You can use containerlab-assigned IP addresses from the summary table as well.</li> </ol> What makes a node name? <p>Each node name by default consists of three parts:</p> <ol> <li>fixed <code>clab</code> prefix</li> <li>lab name as set in the topology file; <code>obgpd</code> in our case</li> <li>node name as set in the topology file; <code>srlinux</code> as per the example above.</li> </ol> <p>To connect to the OpenBGPd container that doesn't have SSH server use <code>docker</code> CLI and execute a shell process:</p> <pre><code>\u276f docker exec -it clab-obgpd-openbgpd ash //(1)!\n/ # bgpctl // (2)!\nmissing argument:\nvalid commands/args:\nreload\nshow\nfib\nneighbor\nnetwork\nlog\n</code></pre> <ol> <li><code>ash</code> is the alpine shell used in openbgpd container image</li> <li><code>bgpctl</code> is a CLI interface to interact with OpenBGPd</li> </ol>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#configuration","title":"Configuration","text":"<p>With the lab deployed and connection methods sorted out, we can move on to the meat of it - checking the basic configuration we embedded into this lab.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#openbgpd","title":"OpenBGPd","text":"<p>OpenBGPd config is provided in a <code>openbgpd.conf</code> file that we bind mounted to the container. Let's have a quick look at what this config has inside:</p> <pre><code># example config for a test lab, DO NOT USE FOR PRODUCTION\n# global configuration\nAS 65003\nrouter-id 10.0.0.3\n\n# do not add our own AS (a route server behavior) in ASPATH\ntransparent-as yes\n\ngroup \"route-server-clients\" {\n# IPv4 Peering LAN\nneighbor 192.168.0.0/24\n}\n# in a lab we can allow ourselves to not do any filtering\nallow to ebgp\nallow from ebgp\n\n# set's these communities to identify from where RS learned a route\nmatch from any set large-community local-as:0:neighbor-as\n</code></pre> <p>As the comments indicate, this is a trivialized configuration that automatically peers with any BGP speaker available in the <code>192.168.0.0/24</code> network. There is no filtering configured, and every received BGP route is sent to all peers. When <code>openbgpd</code> container starts, it reads this file automatically, since we mounted it by the well-known path. As a result of that, the peerings will be automatically set up with our two members running SR Linux and cEOS.</p> <p>Do you remember that <code>exec</code> statement, that we have in our topology file?</p> <pre><code>topology:\nnodes:\n# -- snip --\nopenbgpd:\n# -- snip --\nexec:\n- \"ip address add dev eth1 192.168.0.3/24\"\n# --snip--\nlinks:\n# --snip--\n- endpoints: [\"openbgpd:eth1\", \"ixp:obgp1\"]\n</code></pre> <p>Within the <code>exec</code> block, we configured the IP address on the <code>eth1</code> interface of the <code>openbgpd</code> node, as we added that link connecting the route server to the IXP bridge.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#nokia-sr-linux","title":"Nokia SR Linux","text":"<p>Similarly, a startup configuration file has been provided for SR Linux node.</p> Interfaces configurationRouting policyNetwork instance and BGP <p>First we configure the interfaces. Interface <code>ethernet-1/1</code> connects our router to the IXP bridge, and therefore is addressed with the <code>192.168.0.1</code> address. We also add the loopback <code>lo0</code> interface that is addressed <code>10.0.0.1/32</code> and simulates the network this router will announce towards the route server. <pre><code>interface ethernet-1/1 {\nadmin-state enable\nsubinterface 0 {\nadmin-state enable\nipv4 {\naddress 192.168.0.1/24 {\n}\n}\n}\n}\ninterface lo0 {\nadmin-state enable\nsubinterface 0 {\nadmin-state enable\nipv4 {\naddress 10.0.0.1/32 {\n}\n}\n}\n}\n</code></pre></p> <p>A routing policy is required to fine tune which routes can be imported/exported by SR Linux. In this policy we leverage the <code>prefix-set</code> that includes our loopback address. <pre><code>routing-policy {\nprefix-set loopback {\nprefix 10.0.0.0/24 mask-length-range 24..32 {\n}\n}\npolicy loopbacks {\ndefault-action {\nreject {\n}\n}\nstatement 10 {\nmatch {\nprefix-set loopback\n            }\naction {\naccept {\n}\n}\n}\n}\n}\n</code></pre></p> <p>Lastly, we attach configured interfaces to the default network instance and configure BGP peering with the route server. <pre><code>network-instance default {\ninterface ethernet-1/1.0 {\n}\ninterface lo0.0 {\n}\nprotocols {\nbgp {\nadmin-state enable\nautonomous-system 65001\nrouter-id 10.0.0.1\n            group rs {\nexport-policy loopbacks\n                import-policy loopbacks\n                peer-as 65003\nipv4-unicast {\nadmin-state enable\n}\ntimers {\nconnect-retry 1\nhold-time 9\nkeepalive-interval 3\nminimum-advertisement-interval 1\n}\n}\nneighbor 192.168.0.3 {\npeer-group rs\n            }\n}\n}\n}\n</code></pre></p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#arista-eos","title":"Arista EOS","text":"<p>Arista EOS is configured in the same spirit, and its configuration is contained within <code>ceos.cfg</code> file.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#verification","title":"Verification","text":"<p>Because we used startup configuration files for all of the components of our lab, the peerings will automatically set up once the nodes finish their boot procedures.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#openbgpd_1","title":"OpenBGPd","text":"<p>On <code>openbgpd</code> side we can monitor the logs of the daemon right after we enter the <code>clab deploy</code> command:</p> <pre><code>docker logs -f clab-obgpd-openbgpd\n</code></pre> <p>Watching the log file will show to us when the <code>openbgpd</code> starts to receives messages from the peers reaching out to it:</p> <pre><code>--snip--\nRTR engine reconfigured\nRDE reconfigured\nrunning softreconfig in\nsoftreconfig in done\nRDE soft reconfiguration done\nneighbor 192.168.0.2: state change None -&gt; Idle, reason: None\nneighbor 192.168.0.2: state change Idle -&gt; Connect, reason: Start\nneighbor 192.168.0.2: state change Connect -&gt; OpenSent, reason: Connection opened\nneighbor 192.168.0.2: state change OpenSent -&gt; OpenConfirm, reason: OPEN message received\nneighbor 192.168.0.2: state change OpenConfirm -&gt; Established, reason: KEEPALIVE message received\nneighbor 192.168.0.2: sending IPv4 unicast EOR marker\nneighbor 192.168.0.2: received IPv4 unicast EOR marker\nnexthop 192.168.0.2 now valid: directly connected: via 192.168.0.2\nnexthop 192.168.0.2 update starting\nnexthop 192.168.0.2 update finished\nneighbor 192.168.0.1: state change None -&gt; Idle, reason: None\nneighbor 192.168.0.1: state change Idle -&gt; Connect, reason: Start\nneighbor 192.168.0.1: state change Connect -&gt; OpenSent, reason: Connection opened\nneighbor 192.168.0.1: state change OpenSent -&gt; OpenConfirm, reason: OPEN message received\nneighbor 192.168.0.1: state change OpenConfirm -&gt; Established, reason: KEEPALIVE message received\nneighbor 192.168.0.1: sending IPv4 unicast EOR marker\nneighbor 192.168.0.1: received IPv4 unicast EOR marker\nnexthop 192.168.0.1 now valid: directly connected: via 192.168.0.1\nnexthop 192.168.0.1 update starting\nnexthop 192.168.0.1 update finished\n</code></pre> <p>As the log shows, <code>openbgpd</code> established two BGP sessions with our two IXP members and exchanged records. We can have a deeper look at the RIB and neighbor status<sup>3</sup> by connecting to this node and using <code>bgpctl</code> CLI:</p> RIB INRIBRIB OUTNeighbor status <p>When checking the BGP peerings it is useful to peek into the BGP RIB IN database to see which routes were received by the <code>openbgpd</code>. <pre><code>/ # bgpctl show rib in detail\n\nBGP routing table entry for 10.0.0.1/32\n    65001\n    Nexthop 192.168.0.1 (via 192.168.0.1) Neighbor 192.168.0.1 (10.0.0.1)\n    Origin IGP, metric 0, localpref 100, weight 0, ovs not-found, external\n    Last update: 00:04:31 ago\n\nBGP routing table entry for 10.0.0.2/32\n    65002\n    Nexthop 192.168.0.2 (via 192.168.0.2) Neighbor 192.168.0.2 (10.0.0.2)\n    Origin IGP, metric 0, localpref 100, weight 0, ovs not-found, external\n    Last update: 00:04:48 ago\n</code></pre> All checks out here, we got a loobpack from each of our IXP members.</p> <p>We can have a look at the OpenBGPd's RIB table then to see which routes were accepted: <pre><code>/ # bgpctl show rib\nflags: * = Valid, &gt; = Selected, I = via IBGP, A = Announced,\n      S = Stale, E = Error\norigin validation state: N = not-found, V = valid, ! = invalid\norigin: i = IGP, e = EGP, ? = Incomplete\n\nflags ovs destination          gateway          lpref   med aspath origin\n*&gt;      N 10.0.0.1/32          192.168.0.1       100     0 65001 i\n*&gt;      N 10.0.0.2/32          192.168.0.2       100     0 65002 i\n</code></pre> To our luck both routes were selected and used in the RIB.</p> <p>Finally, we can make sure that the routes from our local RIB are sent out to the memebers of our exchange: <pre><code>/ # bgpctl show rib out\nflags: * = Valid, &gt; = Selected, I = via IBGP, A = Announced,\n      S = Stale, E = Error\norigin validation state: N = not-found, V = valid, ! = invalid\norigin: i = IGP, e = EGP, ? = Incomplete\n\nflags ovs destination          gateway          lpref   med aspath origin\n*       N 10.0.0.2/32          192.168.0.2       100     0 65002 i\n*       N 10.0.0.1/32          192.168.0.1       100     0 65001 i\n</code></pre> Nice, the routes were selected to be in RIB OUT database, and therefore they will be sent out to the respective peers.</p> <p>To check the status of the BGP neighbors use <code>bgpctl show neighbor</code> command. The output is quite lengthy, so we won't paste it here.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#sr-linux","title":"SR Linux","text":"<p>On the SR Linux side we have an extensive list of state information related to BGP. Connect to the node using <code>ssh admin@clab-obgpd-srlinux</code> and try out these commands.</p> Neighbor statusReceived BGP routesAdvertised routesBGP RIBDetailed route information <pre><code>--{ running }--[  ]--\nA:srlinux# show network-instance default protocols bgp neighbor  \n-----------------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n-----------------------------------------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------------------------------------\n+-------------+-------------------+-------------+-----+-------+-----------+-----------+----------+-------------------+\n|  Net-Inst   |       Peer        |    Group    | Fla | Peer- |   State   |  Uptime   | AFI/SAFI |  [Rx/Active/Tx]   |\n|             |                   |             | gs  |  AS   |           |           |          |                   |\n+=============+===================+=============+=====+=======+===========+===========+==========+===================+\n| default     | 192.168.0.3       | rs          | S   | 65003 | establish | 0d:0h:14m | ipv4-uni | [1/1/1]           |\n|             |                   |             |     |       | ed        | :1s       | cast     |                   |\n+-------------+-------------------+-------------+-----+-------+-----------+-----------+----------+-------------------+\n-----------------------------------------------------------------------------------------------------------------------\nSummary:\n1 configured neighbors, 1 configured sessions are established,0 disabled peers\n0 dynamic peers\n</code></pre> <p>To list the routes received from a given BGP peer: <pre><code>--{ running }--[  ]--\nA:srlinux# show network-instance default protocols bgp neighbor 192.168.0.3 received-routes ipv4\n-----------------------------------------------------------------------------------------------------------------------\nPeer        : 192.168.0.3, remote AS: 65003, local AS: 65001\nType        : static\nDescription : None\nGroup       : rs\n-----------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-------------------------------------------------------------------------------------------------------------------+\n| Stat       Network          Next Hop          MED        LocPref                 AsPath                  Origin   |\n|  us                                                                                                               |\n+===================================================================================================================+\n| u*&gt;    10.0.0.2/32       192.168.0.2           -           100       [65002]                                i     |\n+-------------------------------------------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------------------------------------\n1 received BGP routes : 1 used 1 valid\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre></p> <p>Equally important to see which routes were qualified to be sent out: <pre><code>--{ running }--[  ]--\nA:srlinux# show network-instance default protocols bgp neighbor 192.168.0.3 advertised-routes ipv4\n-----------------------------------------------------------------------------------------------------------------------\nPeer        : 192.168.0.3, remote AS: 65003, local AS: 65001\nType        : static\nDescription : None\nGroup       : rs\n-----------------------------------------------------------------------------------------------------------------------\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-------------------------------------------------------------------------------------------------------------------+\n|        Network             Next Hop          MED        LocPref                  AsPath                  Origin   |\n+===================================================================================================================+\n| 10.0.0.1/32             192.168.0.1           -           100       [65001]                                 i     |\n+-------------------------------------------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------------------------------------\n1 advertised BGP routes\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre></p> <p>We can check which routes were populated in the BGP RIB. There we can see our route from the other IXP member \"reflected\" by the route server to us. <pre><code>--{ running }--[  ]--\nA:srlinux# show network-instance default protocols bgp routes ipv4 summary  \n-----------------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n-----------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n-----------------------------------------------------------------------------------------------------------------------\n+-----+-------------+-------------------+-----+-----+-------------------------------------+\n| Sta |   Network   |     Next Hop      | MED | Loc |              Path Val               |\n| tus |             |                   |     | Pre |                                     |\n|     |             |                   |     |  f  |                                     |\n+=====+=============+===================+=====+=====+=====================================+\n| u*&gt; | 10.0.0.1/32 | 0.0.0.0           | -   | 100 |  i                                  |\n| u*&gt; | 10.0.0.2/32 | 192.168.0.2       | -   | 100 | [65002] i                           |\n| u*&gt; | 192.168.0.0 | 0.0.0.0           | -   | 100 |  i                                  |\n|     | /24         |                   |     |     |                                     |\n+-----+-------------+-------------------+-----+-----+-------------------------------------+\n-----------------------------------------------------------------------------------------------------------------------\n3 received BGP routes: 3 used, 3 valid, 0 stale\n3 available destinations: 0 with ECMP multipaths\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre></p> <p>When in need to look which communities were attached to the route, use a zoomed view on a received BGP prefix.</p> <pre><code>--{ running }--[  ]--\nA:srlinux# show network-instance default protocols bgp routes ipv4 prefix 10.0.0.2/32 \n-----------------------------------------------------------------------------------------------------------------------\nShow report for the BGP routes to network \"10.0.0.2/32\" network-instance  \"default\"\n-----------------------------------------------------------------------------------------------------------------------\nNetwork: 10.0.0.2/32\nReceived Paths: 1\n  Path 1: &lt;Best,Valid,Used,&gt;\n    Route source    : neighbor 192.168.0.3\n    Route Preference: MED is -, LocalPref is 100\n    BGP next-hop    : 192.168.0.2\n    Path            :  i [65002]\n    Communities     : 65003:0:65002\nPath 1 was advertised to: \n[  ]\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>There are many more commands that can be of use, feel free to explore them or ask in the comments.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#arista-eos_1","title":"Arista EOS","text":"<p>On EOS side we can verify that we have the prefix from SR Linux received in good order:</p> <pre><code>ceos#sh ip bgp summ\nBGP summary information for VRF default\nRouter identifier 10.0.0.2, local AS number 65002\nNeighbor Status Codes: m - Under maintenance\n  Neighbor    V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  192.168.0.3 4 65003             64        78    0    0 00:30:38 Estab   0      0\n</code></pre>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#datapath","title":"Datapath","text":"<p>It will all be for nothing if the exchanged networks weren't able to talk one to another. We can check this by issuing a ping from either IXP member, targeting the advertised network. Let's do this on SR Linux:</p> <pre><code>--{ running }--[  ]--\nA:srlinux# ping network-instance default 10.0.0.2 -I 192.168.0.1 #(1)!\nUsing network instance default\nPING 10.0.0.2 (10.0.0.2) from 192.168.0.1 : 56(84) bytes of data.\n64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=5.75 ms\n64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=3.38 ms\n64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=2.32 ms\n^C\n--- 10.0.0.2 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 2.319/3.816/5.754/1.438 ms\n</code></pre> <ol> <li>We have to specify the outgoing IP address to be from the IXP network, otherwise, the connected loopback would be used as a source.</li> </ol> <p>Great, it works! We have built a lab simulating a simple IXP setup where members exchange their routes via Route Server in an open-policy fashion. We followed the control plane operations where BGP peerings were established between each memeber and a route server. And finally, we verified that the datapath has been programmed in accordance to the control plane instructions, and the datapath works between the networks of the respective members.</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#lab-lifecycle","title":"Lab lifecycle","text":"<p>Not a single lab is perfect on the first try. Thus, you will often find yourself in need to make changes to the configuration, topology and design. Containerlab strives to give you the best possible user experience in managing your lab and is equipped with a handful set of commands.</p> <p>Once you made some changes to the topology or attached configurations you can redeploy your lab by first removing the running and deploying it again. This sounds like a lot of moves, but <code>clab dep -c</code> (short for <code>containerlab deploy --reconfigure</code>) shortcut is all you need to achieve that.</p> <p>When you need to just remove the lab from your host without re-spinning it up, use <code>clab des -c</code> (short for <code>containerlab destroy --cleanup</code>) and everything will be gone like it never was there.</p> <p>Note</p> <p>The bridge is not removed by containerlab when you destroy the lab. You have to remove it manually if needed.</p> <p>To remind yourself if you have any labs running and which nodes are there, use <code>clab ins -a</code> (short for <code>containerlab inspect --all</code>).</p> <p>These commands will quickly get into your muscle memory and you will feel like you've never been so fast in running labs!</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#what-next","title":"What next?","text":"<p>This lab was designed to be a trivial IXP setup. It lays a foundation on which you can build way more elaborated use cases:</p> <ol> <li>Introduce proper Route Server configuration with route filtering according to MANRS. Maybe utilise ARouteServer project to test route server config automation.</li> <li>Swap bridge-based IXP network with a real emulated network running VPLS or EVPN-VXLAN and try to build the best IXP network setup.</li> <li>Get into the deep woods of MAC filtering, route suppression or BUM optimization.</li> <li>Try out peering automation scenarios using IXP manager or home-grown scripts.</li> </ol> <p>As you can see, there are dozens of complex scenarios that may be added to this lab that serves as an entrypoint to the world of IXP use cases. We would like to hear from you about what you want us to build next in the comments, chao!</p>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/basic-ixp-lab-with-openbgpd-route-server/#additional-info","title":"Additional info","text":"<ul> <li>Containerlab project</li> <li>OpenBGP home page</li> <li>OpenBGPd How To</li> <li>RIPE: Adding diversity to the route server landscape</li> </ul> <ol> <li> <p>Paths are relative to the topology file.\u00a0\u21a9</p> </li> <li> <p>This is planned as an advanced IXP lab, stay tuned!\u00a0\u21a9</p> </li> <li> <p>Check OpenBGP documentation to see what other commands might be useful for your use case.\u00a0\u21a9</p> </li> </ol>","tags":["ixp","openbgp","sr linux","bgp"]},{"location":"blog/2022/material-podcast-design-deploy-and-operate-with-nokia-data-center-fabric-solution/","title":"Design, Deploy, And Operate With Nokia Data Center Fabric Solution","text":"<p> Packet Pushers</p> <p>In this Heavy Networking piece we\u2019re talking data center operations and automation. Data centers aren\u2019t immune from the pressures being felt across the IT organization: things like enabling new applications and services more quickly, getting better visibility to monitor performance and speed up troubleshooting, and tying into new capabilities that come from automation, APIs, containers, and microservices.</p> <p>Today\u2019s sponsor, Nokia, has been thinking about these pressures, and is here to talk about its fabric-based approach to the data center. That approach includes its SR Linux network OS, its Fabric Services System intent-based platform, its NetOps Development Kit, or NDK, and how all this ties together to address your operational life cycle across Day zero, Day 1, Day Two, and beyond.</p> <p>Participants:  Bruce Wallis</p>","tags":["media","packet pushers","podcast","fss"]},{"location":"blog/2022/material-podcast-develop-custom-network-apps-with-nokias-sr-linux-ndk/","title":"Develop Custom Network Apps With Nokia\u2019s SR Linux NDK","text":"<p> Packet Pushers</p> <p>In this Tech Bytes podcast we talk with sponsor Nokia about its SR Linux network OS. More specifically, because SR Linux is open, customers can write homegrown applications to solve specific problems with the network OS using Nokia\u2019s NetOps Development Kit (NDK).</p> <p>The NDK provides documentation and examples to help you write apps quickly and easily. The NDK supports industry standards including YANG, gRPC, and gNMI. Is this a good idea? Do customers actually do this?</p> <p>Participants:  Jon Lundstrom</p>","tags":["media","packet pushers","podcast","ndk"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/","title":"Official Ansible collection for SR Linux","text":"<p>Ever since we released a tutorial that showed how to use Ansible's URI module with SR Linux, we couldn't shake off the feeling that we would need to do more with Ansible. And we did. We are happy to announce that we have released an official Ansible collection for SR Linux - <code>nokia.srlinux</code> - that has four modules inside and leverages JSON-RPC interface.</p> <p>In this blog post, we would like to share some details about our design decisions and why we think this collection is a great addition to the Ansible ecosystem.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#deficiencies-of-the-uri-module","title":"Deficiencies of the URI module","text":"<p>The URI module is a great tool for making REST API calls. It is very flexible, generic and can be used to make any type of HTTP/RESTAPI calls. However, its generic nature can also be seen as a drawback.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#verbosity","title":"Verbosity","text":"<p>The verbosity that comes with URI module can't be ignored. It requires a lot of wiring to be done before the request is ready. Consider the following example where we want to update <code>/system/information</code> container:</p> <pre><code>- name: Configuration\nhosts: all\nconnection: local\ngather_facts: no\ntasks:\n- name: Various configuration tasks\nansible.builtin.uri:\nurl: http://{{inventory_hostname}}/jsonrpc\nurl_username: admin\nurl_password: NokiaSrl1!\nmethod: POST\nbody:\njsonrpc: \"2.0\"\nid: 1\nmethod: set\nparams:\ncommands:\n- action: update\npath: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\nbody_format: json\n</code></pre> <p>Parameters like <code>url</code>, <code>url_username</code>, <code>url_password</code>, <code>method</code> and <code>body</code> are all required and must be specified for every request. This makes the code very verbose and hard to read. Using variables won't help much either, as the parameters still need to be explicitly set.</p> <p>With a custom module we can hide all the boilerplate code and make the request contain only the intent:</p> <pre><code>- name: Configuration\nhosts: all\ngather_facts: false\ntasks:\n- name: Various configuration tasks\nnokia.srlinux.config:\nupdate:\n- path: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\n</code></pre>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#error-handling","title":"Error handling","text":"<p>The URI module doesn't provide any error handling besides the HTTP error codes returned by the server. In case the errors are returned by the application and not a server, it becomes the user's problem to check the response body for error messages. This is not a problem if you are making a single request, but if you are making multiple requests, you will need to check the response code for each request. This can be done using <code>register</code> and <code>failed_when</code> parameters, but it is not convenient.</p> <p>In a custom module you are in full control of the error handling and can make decisions based on the response body. For example, you can check if the response contains an error message and fail the task if it does with a clean error message.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#idempotency","title":"Idempotency","text":"<p>A common problem with the URI module is idempotency. The module doesn't provide any means to check if the configuration is already present on the device without writing additional requests as part of the playbook. This means that the user needs to implement the idempotency logic themselves.</p> <p>In a custom module, you can implement the idempotency logic and make the module idempotent by default. This means that the user doesn't need to worry about idempotency and can focus on the intent.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#check-and-diff","title":"Check and Diff","text":"<p>Besides idempotency, the with URI module it is impossible to implement the <code>check</code> and <code>diff</code> functionality which is table stakes for network automation. Another requirement that warrants a custom module development.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#design-decisions","title":"Design decisions","text":"<p>So, all these fallacies of the URI module led us to the decision to develop a custom module for SR Linux. We wanted to make the module as easy to use as possible, while keeping the generic nature.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#network-resource-vs-generic-module","title":"Network resource vs generic module","text":"<p>The first decision we had to make was whether to follow a network resource module principles or stick with a generic module approach.</p> <p>The network resource module approach says that developers should create a module for each configuration section. For example, a separate module to configure VLANs, another one to configure BGP, a third one to configure EVPN, and so on.</p> <p>The generic module philosophy is different. Instead of creating a horde of modules, you create a single module per distinct operation and use the module parameters to specify the intent. For example, a module to configure the network device may be called <code>config</code> and a module to retrieve information from it - <code>get</code>. The <code>config</code> and <code>get</code> modules will have a parameter that would specify the configuration/state section to work with instead of creating separate modules for each section.</p> <p>We decided to go with the generic modules as it is significantly easier to maintain and the benefits of the network resource modules are not strong enough when you have a fully modelled NOS at your disposal.</p> <p>For example, here is how you configure an interface with a generic <code>nokia.srlinux.config</code> module and a netres example that does the same:</p> nokia.srlinux.getnetwork resource example <pre><code>- name: Add interface\nhosts: clab\ngather_facts: false\ntasks:\n- name: Add interface\nnokia.srlinux.config:\nupdate:\n- path: /interface[name=ethernet-1/1]\nvalue:\nadmin-state: enable\ndescription: \"interface description set with Ansible\"\nsubinterface:\n- index: 0\nadmin-state: enable\ndescription: \"subinterface description set with Ansible\"\nipv4:\nadmin-state: enable\naddress:\n- ip-prefix: 192.168.0.100/24\n</code></pre> <p>And this is how it'd looked like if we opted in for the network resource modules:</p> <pre><code>- name: Add interface\nhosts: clab\ngather_facts: false\ntasks:\n- name: Add interface\nnokia.srlinux.interface:\nadmin-state: enable\ndescription: \"interface description set with Ansible\"\nsubinterface:\n- index: 0\nadmin-state: enable\ndescription: \"subinterface description set with Ansible\"\nipv4:\nadmin-state: enable\naddress:\n- ip-prefix: 192.168.0.100/24\n</code></pre> <p>As you can see, the generic module has only one additional parameter - <code>path</code> which specifies the configuration section to work with. The rest of the parameters are the same. The development effort though is significantly lower as you don't need to create a separate module for each configuration section. This also means that any changes to the underlying YANG model that SR Linux might undergo, won't break the modules.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#check-and-diff_1","title":"Check and Diff","text":"<p>Network people crave for the <code>check</code> and <code>diff</code> functionality. It is almost a must-have for any network automation tool.</p> <p>Our <code>nokia.srlinux.config</code> fully supports both check and diff modes. To implement these functions we had to enhance our JSON-RPC interface to support the <code>diff</code> method.</p> <p>The newly added <code>diff</code> method allows us to send a configuration set to the SR Linux and get back the difference between the current configuration and the configuration set. If the returned diff is empty, it means that the configuration is already present on the device, and therefore the task won't lead to any changes.</p> <p>The difference is returned in the form of a diff string which can be used by Ansible to display the changes to support the diff mode.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#idempotency_1","title":"Idempotency","text":"<p>The same <code>diff</code> method of JSON-RPC enabled us to add a first class support for idempotency. When you use the <code>config</code> module it will automatically check if the configuration is already present on the device and will skip the task if it is.</p> <p>This means that your change set won't even be attempted to be applied to the device if it is already present.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#bulk-operations","title":"Bulk operations","text":"<p>Because we use a custom JSON-RPC API exposed by SR Linux, we can implement bulk operations. This means that you can send multiple configuration changes in a single request and the device will apply them all in a single transaction.</p> <p>This is a solid improvement over the untransactional RESTCONF API where you have to send a separate request for each configuration change and implement sophisticated rollback procedures if errors happen midway.</p> <p>Ivan will keep us honest on this one.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#embedded-validation","title":"Embedded validation","text":"<p>As part of our idempotency workflow when we check if a diff yields any changes, we also check if the configuration set is valid using the <code>validate</code> method of JSON-RPC. This means that you can be sure that the configuration you are trying to apply passes SR Linux validation.</p> <p>This makes it unnecessary to call for the <code>validate</code> method separately and makes the module more user-friendly.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#cli-operations","title":"CLI operations","text":"<p>Besides model-driven based operations exposed by <code>config</code> and <code>get</code> modules we also provide the <code>cli</code> module.</p> <p>The <code>cli</code> module allows you to execute any CLI command on the device and get the output back. This is useful for operational tasks like getting the output of the <code>show</code> commands or calling CLI plugins.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#examples","title":"Examples","text":"<p>To help you get started, we have created a set of examples that you can use as a reference when developing your own playbooks. These examples are outlined in the \"Examples\" section for each module.</p> <p>Besides that, we have adapted the original Ansible tutorial to feature the SR Linux collection.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/official-ansible-collection-for-sr-linux/#conclusion","title":"Conclusion","text":"<p>We hope that you will find the SR Linux collection useful and will be able to use it to automate your SR Linux devices. The generic nature of the modules should enable 100% coverage of the SR Linux YANG model for both configuration and state retrieval operations.</p> <p>The <code>cli</code> module should help you with operational tasks and the <code>check</code> and <code>diff</code> modes should make your automation more robust and error-proof.</p> <p>We are eager to hear from you if you find any functionality missing or if you have any suggestions on how to improve the collection. Please leave comments below or open an issue on GitHub.</p>","tags":["sr linux","ansible"]},{"location":"blog/2023/material-podcast-event-driven-automation-with-nokias-sr-linux-event-handler-framework/","title":"Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework","text":"<p> Packet Pushers</p> <p>In this Tech Bytes podcast we talk about Event Handler, a new automation feature in Nokia\u2019s SR Linux network OS that lets you automatically run scripts to fix problems when an event occurs.</p> <p>We discuss:</p> <ul> <li>SR Linux\u2019s modular design</li> <li>The new Event Handler Framework</li> <li>How Event Handler works</li> <li>Use cases for network engineers and operations teams</li> </ul> <p>Participants:  Roman Dodin</p>","tags":["media","packet pushers","podcast","event handler"]},{"location":"blog/2023/sr-linux-logging-with-elk/","title":"SR Linux logging with ELK","text":"<p>Join the discussion:  LinkedIn post \u00b7  Twitter thread</p> <p>In a not-so-distant past, manually extracting, parsing, and reading log files produced by network elements was standard practice for a sysadmin. With arcane piping of old-but-good <code>grep</code>, <code>awk</code>, and <code>sed</code> tools, one could swiftly identify a problem in a relatively large system. This was a viable approach for quite some time, but it became prey to a massive scale.</p> <p>Today's network infrastructures often count thousands of elements, each emitting log messages. Getting through a log collection of this size with CLI tools designed decades ago might not be the best tactic. As well as correlating logs between network elements and application logs might be impossible without software solutions built with such use cases in mind.</p> <p>The unprecedented growth in the application world boosted the development of multi-purposed centralized/cloud data collectors that make observability and discovery over huge data sets a reality. Elasticsearch / Logstash / Kibana (or ELK for short) is one of the most known open-source stacks tailored for the collection and processing of various documents, logs included.</p> <p>To enable the processing of captured logs and deliver performant and robust search analytics log collectors rely on structured data. Unfortunately, the networking world is infamous for iterating slowly. For example, an outdated and informational Syslog interface still dominates the networking space when it comes to managing and transferring logs. Syslog RFC3164<sup>4</sup> was not designed to allow extensible structured payloads, which adds a fair share of problems with integrating such systems with modern log collectors.</p> <p>This post explains how an SR Linux-powered DC fabric can be integrated with a modern logging infrastructure based on the Elasticsearch / Logstash / Kibana stack to collect, transform, handle, and view logs.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#lab","title":"Lab","text":"<p>As you would have expected, the post is accompanied by the containerlab-based lab that consists of an SR Linux fabric and ELK stack. DC Fabric comes preconfigured with an EVPN-VXLAN L2 domain instance, and syslog-based logging is set up on the SR Linux network elements.</p> <p>Note</p> <p>Anyone can spin this lab on their machine since all the lab elements are using official public container images.</p> Summary Lab name SR Linux fabric with ELK stack Lab components Nokia SR Linux, ELK stack Resource requirements  6 vCPU  12 GB Lab srl-labs/srl-elk-lab Version information<sup>1</sup> <code>containerlab:0.36.1</code>, <code>srlinux:22.11.1</code>, ELK stack 7.17.7 Authors Anton Zyablov   Roman Dodin  <p>Lab repository contains all necessary configuration artifacts, which are mounted to the respective container nodes as outlined in the topology file.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#topology","title":"Topology","text":"<p>A 2-tier DC fabric consists of two spines and four leaves. ELK stack consists of Elasticsearch, Logstash and Kibana nodes.</p> <p>Two clients emulated with Linux containers are connected to the leaves.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#deployment","title":"Deployment","text":"<p>Prior to deploying the lab, make sure that containerlab is installed. The following installation script installs containerlab on most Linux systems.</p> Containerlab installation via installation-script<pre><code>bash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre> <p>In order to bring up your lab, follow the next simple steps:</p> <ol> <li> <p>Clone repo</p> <pre><code>git clone https://github.com/srl-labs/srl-elk-lab.git\ncd srl-elk-lab\n</code></pre> </li> <li> <p>Deploy the lab</p> <pre><code>sudo clab deploy -c\n</code></pre> </li> </ol> <p>After a quick minute, 5 SR Linux nodes and ELK stack should be in a running state.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#logging-with-syslog-and-elk","title":"Logging with Syslog and ELK","text":"<p>Nowadays, instead of keeping unstructured logs on a device and grepping through them, logs are being collected, parsed, and indexed for quick and efficient access. Elasticsearch/Logstash/Kibana (or ELK for short) is the popular stack of technologies that enable modern logging infrastructure. This particular logging stack is used in application and networking realms alike.</p> <p>We will use the ELK stack to collect, parse, transform, and store logs from SR Linux network elements.</p> <p>On a high level, our data pipeline undergoes the following stages:</p> <ol> <li>SR Linux is configured to send syslog messages to Logstash.</li> <li>Logstash receives raw syslog messages, parses it to create a structured document format, and passes it over to Elasticsearch.</li> <li>Elasticsearch receives structured documents that it indexes and stores.</li> </ol> <p>The subsequent chapters of this blog post will zoom in on each of those steps and guide the readers through the process of creating a logging pipeline, running search queries, and using Kibana visualizations.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#configuring-logging-on-sr-linux","title":"Configuring logging on SR Linux","text":"<p>Let's start from the beginning of our data pipeline and configure logging on SR Linux nodes.</p> <p>Nokia SR Linux Network OS design employs a high level of granularity where each service is represented as a standalone process with a messaging bus enabling inter-process communication. These processes implement logging via the standard Linux syslog interface. In particular, SR Linux uses a well-known rsyslog server to manage syslog messages in the underlying Linux OS.</p> <p>Tip</p> <p>Refer to the official Logging configuration guide and Log Events Guide to learn more about logging configuration on SR Linux.</p> <p>You can modify the SR Linux logging configuration using the CLI, northbound API, or by editing the rsyslog config files<sup>2</sup>.</p> <p>Basic logging configuration consists of specifying a source for input log messages, filtering the log messages, and specifying an output destination for the filtered log messages.</p> <p>Messages logged to Linux syslog facilities and messages generated by SR Linux subsystems can be used as input sources for log messages. You can find a list of those facilities and subsystems in the documentation. When defining a Linux OS facility or SR Linux subsystem as a log source, users can provide a priority param that narrows the capture to a given severity<sup>3</sup>, a range, or a specific set of severities.</p> <p>A destination for the ingested and filtered log messages can be one of the following:</p> <ul> <li>local log file: a file on disk that can be configured with retention policies.</li> <li>buffer: an in-memory file.</li> <li>console: console output.</li> <li>remote: remote server.</li> </ul> <p>In the course of this blog post, we will work with a <code>remote-destination</code> output type, as we intend to send the log messages over to Logstash for processing.</p> <p>With a basic understanding of how to configure logging on SR Linux, let's see what does logging configuration look like as seen on <code>leaf1</code> node:</p> Syslog configuration on leaf1<pre><code>--{ running }--[  ]--\nA:leaf1# enter candidate  \n--{ candidate shared default }--[  ]--\nA:leaf1# system logging  \n--{ candidate shared default }--[ system logging ]--\nA:leaf1# info \n    network-instance mgmt\n    buffer messages {\n        # snip\n    }\n    buffer system {\n        # snip\n    }\n    file messages {\n        # snip\n    }\n    remote-server 172.22.22.11 {\n        transport udp\n        remote-port 1514\n        subsystem aaa {\n            priority {\n                match-above informational\n            }\n        }\n        subsystem acl {\n            priority {\n                match-above informational\n            }\n        }\n        # other subsystems snipped here for brevity\n        subsystem vxlan {\n            priority {\n                match-above informational\n            }\n        }\n    }\n</code></pre> <p>The <code>remote-server 172.22.22.11</code> container is where we configure a remote syslog collector; the remote server IP matches the Logstash node address specified in the topology file. A transport protocol/port is provided for the remote log destination, followed by several SR Linux subsystems for which we would like logs to be sent.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#log-format","title":"Log format","text":"<p>Consider the following example of syslog-formatted messages that can be seen on SR Linux:</p> <pre><code>Jan 11 18:39:02 leaf2 sr_bgp_mgr: bgp|1894|1965|00075|N: In network-instance default, the BGP session with VR default (1): Group ibgp-evpn: Peer 10.0.0.5 moved into the ESTABLISHED state\nJan 11 18:39:02 leaf2 sr_bfd_mgr: bfd|1879|1879|00062|N: BFD:  Network-instance default - The protocol BGP is now using BFD session from 10.0.0.2:16395 to 10.0.0.5:0\nJan 11 18:39:02 leaf2 sr_bfd_mgr: bfd|1879|1879|00063|N: BFD:  Network-instance default - Session from 10.0.0.2:16395 to 10.0.0.5:16405 is UP\nJan 11 18:40:31 leaf2 sr_linux_mgr: linux|1658|1658|00256|W: Memory utilization on ram module 1 is above 70%, current usage 83%\n</code></pre> <p>Log message format that is used by rsyslog when sending to remote destination has the following signature:</p> <pre><code>&lt;TIMESTAMP&gt; &lt;HOSTNAME&gt; &lt;APPLICATION&gt;: &lt;SUBSYSTEM&gt;|&lt;PID&gt;|&lt;THREAD_ID&gt;|&lt;SEQUENCE&gt;|&lt;SEVERITY&gt;: &lt;MESSAGE&gt;\n</code></pre> <p>where</p> <pre><code>&lt;TIMESTAMP&gt;   - Time in format - MMM DD HH:MM:SS.\n&lt;HOSTNAME&gt;    - SR Linux hostname.\n&lt;APPLICATION&gt; - SR Linux application name, in the context of Syslog this is the Message ID.\n&lt;SUBSYSTEM&gt;   - SR Linux subsystem name, which is configured under /system/logging/remote-server &lt;PID&gt;         - Process ID.\n&lt;THREAD_ID&gt;   - Thread ID.\n&lt;SEQUENCE&gt;    - Sequence number, which allows to reproduce order of the messages sent by SR Linux.\n&lt;SEVERITY&gt;    - A singe char indicating criticality of the message (I - informational, W - warning, etc.)\n&lt;MESSAGE&gt;     - Application free-form message that provides information about the event, that could contain network-instance name, like \"Network-instance default\".\n</code></pre> Dumping syslog messages sent to the remote-server <p>The format that rsyslog uses to send log messages to the remote destination differs from the default format used for <code>buffer</code> and <code>file</code> destination.</p> <p>To see the messages on the wire as they are being sent towards a remote syslog collector users can leverage <code>tcpdump</code> tool available on SR Linux:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# bash \n[admin@leaf1 ~]$ tcpdump -vAnni any dst 172.22.22.11\ntcpdump: listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes\n23:21:04.782934 mgmt0.0 Out IP (tos 0x0, ttl 64, id 60086, offset 0, flags [DF], proto UDP (17), length 141)\n    172.22.22.21.58170 &gt; 172.22.22.11.1514: UDP, length 113\nE.....@.@..\\.........:...yo.&lt;181&gt;Feb  9 23:21:04 leaf1 sr_aaa_mgr: aaa|1599|1690|00010|N: Opened session for user admin from host 172.22.22.1\n</code></pre> <p>The dump shows the message as it is on the wire and can be used to write parsers in Logstash.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#logstash","title":"Logstash","text":"<p>  Logstash docs</p> <p>Now we are at a point when a raw syslog message has been generated by SR Linux and sent towards its first stop - Logstash.</p> <p>Note</p> <p>When preparing this post, savvy folks on Twitter suggested that Logstash might be overkill for the scope of this lab. Instead, a lightweight Elastic-agent or Filebeat container could've been used.</p> <p>This is a sound idea for a sequel.</p> <p>Logstash's task is to receive syslog messages, parse them into a structured document that Elasticsearch can digest, and pass it over.</p> <p>Logstash configuration includes four artifacts that we mount to the container:</p> <ol> <li>Logstash config file - main configuration file that contains global parameters of the Logstash.</li> <li>Pipeline config - pipeline configuration file.</li> <li>Patterns used in the pipeline - regexp patterns that are used in the pipeline.</li> <li>Index template file - a file that tells logstash which types to use for custom parsed fields.</li> </ol> <p>The most important Logstash file is the pipeline config file that defines inputs, filters, and outputs.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#input","title":"Input","text":"<p>The input section, in our case, looks short and sweet - we take in syslog messages from the socket and apply tags to our messages:</p> pipeline input<pre><code>input {\nsyslog{\nport =&gt; 1514\nuse_labels =&gt; true\nid =&gt; \"srlinux\"\ntags =&gt; [ \"syslog\", \"srlinux\" ]\ntimezone =&gt; \"Europe/Rome\"\n}\n}\n</code></pre> Timezone, ECS and Syslog <p>Dealing with timezones is never easy, especially when Syslog RFC3164 timestamp format is like this:</p> <pre><code>Feb 12 12:48:10\n</code></pre> <p>Note that the timestamp (besides having no ms precision and year) lacks timezone information. Without the timezone information, we have to specifically set which timezone the received timestamps are in.</p> <p>Initially, we used Logstash's <code>date</code> filter plugin and provided timezone information there. The plugin worked in the following way:</p> <ol> <li>The timestamp format was parsed by the <code>date</code> plugin.</li> <li>If the timezone field was set in the plugin config, the time was adjusted accordingly.</li> <li>The resulting timestamp was converted to UTC/GMT format and pushed to the output.</li> </ol> <p>This worked well until we switched to using Elastic Common Schema (ECS) compatibility mode. After the switch, we noticed that the timestamps were not adjusted to the timezone anymore, resulting in the time being incorrect.</p> <p>We found out that by providing the timezone information in the Syslog input chain, we fixed this issue moved the timezone info from the date filter plugin to the input.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#filter","title":"Filter","text":"<p>Most of the work is being done in the filter section, which is the pipeline's core. The ingested unstructured data is being parsed using Logstash's filter plugins, such as grok.</p> pipeline filter<pre><code>filter {\nif \"srlinux\" in [tags] {\ngrok {\npatterns_dir =&gt; [ \"/var/lib/logstash/patterns\" ]\nmatch =&gt; { \"message\" =&gt; \"%{SRLPROC:subsystem}\\|%{SRLPID:pid}\\|%{SRLTHR:thread}\\|%{SRLSEQ:sequence}\\|%{SRLLVL:initial}:\\s+(?&lt;message&gt;(.*))\" }\noverwrite =&gt; [ \"message\" ]\n# srl container\nadd_field =&gt; { \"[srl][syslog][subsystem]\" =&gt; \"%{subsystem}\"}\nadd_field =&gt; { \"[srl][syslog][pid]\" =&gt; \"%{pid}\"}\nadd_field =&gt; { \"[srl][syslog][thread]\" =&gt; \"%{thread}\"}\nadd_field =&gt; { \"[srl][syslog][sequence]\" =&gt; \"%{sequence}\"}\nadd_field =&gt; { \"[srl][syslog][initial]\" =&gt; \"%{initial}\"}\n# set ECS version ecs.version\nadd_field =&gt; { \"[ecs][version]\" =&gt; \"1.12.2\" }\n# remove unused fields\nremove_field =&gt; [ \"@version\", \"event\", \"service\", \"subsystem\", \"pid\", \"thread\", \"sequence\", \"initial\" ]\n}\ndate {\nmatch =&gt; [ \"timestamp\",\n\"MMM dd yyyy HH:mm:ss\",\n\"ISO8601\"\n]\ntimezone =&gt; \"Europe/Rome\"\n}\n}\n}\n</code></pre> <p>Note that when parsing the syslog messages, we leverage Elastic Common Schema v1 (ECS) which unifies the way fields are represented in the output document<sup>5</sup>. Fields such as log and host are used by ECS to nest data pertaining to the logs and host objects. ECS aims to unify message fields used from various applications and thus provide a streamlined search and visualization experience.</p> <p>For SR Linux-specific fields that do not map into the ECS, we use the custom <code>srl</code> object where we put nested fields such as <code>pid</code>, <code>thread</code>, etc.</p> <p>In the filter section, we also parse the date parameter of the syslog message by providing a list of parsing patterns.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#output","title":"Output","text":"<p>Once the parsing is done, Logstash feeds the structured documents to Elasticsearch as instructed by the output plugin:</p> pipeline output<pre><code>output {\nif \"srlinux\" in [tags] {\nif \"_grokparsefailure\" in [tags] {\nfile {\npath =&gt; \"/srl/fail_to_parse_srl.log\"\ncodec =&gt; rubydebug\n}\n} else {\nelasticsearch {\nhosts =&gt; [\"http://elastic\"]\nssl =&gt; false\nindex =&gt; \"fabric-logs-%{+YYYY.MM.dd}\"\nmanage_template =&gt; true\ntemplate =&gt; \"/tmp/index-template.json\"\ntemplate_name =&gt; \"fabric-template\"\ntemplate_overwrite =&gt; true\nid =&gt; \"fabric-logs\"\n}\nstdout {}\n}\n}\n}\n</code></pre> <p>In the output section, we set the address of the elastic server, the desired index name, and the index template file to use.</p> <p>The <code>index-template.json</code> file contains the types that we, as designers of the pipeline, want Elastic to use when indexing and storing documents.</p> <p>The resulting outgoing JSON document generated at the end of the Logstash pipeline looks similar to that example:</p> JSONrdebug <pre><code>{\n\"@timestamp\": \"2023-02-11T16:29:25.000Z\",\n\"host\": {\n\"ip\": \"172.22.22.21\",\n\"hostname\": \"leaf1\"\n},\n\"ecs\": {\n\"version\": \"1.12.2\"\n},\n\"log\": {\n\"syslog\": {\n\"priority\": 181,\n\"facility\": {\n\"code\": 22,\n\"name\": \"local6\"\n},\n\"severity\": {\n\"code\": 5,\n\"name\": \"Notice\"\n}\n}\n},\n\"process\": {\n\"name\": \"sr_aaa_mgr\"\n},\n\"tags\": [\n\"syslog\",\n\"srlinux\"\n],\n\"message\": \"Closed session for user admin from host 172.22.22.1\",\n\"srl\": {\n\"syslog\": {\n\"thread\": \"1681\",\n\"sequence\": \"00007\",\n\"initial\": \"N\",\n\"pid\": \"1630\",\n\"subsystem\": \"aaa\"\n}\n}\n}\n</code></pre> <p>The <code>ruby debug</code> format is seen in the log of the logstash container as enabled by the <code>stdout</code> output for debug purposes. <pre><code>{\n\"@timestamp\" =&gt; 2023-02-11T16:24:25.000Z,\n\"host\" =&gt; {\n\"ip\" =&gt; \"172.22.22.21\",\n\"hostname\" =&gt; \"leaf1\"\n},\n\"ecs\" =&gt; {\n\"version\" =&gt; \"1.12.2\"\n},\n\"log\" =&gt; {\n\"syslog\" =&gt; {\n\"priority\" =&gt; 181,\n\"facility\" =&gt; {\n\"code\" =&gt; 22,\n\"name\" =&gt; \"local6\"\n},\n\"severity\" =&gt; {\n\"code\" =&gt; 5,\n\"name\" =&gt; \"Notice\"\n}\n}\n},\n\"process\" =&gt; {\n\"name\" =&gt; \"sr_aaa_mgr\"\n},\n\"tags\" =&gt; [\n[0] \"syslog\",\n[1] \"srlinux\"\n],\n\"message\" =&gt; \"Opened session for user admin from host 172.22.22.1\",\n\"srl\" =&gt; {\n\"syslog\" =&gt; {\n\"thread\" =&gt; \"1681\",\n\"sequence\" =&gt; \"00006\",\n\"initial\" =&gt; \"N\",\n\"pid\" =&gt; \"1630\",\n\"subsystem\" =&gt; \"aaa\"\n}\n}\n}\n</code></pre></p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#debugging","title":"Debugging","text":"<p>When developing Logstash filters, it is unavoidable to make errors on the first try. There are a couple of tricks worth sharing that can assist in making your life easier.</p> <p>First, when developing the <code>grok</code> patch patterns, it might be helpful to run a simulation using the Dev Tools provided by Kibana to verify that you actually matched the required fields:</p> <p></p> <p>In the event of a parsing failure, Logstash will dump messages it couldn't parse to a file that users can see at <code>./elk/logstash/logs/fail_to_parse_srl.log</code>.</p> <p>In addition to the parsing log, Logstash emits application logs that may provide hints on what went wrong in case of failures as well as outputting events at the end of the pipeline to stdout. You can read this log using <code>docker logs -f logstash</code>.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#elasticsearch","title":"Elasticsearch","text":"<p>  Elastic docs</p> <p>Elasticsearch is the distributed search and analytics engine at the heart of the Elastic Stack. Logstash and Beats facilitate collecting, aggregating, and enriching your data and storing it in Elasticsearch. Kibana enables you to interactively explore, visualize, and share insights into your data and manage and monitor the stack. Elasticsearch is where the indexing, search, and analysis magic happens.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#index-template-and-mappings","title":"Index Template and Mappings","text":"<p>Upon receiving a JSON document from Logstash, Elasticsearch indexes it and stores it in an index structure. When the index is created, Elastic makes the best guess about which types to use for each of the fields belonging to the JSON document. Sometimes the guess is accurate, but in many cases, the auto-guessing misses choosing the right type and, for instance, selects <code>text</code> type for an IP address.</p> <p>Ultimately, however, you know more about your data and how you want to use it than Elasticsearch can. You can define rules to control dynamic mapping and explicitly define mappings to take full control of how fields are stored and indexed. To enforce the correct types for the data we provided the Index Template as explained in the Logstash output section.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#querying-the-index","title":"Querying the index","text":"<p>To see the documents stored in the Elastic index we can leverage Elastic API using <code>curl</code> or Dev Tools panel of Kibana.</p> Query examples with <code>curl</code> Listing all documentsSearching for a particular documentRegexp search pattern <p><pre><code>\u276f curl -s \"http://localhost:9200/fabric-logs-*/_search\" -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}' | jq\n</code></pre> <pre><code>{\n\"took\": 8,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 1,\n\"successful\": 1,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 66,\n\"relation\": \"eq\"\n},\n\"max_score\": 1,\n\"hits\": [\n{\n\"_index\": \"fabric-logs-2023.02.12\",\n\"_type\": \"_doc\",\n\"_id\": \"-AQDRoYB-MNGUHTecRIg\",\n\"_score\": 1,\n\"_source\": {\n\"host\": {\n\"ip\": \"172.22.22.21\",\n\"hostname\": \"leaf1\"\n},\n\"ecs\": {\n\"version\": \"1.12.2\"\n},\n\"@timestamp\": \"2023-02-12T14:24:36.000Z\",\n\"process\": {\n\"name\": \"sr_aaa_mgr\"\n},\n\"message\": \"Closed session for user admin from host 172.22.22.1\",\n\"tags\": [\n\"syslog\",\n\"srlinux\"\n],\n\"srl\": {\n\"syslog\": {\n\"initial\": \"N\",\n\"subsystem\": \"aaa\",\n\"pid\": \"1227\",\n\"thread\": \"1260\",\n\"sequence\": \"00011\"\n}\n},\n\"log\": {\n\"syslog\": {\n\"severity\": {\n\"code\": 5,\n\"name\": \"Notice\"\n},\n\"facility\": {\n\"code\": 22,\n\"name\": \"local6\"\n},\n\"priority\": 181\n}\n}\n}\n},\n// snip\n]\n}\n}\n</code></pre></p> <p>Listing logs emitted by <code>aaa</code> subsystem of SR Linux from <code>leaf1</code> node. <pre><code>curl -s \"http://localhost:9200/fabric-logs-*/_search\" -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"srl.syslog.subsystem\": \"aaa\"\n          }\n        },\n        {\n          \"match\": {\n            \"host.hostname\": \"leaf1\"\n          }\n        }\n      ]\n    }\n  }\n}' | jq\n</code></pre> <pre><code>{\n\"took\": 35,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 1,\n\"successful\": 1,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 6,\n\"relation\": \"eq\"\n},\n\"max_score\": 3.7342224,\n\"hits\": [\n{\n\"_index\": \"fabric-logs-2023.02.12\",\n\"_type\": \"_doc\",\n\"_id\": \"-AQDRoYB-MNGUHTecRIg\",\n\"_score\": 3.7342224,\n\"_source\": {\n\"host\": {\n\"ip\": \"172.22.22.21\",\n\"hostname\": \"leaf1\"\n},\n\"ecs\": {\n\"version\": \"1.12.2\"\n},\n\"@timestamp\": \"2023-02-12T14:24:36.000Z\",\n\"process\": {\n\"name\": \"sr_aaa_mgr\"\n},\n\"message\": \"Closed session for user admin from host 172.22.22.1\",\n\"tags\": [\n\"syslog\",\n\"srlinux\"\n],\n\"srl\": {\n\"syslog\": {\n\"initial\": \"N\",\n\"subsystem\": \"aaa\",\n\"pid\": \"1227\",\n\"thread\": \"1260\",\n\"sequence\": \"00011\"\n}\n},\n\"log\": {\n\"syslog\": {\n\"severity\": {\n\"code\": 5,\n\"name\": \"Notice\"\n},\n\"facility\": {\n\"code\": 22,\n\"name\": \"local6\"\n},\n\"priority\": 181\n}\n}\n}\n},\n// snipped\n]\n}\n}\n</code></pre></p> <p><pre><code>curl -s \"http://localhost:9200/fabric-logs-*/_search\" -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"regexp\": {\n            \"message\": \".*[bB][gG][pP].*\"\n          }\n        }\n      ]\n    }\n  }\n}' | jq\n</code></pre> <pre><code>{\n\"took\": 9,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 1,\n\"successful\": 1,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 44,\n\"relation\": \"eq\"\n},\n\"max_score\": 1,\n\"hits\": [\n{\n\"_index\": \"fabric-logs-2023.02.12\",\n\"_type\": \"_doc\",\n\"_id\": \"3gSSRYYB-MNGUHTegRKj\",\n\"_score\": 1,\n\"_source\": {\n\"host\": {\n\"ip\": \"172.22.22.23\",\n\"hostname\": \"leaf3\"\n},\n\"ecs\": {\n\"version\": \"1.12.2\"\n},\n\"@timestamp\": \"2023-02-12T13:21:14.000Z\",\n\"process\": {\n\"name\": \"sr_bgp_mgr\"\n},\n\"message\": \"In network-instance default, the BGP session with VR default (1): Group ibgp-evpn: Peer 10.0.0.6 was closed because the router sent this neighbor a NOTIFICATION with code CEASE and subcode CONN_COLL_RES\",\n\"tags\": [\n\"syslog\",\n\"srlinux\"\n],\n\"srl\": {\n\"syslog\": {\n\"initial\": \"W\",\n\"subsystem\": \"bgp\",\n\"pid\": \"1595\",\n\"thread\": \"1622\",\n\"sequence\": \"00006\"\n}\n},\n\"log\": {\n\"syslog\": {\n\"severity\": {\n\"code\": 4,\n\"name\": \"Warning\"\n},\n\"facility\": {\n\"code\": 22,\n\"name\": \"local6\"\n},\n\"priority\": 180\n}\n}\n}\n},\n// snipped\n]\n}\n}\n</code></pre></p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#kibana","title":"Kibana","text":"<p>  Kibana docs</p> <p>Even though Elastic is the core of the stack, we made a very short stop at it, because we will leverage Elastic powers through a UI offered by Kibana.</p> <p>Kibana enables you to give shape to your data and navigate the Elastic Stack. With Kibana, you can:</p> <ul> <li>Search, observe, and protect your data. From discovering documents to analyzing logs to finding security vulnerabilities, Kibana is your portal for accessing these capabilities and more.</li> <li>Analyze your data. Search for hidden insights, visualize what you\u2019ve found in charts, gauges, maps, graphs, and more, and combine them in a dashboard.</li> <li>Manage, monitor, and secure the Elastic Stack. Manage your data, monitor the health of your Elastic Stack cluster, and control which users have access to which features.</li> </ul> <p>In the context of this post, we will use the stack management, searching, and visualization capabilities of Kibana.</p> <p>To access Kibana web UI point your browser to http://localhost:5601/ address.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#index-management","title":"Index Management","text":"<p>Kibana makes it easy to manage your stack by providing a nice UI on top of the rich API. Using the menu, navigate to the <code>Management -&gt; Stack Management</code> item to open the management pane of the whole stack.</p> <p>To see the index created by Elastic when Logstash sends data, use the <code>Index Management</code> menu.</p> <p></p> <p>Clicking on the index name brings up the index details, where the most interesting part is the <code>Mappings</code> used by this index.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#index-patterns","title":"Index Patterns","text":"<p>Kibana requires an Index Pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.</p> <p>Select <code>Index Pattern</code> item in the Management section to create an index pattern for the index Elastic created. Apart from the pattern name that should match the index, you have to select a field that denotes the event timestamp, in our case it is <code>@timestamp</code>.</p> <p>When Index Pattern is created, you may start searching through logs and create visualizations.</p> <p>Load saved objects</p> <p>It might be useful to create a search and visualization panels yourself to get a hold of the process, but if you'd rather skip this part we offer an import procedure:</p> <pre><code>bash ./add-saved-objects.sh\n</code></pre> <p>This script will load the Index Pattern, as well as a sample visualization dashboard and a Discover panel.</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#discover","title":"Discover","text":"<p>Now to the meat of it. The parsed logs are now neatly stored in Elastic, and you can explore them by navigating to the <code>Analytics -&gt; Discover</code> menu item. If you have imported the saved objects prepared by us, you will be able to open the <code>[Discover] Fabric Logs</code> saved search configuration that already selects some fields and sort them in descending order.</p> Selecting the saved searchSelecting time window <p></p> <p>Make sure to select the time window that suits the time at which logs were collected. </p> <p>The Discover pane allows you to filter and search the documents stored in the index; you may play with the data using the query language and identify and correlate log events.</p> <p></p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#dashboard","title":"Dashboard","text":"<p>In the <code>Analytics -&gt; Dashboard</code> users get a chance to create interactive dashboards with a wide variety of plots available. We provided a sample dashboard that gives you a sense of what can be displayed using the collected data.</p> <p></p> <p>The provided dashboard also makes it easy to skim through logs and select the ones you need to dive into:</p>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-logging-with-elk/#summary","title":"Summary","text":"<p>Congratulations, you've reached the end of this post  </p> <p>We hope you liked it and learned how logs from the network elements could be fed into the modern logging infrastructure based on ELK stack.</p> <p>It all started with configuring Syslog on SR Linux.</p> <p>Unfortunately, the unstructured nature of RFC3164 Syslog mandated the use of Logstash, which is a feature-rich log collector. We learned how to use Logstash to parse the unstructured logs, transform them into JSON documents, and output them to Elasticsearh for storage.</p> <p>With Index Templates we learned how to provide the type information for our Elastic index making sure the parsed fields use the correct type, which is crucial for an effective discovery process.</p> <p>Finally, using Kibana we learned how to perform basic Stack Management and used search and navigation capabilities.</p> <p>We left A LOT behind the brackets of this post. API access, logs correlation, Logstash-less collection using elastic-agent, k8s-based deployments. Maybe we should cover some of this in the subsequent post. Let us know in the comments, and till the next one </p> <ol> <li> <p>The lab was tested with these particular versions. It might work with a more recent version of the components.\u00a0\u21a9</p> </li> <li> <p>The SR Linux installs a minimal version of the <code>/etc/rsyslog.conf</code> file and maintains an SR Linux-specific configuration file in the <code>/etc/rsyslog.d/</code> directory.\u00a0\u21a9</p> </li> <li> <p>For fine-grain control over the log messages, refer to the Filter section of the logging guide.\u00a0\u21a9</p> </li> <li> <p>The informational RFC 3164 for Syslog was obsoleted by RFC 5424, but unfortunately, it didn't get traction in the industry.\u00a0\u21a9</p> </li> <li> <p>Check the output section to see how ECS populates data in the <code>log</code>, <code>host</code> and <code>process</code> objects automatically when parsing the syslog messages.\u00a0\u21a9</p> </li> </ol>","tags":["syslog","sr linux","elk","logging"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/","title":"SR Linux Syntax Highlighting with Pygments","text":"<p>For a very long time, I wanted to make a syntax highlighter for the SR Linux command-line interface mainly because I belong to a cohort of readers who appreciate visual aids in lengthy CLI snippets. Give me a piece of code that is not syntax highlighted, and my reading speed will significantly drop.</p> <p>And even though the Network OS CLI snippets do not contain code per-se, they have markers (such as a current command, IP addresses, up/down statuses, etc.) that when highlighted, contribute to the clarity of the provided snippet.</p> <p>So during a lazy first Thursday of 2023 I finally made myself looking into it and created the <code>srlinux-pygments</code> - a Pygments lexer to highlight SR Linux CLI snippets.</p> Raw text CLI snippetWith <code>srl</code> syntax applied <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n            route 2001:1::192:168:18:0/64 {\n                admin-state enable\n                metric 1\n                preference 6\n                next-hop-group static-ipv6-grp\n            }\n        }\n</code></pre> <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n            route 2001:1::192:168:18:0/64 {\n                admin-state enable\n                metric 1\n                preference 6\n                next-hop-group static-ipv6-grp\n            }\n        }\n</code></pre> <p>Jump under the cut to know how to create a custom syntax highlighter based on SR Linux CLI example and integrate it with mkdocs-material doc theme.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#pygments-lexers","title":"Pygments? Lexers?","text":"<p>Whenever you see a nicely highlighted code block on the web, chances are high that syntax highlighting was done using Pygments.</p> <p>Info</p> <p>Pygments is a generic syntax highlighter suitable for use in code hosting, forums, wikis or other applications that need to prettify source code. Highlights are:</p> <ul> <li>a wide range of 548 languages and other text formats is supported</li> <li>special attention is paid to details that increase highlighting quality</li> <li>support for new languages and formats are added easily; most languages use a simple regex-based lexing mechanism</li> <li>a number of output formats is available, among them HTML, RTF, LaTeX and ANSI sequences</li> <li>it is usable as a command-line tool and as a library</li> </ul> <p>Almost all python-based documentation engines exclusively use Pygments for syntax highlighting; mkdocs-material engine, which powers this learning portal, is no exception. When you create a code block in your markdown file and annotate it with some language class, Pygments kicks in and colorizes it.</p> <p>A lexer is a Pygments' component that parses the code block's content and generates tokens. Tokens are then rendered by the formatter in one of the supported ways, for example, HTML code. This might sound confusing at first, but the key takeaway here is that lexer is a python program that leverages Pygments' API to parse the raw text and extract the tokens that will be highlighted later on. So when you need to create a new syntax highlighting for a custom language, you typically only need to create a new lexer.</p> <p>Bear with me, it is less scary than it sounds .</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#what-to-highlight","title":"What to highlight?","text":"<p>Before jumping to creating our new lexer, let's draft the requirements. What do we want to highlight? There is no public standard for a Network OS CLI syntax, thus, we can choose what tokens we want to highlight.</p> <p>Consider the following SR Linux CLI snippet that displays static routes configuration stanza:</p> <pre><code># displaying configured static routes\n\n--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n            route 2001:1::192:168:18:0/64 {\n                admin-state enable\n                metric 1\n                preference 6\n                next-hop-group static-ipv6-grp\n            }\n        }\n</code></pre> <p>In this \"wall of text\", I think it is suitable to make the following adjustments:</p> <ol> <li>Make the 1<sup>st</sup> line of the prompt less intrusive. It displays auxiliary information about the selected datastore and present context, but it is not the \"meat\" of the snippet, so it's better to make it less visible.</li> <li>On the 2<sup>nd</sup> prompt line, we typically have the command - <code>info static-routes</code> in our example. This is the crux of the snippet, the key piece of information. Thus it makes sense to put the accent on the command we display.</li> <li>Interface names, IP addresses, string, and number literals are often the key user input in many configuration blocks. It makes sense to highlight these tokens to improve visibility.</li> <li>Keywords like <code>enable</code>/<code>disable</code>/<code>up</code>/<code>down</code> are often the most important part of the code blocks, especially if this is a <code>show</code> command output. We need to articulate those keywords visually.</li> <li>Authors often augment raw CLI snippets with comments; we need to make those strings render with comments style.</li> </ol> <p>Those styling requirements laid out the base for srlinux-pygments lexer project, and you can see the effect of it at the beginning of this post.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#writing-a-custom-lexer","title":"Writing a custom lexer","text":"<p>Once the requirements are fleshed out, let's create a custom Pygments lexer for SR Linux CLI snippets. Pygments documentation on writing a lexer is a good start, but it is not as welcoming as I wanted it to be, so let me fill in the gaps.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#lexer-structure","title":"Lexer structure","text":"<p>A lexer is a Python module that leverages Pygments' API to parse the input text and emit tokens which are subject to highlight. Typically, the lexer module contains a single class that subclasses Pygments's <code>RegexLexer</code> class:</p> <pre><code>from pygments.lexer import RegexLexer\nfrom pygments.token import *\nclass SRLinuxLexer(RegexLexer):\nname = 'SRLinux'\naliases = ['srl']\nfilenames = ['*.srl.cli']\ntokens = {\n'root': [\n(r\"^\\s*#.*$\", Comment), # comments\n(r\"\\s\\\".+\\\"\\s\", Literal), # strings\n]\n}\n</code></pre> <p>With the <code>name</code> field, we give a name to the lexer. The <code>aliases</code> list defines the aliases our lexers can be found by (as in the fenced code block). And <code>filenames</code> field will auto-guess this lexer for files which conform to the provided pattern.</p> <p>The whole deal of the lexer is within the <code>tokens</code> variable, which defines states and state's tuples with regular expressions and corresponding tokens. Let's zoom in.</p> <p>The <code>tokens</code> var defines a single state called <code>root</code>, which contains a list of tuples. Each tuple contains at most three elements:</p> <ol> <li>regexp expression</li> <li>token to emit for the match</li> <li>next state</li> </ol> <p>What are states?</p> <p>I have mentioned states a few times by now; they are a powerful concept for complex syntax highlighting rules. Luckily, in our simple case, we don't have to deal with states, thus we have only a single <code>root</code> state. Consequently, all our tuples have at most two elements.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#match-tuples","title":"Match tuples","text":"<p>Currently, our lexer has a single state with two tuples containing match rules written to handle Comments and String literals. Let's consider the first tuple that handles comments in our snippets:</p> <pre><code>(r\"^\\s*#.*$\", Comment)\n</code></pre> <p>The regexp matches on every string that may start with a space characters, followed by the <code>#</code> char and any number of characters after it till the end of the string. The whole match of this regexp will be assigned the <code>Comment</code> token.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#tokens","title":"Tokens","text":"<p>Pygments maintains an extensive collection of Tokens for different cases. When HTML output is used, each token is marked with a distinctive CSS class, which makes it possible to highlight it differently.</p> <p>Like in the case above, when lexer matches the comment string and HTML output is used, the whole match will be assigned a CSS class of <code>c</code> (short for Comment), and documentation themes may create CSS rules to style elements with this particular class according to their theme.</p> <p>Tip</p> <p>Read along to see how mkdocs-material uses those classes to style the elements in the code blocks.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#sr-linux-cli-match-rules","title":"SR Linux CLI match rules","text":"<p>By now, you probably figured out, that, in a nutshell, a simple lexer is just a bunch of regexps and associated tokens. Let's see which match rules and tokens we chose for SR Linux CLI snippets and for what purpose.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#handling-prompt","title":"Handling prompt","text":"<p>SR Linux prompt consists of two lines. First one holding the current datastore and its state plus the current working context. On the second line, you get the active CPM literal and the hostname. The rest is vacant for the command to type in.</p> <p>Since prompt appears in the snippet potentially many times (you show multiple commands typed in) it makes sense to make it less intrusive. On the other hand, the command you typed in is what needs to stand out, and thus it is better to be highlighted.</p> <p>We used two match tuples to handle the prompt lines. First one handles the first line and marks it with a Comment token, and second one marks the command string with Name token.</p> BeforeAfter <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n</code></pre> <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n</code></pre> <p>Tip</p> <p>Most parsers you find in srlinux-pygments repo augmented with regexp101.com links to visualise the work of the matching expression.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#keywords-positive-and-negative-words","title":"Keywords, positive and negative words","text":"<p>All CLIs have some keywords like <code>enable</code>, <code>enter</code> or <code>commit</code>. Those keywords bear significant value and thus are good candidates for highlighting. In the same spirit, words like <code>up</code>, <code>established</code> or <code>down</code> and <code>disabled</code> are important markers that a human desperately searches for during the debugging session.</p> <p>What unites those three categories is that all of them are simple words which can be easily matched using a list containing those words. This is exactly what Pygements allow us to do using the <code>word()</code> function. We keep a list of keywords, positive and negative words in a <code>words.py</code> file and then corresponding parser tuples leverage those.</p> BeforeAfter <pre><code>enter candidate\nset / interface ethernet-1/49 admin-state enable\nset / interface ethernet-1/50 admin-state disable\n</code></pre> <pre><code>enter candidate\nset / interface ethernet-1/49 admin-state enable\nset / interface ethernet-1/50 admin-state disable\n</code></pre>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#interface-names-and-ipv46-addresses","title":"Interface names and IPv4/6 addresses","text":"<p>Highlighting interface names and IP addresses is equally important. They are the beacons and key elements in Network OS configuration to which many objects bind. Making them distinguishable aids in clarity.</p> <p>These tuple parsers are responsible for matching these elements.</p> BeforeAfter <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.1/30 {\n            }\n        }\n    }\n}\n</code></pre> <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.1/30 {\n            }\n        }\n    }\n}\n</code></pre>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#numerals","title":"Numerals","text":"<p>We also decided to highlight digits (aka numerals) as they often indicate an index, a VLAN ID, or some other significant parameter.</p> <p>Here is a parser responsible for matching digits in different positions in the text.</p> BeforeAfterWith <code>srlmin</code> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n</code></pre> <p>Highlighting numbers can be too much for some users, for that reason we also created a minified lexer, that has everythin, but numbers highlighted. It can be selected with <code>srlmin</code> language identifier.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#other","title":"Other","text":"<p>Other parsers in the <code>parsers.py</code> file are responsible for handling Route Targets, Comments, and String literals and are simple regexp rules.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#constructing-the-lexer","title":"Constructing the lexer","text":"<p>At this stage, we created match tuples contained in the <code>parsers.py</code> file, but parsers need to be attached to the <code>token</code> variable of the lexer class as discussed in the Lexer structure section.</p> <p>This is done in the <code>srlinux.py</code> file where parsers are imported and added to the <code>root</code> state of the token variable:</p> <pre><code>\"\"\"A Pygments lexer for SR Linux configuration snippets.\"\"\"\nimport re\nfrom pygments.lexer import RegexLexer\nfrom pygments.token import *\nfrom .parsers import (\nsrl_prompt,\ncomments,\nstrings,\nkeywords,\npos_words,\nneg_words,\nsys_lo_if,\neth_if,\nipv4,\nipv6,\nnums,\nrt,\n)\n__all__ = (\"SRLinuxLexer\",)\nclass SRLinuxLexer(RegexLexer):\n\"\"\"\n    A lexer to highlight SR Linux CLI snippets.\n    \"\"\"\nname = \"SR Linux\"\naliases = [\"srl\"]\nflags = re.MULTILINE | re.IGNORECASE\ntokens = {\"root\": []}\ntokens[\"root\"].extend(srl_prompt)\ntokens[\"root\"].extend(comments)\ntokens[\"root\"].extend(strings)\ntokens[\"root\"].extend(keywords)\ntokens[\"root\"].extend(pos_words)\ntokens[\"root\"].extend(neg_words)\ntokens[\"root\"].extend(eth_if)\ntokens[\"root\"].extend(sys_lo_if)\ntokens[\"root\"].extend(ipv4)\ntokens[\"root\"].extend(ipv6)\ntokens[\"root\"].extend(nums)\ntokens[\"root\"].extend(rt)\n</code></pre> <p>Note</p> <p>The order of adding the parsers is important, as they are processed sequentially.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#installing-a-custom-lexer","title":"Installing a custom lexer","text":"<p>Now that our lexer has its structure formed with parser tuples attached, the question is how to install it so that the pygments package can use it?<sup>1</sup></p> <p>To our luck, pygments uses setuptools entry_points property that allows plugins to register easily. In the <code>setup.py</code> file we specify the <code>entry_points</code> values registering our lexer classes with pygments.lexers.</p> <p>Now, to install our custom lexer and make it known to the pygments all we need to do is:</p> Local or pypi installInstall via GitHub (from <code>branch</code>)Install via GitHub (from <code>tag</code>) <p><pre><code>pip install setup.py\n</code></pre> or <pre><code>pip install &lt;pypi package name&gt;\n</code></pre></p> <pre><code>pip install https://github.com/srl-labs/srlinux-pygments/archive/main.zip\n</code></pre> <pre><code>pip install https://github.com/srl-labs/srlinux-pygments/archive/v0.0.1/main.zip\n</code></pre>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#using-the-custom-syntax-highlighter","title":"Using the custom syntax highlighter","text":"<p>To use your custom syntax highligter, use the <code>alias</code> you provided in the lexer class definition (in our case it was <code>aliases = ['srl']</code>) in the fenced code block:</p> Code block with custom highlighter syntax<pre><code>```srl\n# displaying configured static routes\n--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n```\n</code></pre>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#testing-and-developing-the-lexer","title":"Testing and developing the lexer","text":"<p>When doing the initial development of a lexer, I wanted to have an immediate feedback loop and see the results of the changes I made to parsers. To assist in that, I have created a dockerized test environment that consists of mkdocs-material doc engine which installs the lexer on startup.</p> <p>With the <code>make test</code> command developers should have the mkdocs-material container to start and have lexers installed in editable mode. Now, to start the dev server paste in <code>mkdocs serve -a 0.0.0.0:8000</code> command and you should be able to open the web page with the mkdocs-material doc portal that displays various CLI snippets with applied highlighting.</p> <p>When you made changes to the parsers, simply <code>Ctrl+C</code> the live web server and start it again to reload pygments.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#integrating-lexer-with-mkdocs-material","title":"Integrating lexer with MkDocs-Material","text":"<p>Ok, it is all cool, but how do you make mkdocs-material to make use of the custom parser we just created? And how to know which colors it uses for which tokens? All the hard questions.</p> <p>First, we have to install the custom lexer along with the mkdocs-material. If you use mkdocs-material as a python project, install the lexer as explained before in the same virtual environment which mkdocs-material uses. Should you use mkdocs-material container image (you really should), you have to either modify the container image run command and embed the <code>pip install</code> step before calling <code>mkdocs build/serve</code> or create your own image based on original mkdocs-material image and add this step in the dockerfile.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#tokens-and-their-colors","title":"Tokens and their colors","text":"<p>Mkdocs-material offers a single color palette for code blocks syntax, and the question is how to understand which color is used for which token? To discover that we have to dig into some source files.</p> <p>First, we need to know which tokens are associated with which CSS classes (aka short names). You can find the mapping between the Token name and the corresponding CSS classes in the <code>token.py</code> file of the pygments project. For example, the <code>Comment</code> token is associated with <code>c</code> class.</p> <p>Knowing the CSS class of a particular token let's find which color variable mkdocs-material uses. This information is avilable in the <code>_highlight.scss</code> file of mkdocs-material. For example, there we can find that for a <code>c</code> CSS class the <code>var(--md-code-hl-comment-color)</code> is associated.</p> <p>With this information, you can pick up the Tokens and the corresponding colors to make your syntax highlighting style to match your design ideas.</p>","tags":["sr linux","pygments"]},{"location":"blog/2023/sr-linux-syntax-highlighting-with-pygments/#summary","title":"Summary","text":"<p>Making a simple custom highlighter for Pygments turned out to be an easy job. The only prerequisite - is familiarity with regular expressions, and Pygments handles the rest.</p> <p>I am quite happy with the result and plan to fine-tune the parsers based on users' feedback. Likelty, there is a bunch of important keywords we will discover in the CLI snippets worth highlighting.</p> <p>You can check the EVPN Layer 2 Tutorial, where snippets have been fixed to use the <code>srl</code> highlighting style.</p> <p>Tip</p> <p>Make sure to subscribe to receive email/rss notifications when new blog posts are published.</p> <ol> <li> <p>Thanks to @facelessuser and his https://github.com/facelessuser/pymdown-lexers project that helped me to get familiar with installation procedures.\u00a0\u21a9</p> </li> </ol>","tags":["sr linux","pygments"]},{"location":"kb/cfgmgmt/","title":"Cfgmgmt","text":"<p>SR Linux employs a transaction-based configuration management system. That allows for a number of changes to be made to the configuration with an explicit <code>commit</code> required to apply the changes as a single transaction.</p>"},{"location":"kb/cfgmgmt/#configuration-file","title":"Configuration file","text":"<p>The default location for the configuration file is <code>/etc/opt/srlinux/config.json</code>.</p> <p>If there is no configuration file present, a basic configuration file is auto-generated with the following defaults:</p> <ul> <li>Creation of a management network instance</li> <li>Management interface is added to the mgmt network instance</li> <li>DHCP v4/v6 is enabled on mgmt interface</li> <li>A set of default of logs are created</li> <li>SSH server is enabled</li> <li>Some default IPv4/v6 CPM filters</li> </ul>"},{"location":"kb/cfgmgmt/#configuration-modes","title":"Configuration modes","text":"<p>Configuration modes define how the system is running when transactions are performed. Supported modes are the following:</p> <ul> <li>Running: the default mode when logging in and displays displays the currently running or active configuration.</li> <li>State: the running configuration plus the addition of any dynamically added data.  Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc.</li> <li>Candidate: this mode is used to modify configuration. Modifications are not applied until the <code>commit</code> is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes:<ul> <li>Shared: this is the default mode when entering the candidate mode with <code>enter candidate</code> command.  This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied.</li> <li>Exclusive Candidate: When entering candidate mode with <code>enter candidate exclusive</code>, it locks out other users from making changes to the candidate configuration.   You can enter candidate exclusive mode only under the following conditions:  <ul> <li>The current shared candidate configuration has not been modified.</li> <li>There are no other users in candidate shared mode.</li> <li>No other users have entered candidate exclusive mode.</li> </ul> </li> <li>Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed.   When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name <code>private-&lt;username&gt;</code> unless a unique name is defined.</li> </ul> </li> </ul> <p>Note</p> <p>gNMI &amp; JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.</p>"},{"location":"kb/cfgmgmt/#setting-the-configuration-mode","title":"Setting the configuration mode","text":"<p>After logging in to the CLI, you are initially placed in <code>running</code> mode. The following table provides commands to enter in a specific mode:</p> Candidate mode Command to enter Candidate shared <code>enter candidate</code> Candidate mode for named shared candidate <code>enter candidate name &lt;name&gt;</code> Candidate private <code>enter candidate private</code> Candidate mode for named private candidate <code>enter candidate private name &lt;name&gt;</code> Candidate exclusive <code>enter candidate exclusive</code> Exclusive mode for named candidate <code>enter candidate exclusive name &lt;name&gt;</code> Running <code>enter running</code> State <code>enter state</code> Show <code>enter show</code>"},{"location":"kb/cfgmgmt/#committing-configuration","title":"Committing configuration","text":"<p>Changes made during a configuration modification session do not take effect until a <code>commit</code> command is issued. Several options are available for <code>commit</code> command, below are the most notable ones:</p> Option Action <code>commit now</code> Apply the changes, exit candidate mode, and enter running mode <code>commit stay</code> Apply the changes and then remain in candidate mode <code>commit save</code> Apply the changes and automatically save the commit to the startup configuration <code>commit confirmed</code> Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted"},{"location":"kb/cfgmgmt/#deleting-configuration","title":"Deleting configuration","text":"<p>Use the <code>delete</code> command to delete configurations while in candidate mode.</p> <p>The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: <pre><code>--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n            login-banner \"Welcome to SRLinux!\"\n        }\n    }\n\n--{ candidate shared default}--[ ]--\nA:leaf1# delete system banner\n\n--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n        }\n    }\n</code></pre></p>"},{"location":"kb/cfgmgmt/#discarding-configuration","title":"Discarding configuration","text":"<p>You can discard previously applied configurations with the <code>discard</code> command.</p> <ul> <li>To discard the changes and remain in candidate mode with a new candidate session, enter <code>discard stay</code>.</li> <li>To discard the changes, exit candidate mode, and enter running mode, enter <code>discard now</code>.</li> </ul>"},{"location":"kb/cfgmgmt/#displaying-configuration-diff","title":"Displaying configuration diff","text":"<p>Use the <code>diff</code> command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore.</p> <p>The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination.</p> <p>Global arguments include: <code>baseline</code>, <code>candidate</code>, <code>checkpoint</code>, <code>factory</code>, <code>file</code>, <code>from</code>, <code>rescue</code>, <code>running</code>, and <code>startup</code>.</p> <p>The diff command can be used outside of candidate mode, but only if used with arguments.</p> <p>The following shows a basic <code>diff</code> command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: <pre><code>--{ candidate shared default }--[ ]--\n# interface ethernet-1/1 admin-state disable\n\n--{ * candidate shared default }--[ ]--\n# interface ethernet-1/2 description \"updated\"\n\n--{ * candidate shared default }--[ ]--\n# diff\n    interface ethernet-1/1 {\n+       admin-state disable\n    }\n+   interface ethernet-1/2 {\n+       description updated\n+   }\n</code></pre></p>"},{"location":"kb/cfgmgmt/#displaying-configuration-details","title":"Displaying configuration details","text":"<p>Use the <code>info</code> command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context.</p> <p>To display the entire configuration, enter <code>info</code> from the root context: <pre><code>--{ candidate shared default}--[ ]--\n# info\n&lt;all the configuration is displayed&gt;\n--{ candidate }--[ ]--\n</code></pre></p> <p>To display the configuration for a specific context, enter info and specify the context: <pre><code>--{ candidate shared default}--[ ]--\n# info system lldp\n    system {\n        lldp {\n            admin-state enable\n            hello-timer 600\n            management-address mgmt0.0 {\n                type [\n                    IPv4\n                ]\n            }\n            interface mgmt0 {\n                admin-state disable\n            }\n        }\n    }\n</code></pre></p> <p>The following <code>info</code> command options are rather useful:</p> <ul> <li><code>as-json</code> - to display JSON-formatted output</li> <li><code>detail</code> - to display values for all parameters, including those not specifically configured</li> <li><code>flat</code> -  to display the output as a series of set statements, omitting indentation for any sub-contexts</li> </ul>"},{"location":"kb/hwtypes/","title":"Hwtypes","text":"<p>The SR Linux software supports the following Nokia hardware platforms<sup>1</sup>:</p> <ul> <li>7250 IXR-6</li> <li>7250 IXR-10</li> <li>7220 IXR-D1</li> <li>7220 IXR-D2</li> <li>7220 IXR-D2L</li> <li>7220 IXR-D3</li> <li>7220 IXR-D3L</li> <li>7220 IXR-H2</li> <li>7220 IXR-H3</li> </ul> <p>The <code>type</code> field under the node configuration sets the emulated hardware type in the containerlab file:</p> <pre><code># part of the evpn01.clab.yml file\nnodes:\nleaf1:\nkind: srl\ntype: ixrd3 # &lt;- hardware type this node will emulate\n</code></pre> <p>The <code>type</code> field defines the hardware variant that this SR Linux node will emulate. The available <code>type</code> values are:</p> type value HW platform ixr6 7250 IXR-6 ixr10 7250 IXR-10 ixrd1 7220 IXR-D1 ixrd2 7220 IXR-D2 ixrd2l 7220 IXR-D2L ixrd3 7220 IXR-D3 ixrd3l 7220 IXR-D3L ixrh2 7220 IXR-H2 ixrh3 7220 IXR-H3 <p>Tip</p> <p>Containerlab-launched nodes are started as <code>ixrd2</code> hardware type unless set to a different type in the clab file.</p> <ol> <li> <p>SR Linux can also run on the whitebox/3<sup>rd</sup> party switches.\u00a0\u21a9</p> </li> </ol>"},{"location":"kb/ifaces/","title":"Ifaces","text":"<p>On the SR Linux, an interface is any physical or logical port through which packets can be sent to or received from other devices.</p>"},{"location":"kb/ifaces/#loopback","title":"Loopback","text":"<p>Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named <code>loN</code>, where N is 0 to 255.</p>"},{"location":"kb/ifaces/#system","title":"System","text":"<p>The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces:</p> <ul> <li>The system interface can be bound to the default network-instance only.</li> <li>The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses.</li> <li>The system interface cannot be administratively disabled. Once configured, it is always up.</li> </ul> <p>The SR Linux supports a single system interface named <code>system0</code>. When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels.</p>"},{"location":"kb/ifaces/#network","title":"Network","text":"<p>Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic.</p> <p>The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options.</p> <p>In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: <code>ethernet-slot/port</code>. For example, interface <code>ethernet-2/1</code> refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card.</p> <p>On 7220 IXR-D3 systems, the QSFP28 connector ports (ports <code>1/3-1/33</code>) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: <code>ethernet-slot/port/breakout-port</code>.</p> <p>For example, if interface <code>ethernet 1/3</code> is enabled for breakout mode, its breakout ports are named as follows:</p> <ul> <li><code>ethernet 1/3/1</code></li> <li><code>ethernet 1/3/2</code></li> <li><code>ethernet 1/3/3</code></li> <li><code>ethernet 1/3/4</code></li> </ul>"},{"location":"kb/ifaces/#management","title":"Management","text":"<p>Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named <code>mgmt0</code>. The <code>mgmt0</code> interface supports the same functionality and defaults as a network interface, except for the following:</p> <ul> <li>Packets sent and received on the mgmt0 interface are processed completely in software.</li> <li>The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class.</li> <li>The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port.</li> </ul>"},{"location":"kb/ifaces/#integrated-routing-and-bridging-irb","title":"Integrated Routing and Bridging (IRB)","text":"<p>IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface.</p>"},{"location":"kb/ifaces/#subinterfaces","title":"Subinterfaces","text":"<p>On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface.</p> <p>Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags.</p> <p>While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance.</p> <p>You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface.</p> <p>The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class</p> <p>SR Linux subinterfaces can be specified as type routed or bridged:</p> <ul> <li>Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf.</li> <li>Bridged subinterfaces can be assigned to a network-instance of type mac-vrf.</li> </ul> <p>Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.</p>"},{"location":"kb/mgmt/","title":"Mgmt","text":"<p>Nokia SR Linux is equipped with 100% YANG modelled management interfaces. The supported management interfaces (CLI, JSON-RPC, and gNMI) access the common management API layer via a gRPC interface. Since all interfaces act as a client towards a common management API, SR Linux provides complete consistency across all the management interfaces with regards to the capabilities available to each of them.</p>"},{"location":"kb/mgmt/#sr-linux-cli","title":"SR Linux CLI","text":"<p>The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session.</p> <p>Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces.</p>"},{"location":"kb/mgmt/#features","title":"Features","text":"<ul> <li>Output Modifiers.   Advanced Linux output modifiers <code>grep</code>, <code>more</code>, <code>wc</code>, <code>head</code>, and <code>tail</code> are exposed directly through the SR Linux CLI.</li> <li>Suggestions &amp; List Completions.   As commands are typed suggestions are provided.  Tab can be used to list options available.</li> <li>Output Format.   When displaying info from a given datastore, the output can be formatted in one of three ways:<ul> <li>Text: this is the default out, it is JSON-like but not quite JSON.</li> <li>JSON: the output will be in JSON format.</li> <li>Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful.</li> </ul> </li> <li>Aliases.   An alias is used to map a CLI command to a shorter easier to remember command.  For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long.   An alias could be configured so that a shorter string of text could be used to execute that long CLI command.  Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.</li> </ul>"},{"location":"kb/mgmt/#accessing-the-cli","title":"Accessing the CLI","text":"<p>After the SR Linux device is initialized, you can access the CLI using a console or SSH connection.</p> <p>Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to <code>leaf1</code>:</p> <p><pre><code>ssh admin@clab-quickstart-leaf1\n</code></pre> <pre><code>Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts.\nadmin@clab-quickstart-leaf1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:leaf1#\n</code></pre></p>"},{"location":"kb/mgmt/#prompt","title":"Prompt","text":"<p>By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: <pre><code>--{ modified? mode_and_session_type }--[ context ]--\nhostname#\n</code></pre></p> <p>Example: <pre><code>--{ * candidate shared }--[ acl ]--\n3-node-srlinux-A#\n</code></pre></p> <p>The CLI prompt is configurable and can be changed within the <code>environment prompt</code> configuration context.</p> <p>In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays:</p> <ul> <li>the current mode and session type</li> <li>whether the configuration has been modified</li> <li>the user name and session ID of the current AAA session</li> <li>and the local time</li> </ul> <p>For example: <pre><code>Current mode: * candidate shared     root (36)   Wed 09:52PM\n</code></pre></p>"},{"location":"kb/mgmt/#gnmi","title":"gNMI","text":"<p>The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system.</p> <p>SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information.</p> <p>Supported gNMI RPCs are:</p> <ul> <li>Get</li> <li>Set</li> <li>Subscribe</li> <li>Capabilities</li> </ul>"},{"location":"kb/mgmt/#json-rpc","title":"JSON-RPC","text":"<p>The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system.</p> <p>If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.</p>"},{"location":"kb/netwinstance/","title":"Netwinstance","text":"<p>On the SR Linux, you can configure one or more virtual routing instances, known as network instances. Each network instance has its own interfaces, its own protocol instances, its own route table, and its own FIB.</p> <p>When a packet arrives on a subinterface associated with a network instance, it is forwarded according to the FIB of that network instance. Transit packets are normally forwarded out another subinterface of the network instance.</p> <p>SR Linux supports the following types of network instances:</p> <ul> <li>default</li> <li>ip-vrf</li> <li>mac-vrf</li> </ul> <p>The initial startup configuration for SR Linux has a single <code>default</code> network instance.</p> <p>By default, there are no ip-vrf or mac-vrf network instances; these must be created by explicit configuration. The ip-vrf network instances are the building blocks of Layer 3 IP VPN services, and mac-vrf network instances are the building blocks of EVPN services.</p> <p>Within a network instance, you can configure BGP, OSPF, and IS-IS protocol options that apply only to that network instance.</p>"},{"location":"mgmt/","title":"SR Linux Management","text":"<p>Nokia SR Linux Network OS offers performant, fully-modelled and standardized management interfaces such as:</p> <ul> <li>Open-sourced, fully customizable and feature-rich CLI.</li> <li>High-performant, telemetry-optimized gNMI.</li> <li>Friendly and easy-going JSON-RPC interface.</li> </ul> <p>We can't wait to populate this page with extensive examples showcasing every one of them. Stay tuned!</p>"},{"location":"ndk/","title":"NetOps Development Kit","text":"<p>Nokia SR Linux enables its users to create high-performance applications which run alongside native apps on SR Linux Network OS. These \"on-box custom applications\" can be deeply integrated with the rest of the SR Linux system and thus can perform tasks that are not possible with traditional management interfaces standard for the typical network operating systems.</p> <p> </p> Custom applications run natively on SR Linux NOS <p>The on-box applications (which we also refer to as \"agents\") leverage the SR Linux SDK called NetOps Development Kit or NDK for short.</p> <p>Applications developed with SR Linux NDK have a set of unique characteristics which set them aside from the traditional off-box automation solutions:</p> <ol> <li>Native integration with SR Linux system     SR Linux architecture is built so that NDK agents look and feel like any other regular application such as bgp or acl. This seamless integration is achieved on several levels:<ol> <li>System integration: when deployed on SR Linux system, an NDK agent renders itself like any other \"standard\" application. That makes lifecycle management unified between Nokia-provided system apps and custom agents.</li> <li>CLI integration: every NDK agent automatically becomes a part of the global CLI tree, making it possible to configure the agent and query its state the same way as for any other configuration region.</li> <li>Telemetry integration: an NDK agent configuration and state data will automatically become available for Streaming Telemetry consumption.</li> </ol> </li> <li>Programming language-neutral     With SR Linux NDK, the developers are not forced to use any particular language when writing their apps. As NDK is a gRPC service defined with Protocol Buffers, it is possible to use any<sup>1</sup> programming language for which protobuf compiler is available.</li> <li>Deep integration with system components     NDK apps are not constrained to only configuration and state management, as often happens with traditional north-bound interfaces. On the contrary, the NDK service exposes additional services that enable deep integration with the SR Linux system, such as listening to RIB/FIB updates or having direct access to the datapath.</li> </ol> <p>With the information outlined in the NDK Developers Guide, you will learn about NDK architecture and how to develop apps with this kit.</p> <p>Please navigate to the Apps Catalog to browse our growing list of NDK apps that Nokia or 3<sup>rd</sup> parties wrote.</p> <ol> <li> <p>This in practice covers all popular programming languages: Python, Go, C#, C, C++, Java, JS, etc.\u00a0\u21a9</p> </li> </ol>","tags":["ndk"]},{"location":"ndk/apps/","title":"App Catalog","text":"<p>SR Linux NetOps Development Kit (NDK) enables its users to write apps to solve many automation tasks, operational hurdles, or optimization problems.</p> <p>gRPC based service that provides deep integration with Network OS is quite a novel thing for a networking domain, making NDK application examples the second most valuable asset after the NDK documentation. Sometimes the best applications are born after getting inspired by others' work or ideas implemented in different projects.</p> <p>With the App Catalog, we intend to collect references to the noteworthy NDK applications that Nokia engineers or 3<sup>rd</sup> parties have open-sourced. With that growing catalog of examples, we hope that both new and seasoned NDK users will find something that can inspire them to create their next app.</p> <p>Disclaimer</p> <p>The examples listed in the App Catalog are not of production quality and should not be used \"as is.\" Visitors of App Catalog should treat those applications/agents as demo examples of what can be achieved with NDK.</p> <p>The applications kept under <code>srl-labs</code> or <code>nokia</code> GitHub organizations are not official Nokia products unless explicitly mentioned.</p>"},{"location":"ndk/apps/#ndk-agents","title":"NDK agents","text":""},{"location":"ndk/apps/#evpn-proxy","title":"EVPN Proxy","text":"<p> \u00b7 <code>jbemmel/srl-evpn-proxy</code></p> <p>SR Linux EVPN Proxy agent that allows bridging EVPN domains with domains that only employ static VXLAN.  Read more</p>"},{"location":"ndk/apps/#kbutler","title":"kButler","text":"<p> \u00b7 <code>brwallis/srlinux-kbutler</code></p> <p>kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service's external IP with a next-hop of the worker node.  Read more</p>"},{"location":"ndk/apps/#prometheus-exporter","title":"Prometheus Exporter","text":"<p> \u00b7 <code>karimra/srl-prometheus-exporter</code></p> <p>SR Linux Prometheus Exporter agent creates Prometheus scrape-able endpoints on individual switches. This horizontally-scaled telemetry collection model has additional operational enhancements over traditional setups with a central telemetry collector.  Read more</p>"},{"location":"ndk/apps/#satellite-tracker","title":"Satellite Tracker","text":"<p> \u00b7 <code>KTodts/srl-satellite-tracker</code></p> <p>A fun educational NDK app that displays current coordinates of the Internation Space Station by querying public Internet service providing raw location data.  Read more</p>"},{"location":"ndk/apps/evpn-proxy/","title":"SR Linux EVPN Proxy","text":"Description SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN Components Nokia SR Linux, Cumulus VX Programming Language Python Source Code <code>jbemmel/srl-evpn-proxy</code> Authors Jeroen van Bemmel"},{"location":"ndk/apps/evpn-proxy/#introduction","title":"Introduction","text":"<p>Most data center designs start small before they evolve. At small scale, it may make sense to manually configure static VXLAN tunnels between leaf switches, as implemented on the 2 virtual lab nodes on the left side.</p> <p></p> <p>There is nothing wrong with such an initial design, but as the fabric grows and the number of leaves reaches a certain threshold, having to touch every switch each time a device is added can get cumbersome and error prone.</p> <p>The internet and most modern large scale data center designs use dynamic control plane protocols and volatile in-memory configuration to configure packet forwarding. BGP is a popular choice, and the Ethernet VPN address family (EVPN RFC8365) can support both L2 and L3 overlay services. However, legacy fabrics continue to support business critical applications, and there is a desire to keep doing so without service interruptions, and with minimal changes.</p> <p>So how can we move to the new dynamic world of EVPN based data center fabrics, while transitioning gradually and smoothly from these static configurations?</p>"},{"location":"ndk/apps/evpn-proxy/#evpn-proxy-agent","title":"EVPN Proxy Agent","text":"<p>The <code>evpn-proxy</code> agent developed with NDK can answer the need of gradually transitioning from the static VXLAN dataplane to the EVPN based service. It has a lot of embedded functionality, we will cover the core feature here which is the Static VXLAN &lt;-&gt; EVPN Proxy functionality for point to point tunnels.</p> <p>The agent gets installed on SR Linux NOS and enables the control plane stitching between static VXLAN VTEP and EVPN-enabled service by generating EVPN routes on behalf of a legacy VTEP device.</p> <p></p>"},{"location":"ndk/apps/kbutler/","title":"kButler - k8s aware agent","text":"Description kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node Components Nokia SR Linux, Kubernetes, MetalLB Programming Language Go Source Code <code>brwallis/srlinux-kbutler</code> Additional resources This agent was demonstrated at NFD 25 Authors Bruce Wallis"},{"location":"ndk/apps/kbutler/#introduction","title":"Introduction","text":"<p>In the datacenter fabrics where applications run in Kubernetes clusters it is common to see Metallb to be used as a mean to advertise k8s services external IP addresses towards the fabric switches over BGP.</p> <p> </p> kButler agent demo setup <p>From the application owner standpoint as long as all the nodes advertise IP addresses of the application-related services things are considered to work as expected. But applications users do not get connected to the apps directly, there is always a network in-between which needs to play in unison with the applications.</p> <p>How can we make sure, that the network state matches the expectations of the applications? The networking folks may have little to no visibility into the application land, thus they may not have the necessary information to say if a network state reflects the applications configuration.</p> <p>Consider the diagram above, and the following state of affairs:</p> <ul> <li>application App1 is scaled to run on all three nodes of a cluster</li> <li>a service is created to make this application available from the outside of the k8s cluster</li> <li>all three nodes advertise the virtual IP of the App1 with its own nexthop via BGP</li> </ul> <p>If all goes well, the Data Center leaf switch will install three routes in its forwarding and will ECMP load balance requests towards the nodes running application pods.</p> <p>But what if the leaf switch has installed only two routes in its FIB? This can be a result of a fat fingering during the BGP configuration, or a less likely event of a resources congestion. In any case, the disparity between the network state and the application can arise.</p> <p>The questions becomes, how can we make the network to be aware of the applications configuration and make sure that those deviations can be easily spotted by the NetOps teams?</p>"},{"location":"ndk/apps/kbutler/#kbutler","title":"kButler","text":"<p>The kButler NDK agent is designed to demonstrate how data center switches can tap into the application land and correlated the network state with the application configuration.</p> <p>At a high level, the agent does the following:</p> <ul> <li>subscribes to the K8S service API and is specifically interested in any new services being exposed or changes to existing exposed services. Objective is to gain view of which worker nodes host an application which has an associated exposed service</li> <li>subscribes to the SR Linux NDK API listening for any changes to the FIB</li> <li>ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node</li> <li>reports the operational status within SR Linux allowing quick alerts of any K8S service degradation</li> <li>provides contextualized monitoring, alerting and troubleshooting, exposing its data model through all available SR Linux management interfaces</li> </ul>"},{"location":"ndk/apps/satellite/","title":"Satellite Tracker","text":"Description A fun and inspirational SR Linux agent displays current ISS<sup>1</sup> coordinates on an ASCII world map Components Nokia SR Linux Programming Language Python Source Code <code>KTodts/srl-satellite-tracker</code> Authors Kevin Todts"},{"location":"ndk/apps/satellite/#description","title":"Description","text":"<p>With SR Linux we provide a NetOps Development Kit (NDK) for writing your own on-box applications which we refer to as agents. This protobuf-based gRPC framework allows users to interact with the NOS on a whole new level: directly installing routes or MPLS routes in the FIB, receiving notifications when state changes for interfaces, BFD sessions or LLDP neighborships.</p> <p>Or, you make an application that can track the international space station location. But why on earth would you make such application for a router you may ask? Just because we can \ud83d\ude0e</p>"},{"location":"ndk/apps/satellite/#satellite-tracker_1","title":"Satellite tracker","text":"<p>The Satellite Tracker app is a nice little NDK app that introduces the NDK concepts and bridges it with a pinch of CLI programmability topping. The app provides a fun way to learn how NDK apps can communicate with the Internet services by switching to the management network namespace and firing up HTTP requests towards the public ISS tracking services.</p> <p>In addition to showcasing the interaction with external services, the app touches on our programmable CLI by creating a custom output plugin that displays the ISS coordinates on an ASCII app.</p> <p></p> <p>ISS coordinates are populated into the SR Linux'es state datastore and can be retrieved via any available interface (CLI, gNMI, JSON-RPC).</p> <p></p> <ol> <li> <p>International Space Station\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/apps/srl-prom-exporter/","title":"SR Linux Prometheus Exporter","text":"Description SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector. Components Nokia SR Linux, Prometheus Programming Language Go Source Code <code>karimra/srl-prometheus-exporter</code> Authors Karim Radhouani"},{"location":"ndk/apps/srl-prom-exporter/#introduction","title":"Introduction","text":"<p>Most Streaming Telemetry stacks are built with a telemetry collector<sup>1</sup> playing a key part in getting data out of the network elements via gNMI subscriptions. While this deployment model is valid and common it is not the only model that can be used.</p> <p>With SR Linux Prometheus Exporter agent we offer SR Linux users another way to consume Streaming Telemetry in a scaled out fashion.</p> <p> </p> Classic and agent-enabled telemetry stacks <p>With Prometheus Exporter agent deployed on SR Linux switches the telemetry deployment model changes from a \"single collector - many targets\" to a \"many collectors - single target\" mode. The collection role is now distributed across the network with Prometheus TSDB scraping metrics endpoints exposed by the agents.</p> <p>Adopting this model has some interesting benefits beyond load sharing the collection task across the network fleet:</p> <ol> <li>\"Removing\" gNMI complexity     As gNMI based collection now happens \"inside\" the switch, the monitoring teams do not need to be exposed to gNMI subscription internals or to worry about managing collectors. This streamlines the telemetry scraping workflows, as now the switches practically behave the same way as any other system that provides telemetry metrics. </li> <li>Easy way to add/remove subscription     Since SR Linux NDK agents provide seamless integration with all the management interfaces, the subscription handling can be done via CLI/gNMI/JSON-RPC. Users will add them the same way they do any configuration on their switches.     Most common subscriptions come pre-baked into the agent, removing the need to do anything for getting basic statistics out of the switches.  </li> <li>Auto discovery of nodes     Agents can register the prometheus endpoints they expose in Consul, which will enable Prometheus server to auto-discover the new nodes as they come This is your self-organizing telemetry fleet.</li> </ol>"},{"location":"ndk/apps/srl-prom-exporter/#agents-operations","title":"Agent's operations","text":"Agent's core components and interactions map <p>The high level operations model of the <code>srl-prometheus-exported</code> consists of the following steps:</p> <ol> <li>Agent maps metric names to gNMI XPATHs.</li> <li>A user can disable/enable metrics via any mgmt interface (CLI, gNMI, JSON-RPC)</li> <li>On each scrape request, agent performs a gNMI subscription with mode <code>ONCE</code> for all paths mapped to metrics with state enable (one subscription per metric).</li> <li>The agent will then transform the subscribe responses into prometheus metrics and send them back in the HTTP GET response body.</li> </ol> <p>The following diagram outlines the core components of the agent.</p> <p>Consult with the repository's readme on how to install and configure this agent.</p> <ol> <li> <p>collectors such as gnmic and others.\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/guide/agent-install-and-ops/","title":"Agent Installation & Operations","text":""},{"location":"ndk/guide/agent-install-and-ops/#installing-the-agent","title":"Installing the agent","text":"<p>The onboarding of an NDK agent onto the SR Linux system is simply a task of copying the agent and its files over to the SR Linux filesystem and placing them in the relevant directories.</p> <p>This table summarizes an agent's components and the recommended locations to use.</p> Component Filesystem location Executable file <code>/usr/local/bin/</code> YANG modules <code>/opt/$agentName/yang</code> Config file <code>/etc/opt/srlinux/appmgr/$agentName.yml</code> Other files <code>/opt/$agentName/</code> <p>The agent installation procedure can be carried out in different ways:</p> <ol> <li>manual copy of files via <code>scp</code> or similar tools</li> <li>automated files delivery via configuration management tools (Ansible, etc.)</li> <li>creating an <code>rpm</code> package for the agent and its files and installing the package on SR Linux</li> </ol> <p>The first two options are easy to execute, but they are a bit more involved as the installers need to maintain the remote paths for the copy commands. When using the <code>rpm</code> option, though, it becomes less cumbersome to install the package. All the installers deal with is a single <code>.rpm</code> file and a copy command. Of course, the build process of the <code>rpm</code> package is still required, and we would like to explain this process in detail.</p>"},{"location":"ndk/guide/agent-install-and-ops/#rpm-package","title":"RPM package","text":"<p>One of the easiest ways to create an rpm, deb, or apk package is to use the nFPM tool - a simple, 0-dependencies packager.</p> <p>The only thing that nFPM requires of a user is to create a configuration file with the general instructions on how to build a package, and the rest will be taken care of.</p>"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-installation","title":"nFPM installation","text":"<p>nFPM offers many installation options for all kinds of operating systems and environments. In the course of this guide, we will use the universal nFPM docker image.</p>"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-configuration-file","title":"nFPM configuration file","text":"<p>nFPM configuration file is the way of letting nFPM know how to build a package for the software artifacts that users created.</p> <p>The complete list of options the <code>nfpm.yml</code> file can have is documented on the project's site. Here we will have a look at the configuration file that is suitable for a typical NDK application written in Go.</p> <p>The file named <code>ndkDemo.yml</code> with the following contents will instruct nFPM how to build a package:</p> <pre><code>name: \"ndkDemo\"       # name of the go package\narch: \"amd64\"         # architecture you are using \nversion: \"v1.0.0\"     # version of this rpm package\nmaintainer: \"John Doe &lt;john@doe.com&gt;\"\ndescription: Sample NDK agent # description of a package\nvendor: \"JD Corp\"     # optional information about the creator of the package\nlicense: \"BSD 2\"\ncontents:                              # contents to add to the package\n- src: ./ndkDemo                     # local path of agent binary\ndst: /usr/local/bin/ndkDemo        # destination path of agent binary\n- src: ./yang                        # local path of agent's YANG directory\ndst: /opt/ndkDemo/yang             # destination path of agent YANG\n- src: ./ndkDemo.yml                 # local path of agent yml\ndst: /etc/opt/srlinux/appmgr/      # destination path of agent yml\n</code></pre>"},{"location":"ndk/guide/agent-install-and-ops/#running-nfpm","title":"Running nFPM","text":"<p>When nFPM configuration and NDK agent files are present, proceed with building an <code>rpm</code> package.</p> <p>Consider the following file layout:</p> <pre><code>.\n\u251c\u2500\u2500 ndkDemo          # agent binary file\n\u251c\u2500\u2500 ndkDemo.yml      # agent config file\n\u251c\u2500\u2500 nfpm.yml         # nFPM config file\n\u2514\u2500\u2500 yang             # directory with agent YANG modules\n\u2514\u2500\u2500 ndkDemo.yang\n\n1 directory, 4 files\n</code></pre> <p>With these files present we can build an RPM package using the containerized nFPM image like that:</p> <pre><code>docker run --rm -v $PWD:/tmp -w /tmp goreleaser/nfpm package \\\n--config /tmp/nfpm.yml \\\n--target /tmp \\\n--packager rpm\n</code></pre> <p>This command will create <code>ndkDemo-1.0.0.x86_64.rpm</code> file in the current directory that can be copied over to the SR Linux system for installation.</p>"},{"location":"ndk/guide/agent-install-and-ops/#installing-rpm","title":"Installing RPM","text":"<p>Delivering the available rpm package to a fleet of SR Linux boxes can be done with any configuration management tools. For demo purposes, we will utilize the <code>scp</code> utility:</p> <pre><code># this example copies the rpm via scp command to /tmp dir\nscp ndkDemo-1.0.0.x86_64.rpm admin@&lt;srlinux-mgmt-address&gt;:/tmp\n</code></pre> <p>Once the package has been delivered to the SR Linux system, it is ready to be installed. First, we login to SR Linux CLI and drill down to the Linux shell:</p> <pre><code>ssh admin@&lt;srlinux-address&gt;\n\nadmin@clab-srl-srl's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl# bash\n</code></pre> <p>Once in the bash shell, install the package with <code>yum install</code> or <code>rpm</code>:</p> <pre><code>sudo rpm -U /tmp/ndkDemo-1.0.0.x86_64.rpm\n</code></pre> <p>Tip</p> <p>To check if the package was installed, issue <code>rpm -qa | grep ndkDemo</code></p> <pre><code>admin@srl ~]$ rpm -qa | grep ndkDemo\nndkDemo-1.0.0-1.x86_64\n</code></pre> <p>During the package installation, the agent related files are copied over to the relevant paths as stated in the nFPM config file:</p> <pre><code># check the executable location\n[admin@srl ~]$ ls -la /usr/local/bin/ | grep ndkDemo\n-rw-r--r-- 1 root root    12312 Nov  4 11:28 ndkDemo\n\n# check YANG modules dir is present\n[admin@srl ~]$ ls -la /opt/ndkDemo/yang/\ntotal 8\ndrwxr-xr-x 2 root root 4096 Nov  4 12:58 .\ndrwxr-xr-x 3 root root 4096 Nov  4 12:53 ..\n-rw-r--r-- 1 root root    0 Nov  4 11:28 ndkDemo.yang\n\n# check ndkDemo config file is present\n[admin@srl ~]$ ls -la /etc/opt/srlinux/appmgr/\ntotal 16\ndrwxr-xr-x+  2 root    root    4096 Nov  4 12:58 .\ndrwxrwxrwx+ 10 srlinux srlinux 4096 Nov  4 12:53 ..\n-rw-r--r--+  1 root    root       0 Nov  4 11:28 ndkDemo.yml\n</code></pre> <p>All the agent components are available by the paths specified in the nFPM configuration file.</p> <p>Note</p> <p>To update the SR Linux NDK app, the package has to be removed first <pre><code>sudo yum remove ndkDemo-1.0.0 # using yum\nsudo rpm -e ndkDemo-1.0.0     # using rpm\n</code></pre></p> <p>Congratulations, the agent has been installed successfully.</p>"},{"location":"ndk/guide/agent-install-and-ops/#loading-the-agent","title":"Loading the agent","text":"<p>SR Linux's Application Manager is in charge of managing the applications lifecycle. App Manager controls both the native apps and customer-written agents.</p> <p>After a user installs the agent on the SR Linux system by copying the relevant files, they need to reload the <code>app_mgr</code> process to detect new applications. App Manager gets to know about the available apps by reading the app configuration files located at the following paths:</p> Directory Description <code>/opt/srlinux/appmgr/</code> SR Linux embedded applications <code>/etc/opt/srlinux/appmgr/</code> User-provided applications <p>To reload the App Manager:</p> <pre><code>/ tools system app-management application app_mgr reload\n</code></pre> <p>Once reloaded, App Manager will detect the new applications and load them according to their configuration. The users will be able to see their app in the list of applications:</p> <pre><code>/show system application &lt;app-name&gt;\n</code></pre>"},{"location":"ndk/guide/agent-install-and-ops/#managing-the-agents-lifecycle","title":"Managing the agent's lifecycle","text":"<p>An application's lifecycle can be managed via any management interface by using the following knobs from the <code>tools</code> schema.</p> <pre><code>/ tools system app-management application &lt;app-name&gt; &lt;start|stop|reload|restart&gt;\n</code></pre> <p>The commands that can be given to an application are translated to system signals as per the following table:</p> Command Description <code>start</code> Executes the application <code>reload</code> Send <code>SIGHUP</code> signal to the app. This signal can be handled by the app and reload its config and change initialization values if necessary <code>stop</code> Send <code>SIGTERM</code> signal to the app. The app should handle this signal and exit gracefully <code>quit</code> Send <code>SIGQUIT</code> signal to the app. Default behavior is to terminate the process and dump core info <code>kill</code> Send <code>SIGKILL</code> signal to the app. Kills the process without any cleanup"},{"location":"ndk/guide/agent/","title":"Agent Structure","text":"<p>As was explained in the NDK Architecture section, an agent<sup>1</sup> is a custom software that can extend SR Linux capabilities by running alongside SR Linux native applications and performing some user-defined tasks.</p> <p>To deeply integrate with the rest of the SR Linux architecture, the agents have to be defined like an application that SR Linux's application manager can take control of. The structure of the agents is the main topic of this chapter.</p> <p>The main three components of an agent:</p> <ol> <li>Agent's executable file</li> <li>YANG module</li> <li>Agent configuration file</li> </ol>"},{"location":"ndk/guide/agent/#executable-file","title":"Executable file","text":"<p>An executable file is called when the agent starts running on SR Linux system. It contains the application logic and is typically an executable binary or a script.</p> <p>The application logic handles the agents' configuration that may be provided via any management interface (CLI, gNMI, etc.) and contains the core logic of interfacing with gRPC based NDK services.</p> <p>In the subsequent sections of the Developers Guide, we will cover how to write the logic of an agent and interact with various NDK services.</p> <p>An executable file can be placed at <code>/usr/local/bin</code> directory.</p>"},{"location":"ndk/guide/agent/#yang-module","title":"YANG module","text":"<p>SR Linux is a fully modeled Network OS - any native or custom application that can be configured or can have state is required to have a proper YANG model.</p> <p>The \"cost\" associated with requiring users to write YANG models for their apps pays off immensely as this</p> <ul> <li>enables seamless integration of an agent with all management interfaces: CLI, gNMI, JSON-RPC.     Any agent's configuration knobs that users expressed in YANG will be immediately available in the SR Linux CLI as if it was part of it from the beginning. Yes, with auto-suggestion of the fields as well.</li> <li>provides out-of-the-box Streaming Telemetry (gNMI) support for any config or state data that the agent maintains</li> </ul> <p>And secondly, the YANG modules for custom apps are not that hard to write as their data model is typically relatively small.</p> <p>Note</p> <p>The YANG module is only needed if a developer wants their agent to be configurable via any management interfaces or keep state.</p> <p>YANG files related to an agent are typically located by the <code>/opt/$agentName/yang</code> path.</p>"},{"location":"ndk/guide/agent/#configuration-file","title":"Configuration file","text":"<p>Due to SR Linux modular architecture, each application, be it an internal app like <code>bgp</code> or a custom NDK agent, needs to have a configuration file. This file contains application parameters read by the Application Manager service to onboard the application onto the system.</p> <p>With an agent's config file, users define properties of an application, for example:</p> <ul> <li>application version</li> <li>location of the executable file</li> <li>YANG modules related to this app</li> <li>lifecycle management policy</li> <li>and others</li> </ul> <p>Custom agents must have their config file present by the <code>/etc/opt/srlinux/appmgr</code> directory. It is a good idea to name the agent's config file after the agent's name; if we have the agent called <code>myCoolAgent</code>, then its config file can be named <code>myCoolAgent.yml</code> and stored by the <code>/etc/opt/srlinux/appmgr</code> path.</p> <p>Through the subsequent chapters of the Developers Guide, we will cover the most important options, but here is a complete list of config file parameters:</p> Complete list of config files parameters <pre><code># Example configuration file for the applications on sr_linux\n# All valid options are shown and explained\n# The name of the application.\n# This must be unique.\napplication-name:\n# [Mandatory] The source path where the binary can be found\npath: /usr/local/bin\n# [Optional, default='./&lt;application-name&gt;'] The command to launch the application.\n# Note these replacement rules:\n#   {slot-num} will be replaced by the slot number the process is running on\n#   {0}, {1}, ... can be replaced by parameters provided in the launch request (launch-by-request: Yes)\nlaunch-command: \"VALUE=2 ./binary_name --log-level debug\"\n# [Optional, default='&lt;launch-command&gt;'] The command to search for when checking if the application is running.\n# This will be executed as a prefix search, so if the application was launched using './app-name -loglevel debug'\n# a search-command './app-name' would work.\n# Note: same replacement rules as launch-command\nsearch-command: \"./binary_name\"\n# [Optional, default=No] Indicates whether the application needs to be launched automatically\nnever-start: No\n# [Optional, default=No] Indicates whether the application can be restarted automatically when it crashes.\n# Applies only when never-start is No (if the app is not started by app_mgr it would not be restarted either).\n# Applications are only restarted when running app_mgr in restart mode (e.g. sr_linux --restart)\nnever-restart: No\n# [Optional, default=No] Indicates whether the application will be shown in app manager status\nnever-show: No\n# [Optional, default=No] Indicates whether the launch of the application is delayed\n# until any configuration is loaded in the application's YANG modules.\nwait-for-config: No\n# [Optional] Indicates the application is run as 'user' including 'root'\nrun-as-user: root\n# [Optional, default=200] Indicates the order in which the application needs to be launched.\n# The applications with the lowest value are launched first.\n# Applications with the same value are launched in an undefined order.\n# By convention, start-order &gt;= 100 require idb.  1 is reserved for device-mgr, which determines chassis type.\nstart-order: 123\n# [Optional, default=No] Indicates whether this application is launched via an request (idb only at this point).\nlaunch-by-request: No\n# [Optional, default=No] Indicates whether this application is launched in a net namespace (launch-by-request\n# must be set to Yes).\nlaunch-in-net-namespace: No\n# [Optional, default=3] Indicates the number of restarts within failure-window which will trigger the system restart\nfailure-threshold: 3\n# [Optional, default=300] Indicates the window in seconds over which to count restarts towards failure-threshold\nfailure-window: 400\n# [Optional, default=reboot] Indicates the action taken after 'failure-threshold' failures within 'failure-window'\nfailure-action: 'reboot'\n# [Optional, default=Nokia] Indicates the author of the application\nauthor: 'Nokia'\n# [Optional, default=\u201d\u201d] The command for app_mgr to run to read the application version\nversion-command: 'snmpd --version'\n# [Optional The operations that may not be manually performed on this application\nrestricted-operations: ['start', 'stop', 'restart', 'quit', 'kill']\n# [Optional, default No] app-mgr will wait for app to acknowledge it via oob channel\noob-init: No\n# [Optional] The list of launch restrictions - if of all of the restrictions of an element in the list are met,\n# then the application is launched.  The restrictions are separated by a ':'.  Valid restrictions are:\n#   'sim' - running in sim mode (like in container env.)\n#   'hw' - running on real h/w\n#   'chassis' - running on a chassis (cpm and imm are running on different processors)\n#   'imm' - runs on the imm\n#   'cpm' - runs on the cpm (default)\nlaunch-restrictions: ['hw:cpm', 'hw:chassis:imm']\nyang-modules:\n# [Mandatory] The names of the YANG modules to load. This is usually the file-name without '.yang'\nnames: [module-name, other-module-name]\n# [Optional] List of enabled YANG features. Each needs to be qualified (e.g. srl_nokia-common:foo)\nenabled-features: ['module-name:foo', 'other-module-name:bar']\n# [Optional] The names of the YANG validation plugins to load.\nvalidation-plugins: [plugin-name, other-plugin-name]\n# [Mandatory] All the source-directories where we should search for:\n#    - The YANG modules listed here\n#    - any YANG module included/imported in these modules\nsource-directories: [/path/one, /path/two]\n# [Optional] The names of the not owned YANG modules to load for commit confirmation purposes.\nnot-owned-names: [module-name, other-module-name]\n# [Optional] Multiple applications can be defined in the same YAML file\nother-application-name:\ncommand: \"./other-binary\"\npath: /other/path\n</code></pre>"},{"location":"ndk/guide/agent/#dependency-and-other-files","title":"Dependency and other files","text":"<p>Quite often, an agent may require additional files for its operation. It can be a virtual environment for your Python agent or some JSON file that your agent consumes.</p> <p>All those auxiliary files can be saved by the <code>/opt/$agentName/</code> directory.</p> <ol> <li> <p>terms NDK agent and NDK app are used interchangeably\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/guide/architecture/","title":"NDK Architecture","text":"<p>SR Linux provides a Software Development Kit (SDK) to assist operators with developing agents that run alongside SR Linux applications. This SDK is named NetOps Development Kit, or NDK for short.</p> <p>NDK allows operators to write applications (a.k.a agents) that deeply integrate with other native SR Linux applications. The deep integration is the courtesy of the NDK gRPC service that enables custom applications to interact with other SR Linux applications via Impart Database (IDB).</p> <p>In Fig. 1, custom NDK applications <code>app-1</code> and <code>app-2</code> interact with other SR Linux subsystems via gRPC-based NDK service.</p> Fig 1. NDK applications integration <p>In addition to the traditional tasks of reading and writing configuration, NDK-based applications gain low-level access to the SR Linux system. For example, these apps can install FIB routes or listen to LLDP events.</p>"},{"location":"ndk/guide/architecture/#grpc-protocol-buffers","title":"gRPC &amp; Protocol buffers","text":"<p>NDK uses gRPC - a high-performance, open-source framework for remote procedure calls.</p> <p>gRPC framework by default uses Protocol buffers as its Interface Definition Language as well as the underlying message exchange format.</p> <p>Info</p> <p>Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p> <p>In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types.</p> <p>On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client provides the same methods as the server.</p> Fig 2. gRPC client-server interactions <p>Leveraging gRPC and protobufs provides some substantial benefits for NDK users:</p> <ol> <li>Language neutrality: NDK apps can be written in any language for which protobuf compiler exists. Go, Python, C, Java, Ruby, and more languages are supported by Protocol buffers enabling SR Linux users to write apps in the language of their choice.</li> <li>High-performance: protobuf-encoded messaging is an efficient way to exchange data in a client-server environment. Applications that consume high-volume streams of data (for example, route updates) benefit from an efficient and fast message delivery enabled by protobuf.</li> <li>Backwards API compatibility: a protobuf design property of using IDs for data fields makes it possible to evolve API over time without ever breaking backward compatibility. Old clients will still be able to consume data stored in the original fields, whereas new clients will benefit from accessing data stored in the new fields.</li> </ol>"},{"location":"ndk/guide/architecture/#ndk-service","title":"NDK Service","text":"<p>NDK provides a collection of gRPC services, each of which enables custom applications to interact with a particular subsystem on an SR Linux OS, delivering a high level of integration and extensibility.</p> <p>With this architecture, NDK agents act as gRPC clients that execute remote procedure calls (RPC) on a system that implements a gRPC server.</p> <p>On SR Linux, <code>ndk_mgr</code> is the application that runs the NDK gRPC server. Fig 3. shows how custom agents interact via gRPC with NDK, and NDK executes the remote procedure and communicates with other system applications through IDB and pub/sub interface to return the result of the RPC to a client.</p> Fig 3. gRPC as an Inter Process Communication (IPC) protocol <p>As a result, custom applications are able to communicate with the native SR Linux apps as if they were shipped with SR Linux OS.</p>"},{"location":"ndk/guide/architecture/#proto-files","title":"Proto files","text":"<p>NDK services, underlying RPCs, and messages are defined in <code>.proto</code> files. These files are used to generate language bindings essential for the NDK apps development process and serve as the data modeling language for the NDK itself.</p> <p>The source <code>.proto</code> files for NDK are open and published in <code>nokia/srlinux-ndk-protobufs</code> repository. Anyone can clone this repository and explore the NDK gRPC services or build language bindings for the programming language of their choice.</p>"},{"location":"ndk/guide/architecture/#documentation","title":"Documentation","text":"<p>Although the proto files are human-readable, it is easier to browse the NDK services using the generated documentation that we keep in the same <code>nokia/srlinux-ndk-protobufs</code> repo. The HTML document is provided in the readme file that appears when a user selects a tag that matches the NDK release version<sup>1</sup>.</p> <p>The generated documentation provides the developers with a human-readable reference of all the services, messages, and types that comprise the NDK service.</p>"},{"location":"ndk/guide/architecture/#operations-flow","title":"Operations flow","text":"<p>Regardless of the language in which the agents are written, at a high level, the following flow of operations applies to all agents when interacting with the NDK service:</p> Fig 4. NDK operations flow <ol> <li>Establish gRPC channel with NDK manager and instantiate an NDK client</li> <li>Register the agent with the NDK manager</li> <li>Register notification streams for different types of NDK services (config, lldp, interface, etc.)</li> <li>Start streaming notifications</li> <li>Handle the streamed notifications</li> <li>Update agent's state data if needed</li> <li>Exit gracefully if required</li> </ol> <p>To better understand the steps each agent undergoes, we will explain them in a language-neutral manner. For language-specific implementations, read the \"Developing with NDK\" chapter.</p>"},{"location":"ndk/guide/architecture/#grpc-channel-and-ndk-manager-client","title":"gRPC Channel and NDK Manager Client","text":"<p>NDK agents communicate with gRPC based NDK service by invoking RPCs and handling responses. An RPC generally takes in a client request message and returns a response message from the server.</p> <p>A gRPC channel must be established before communicating with the NDK manager application running on SR Linux<sup>2</sup>. NDK server runs on port <code>50053</code>; agents which are installed on SR Linux OS use <code>localhost:50053</code> socket to establish the gRPC channel.</p> <p>Once the gRPC channel is set up, a gRPC client (often called stub) needs to be created to perform RPCs. Each gRPC service needs to have its own client. In NDK, the <code>SdkMgrService</code> service is the first service that agents interact with, therefore, users first need to create the NDK Manager Client (Mgr Client on diagram) that will be able to call RPCs defined for <code>SdkMgrService</code>.</p>"},{"location":"ndk/guide/architecture/#agent-registration","title":"Agent registration","text":"<p>Agent must be first registered with SRLinux NDK by calling <code>AgentRegister</code> RPC of <code>SdkMgrService</code>. Initial agent state is created during the registration process.</p> <p>An <code>AgentRegistrationResponse</code> is returned (omitted in Fig. 4) with the status of the registration process.</p>"},{"location":"ndk/guide/architecture/#registering-notifications","title":"Registering notifications","text":"<p>Agents interact with other services like Network Instance, Config, LLDP, BFD by subscribing to notification updates from these services.</p> <p>Before subscribing to a notification stream of a certain service the subscription stream needs to be created. To create it, a client of <code>SdkMgrService</code> calls <code>NotificationRegister</code> RPC with <code>NotificationRegistrationRequest</code> field <code>Op</code> set to <code>Create</code> and other fields absent.</p> <p>Info</p> <p><code>NotificationRegistrationRequest</code> message's field <code>Op</code> (for Operation) may have one of the following values:</p> <ul> <li><code>Create</code> creates a subscription stream and returns a <code>StreamId</code> that is used when adding subscriptions with the <code>AddSubscription</code> operation.</li> <li><code>Delete</code> deletes the existing subscription stream that has a particular <code>SubId</code>.</li> <li><code>AddSubscription</code> adds a subscription. The stream will now be able to stream notifications of that subscription type (e.g., Intf, NwInst, etc).</li> <li><code>DeleteSubscription</code> deletes the previously added subscription.</li> </ul> <p>When <code>Op</code> field is set to <code>Create</code>, NDK Manager responds with <code>NotificationRegisterResponse</code> message with <code>stream_id</code> field set to some value. The stream has been created, and the subscriptions can be added to the created stream.</p> <p>To subscribe to a certain service notification updates another call of <code>NotificationRegister</code> RPC is made with the following fields set:</p> <ul> <li><code>stream_id</code> set to an obtained value from the <code>NotificationRegisterResponse</code></li> <li><code>Op</code> is set to <code>AddSubscription</code></li> <li>one of the <code>subscription_types</code> is set according to the desired service notifications. For example, if notifications from the <code>Config</code> service are of interest, then <code>config</code> field of type <code>ConfigSubscriptionRequest</code> is set.</li> </ul> <p><code>NotificationRegisterResponse</code> message follows the request and contains the same <code>stream_id</code> but now also the <code>sub_id</code> field - subscription identifier. At this point agent successfully indicated its desire to receive notifications from certain services, but the notification streams haven't been started yet.</p>"},{"location":"ndk/guide/architecture/#streaming-notifications","title":"Streaming notifications","text":"<p>Requesting applications to send notifications is done by interfacing with <code>SdkNotificationService</code>. As this is another gRPC service, it requires its own client - Notification client.</p> <p>To initiate streaming of updates based on the agent subscriptions the Notification Client executes <code>NotificationStream</code> RPC which has <code>NotificationStreamRequest</code> message with <code>stream_id</code> field set to the ID of a stream to be used. This RPC returns a stream of <code>NotificationStreamResponse</code>, which makes this RPC of type \"server streaming RPC\".</p> Server-streaming RPC <p>A server-streaming RPC is similar to a unary RPC, except that the server returns a stream of messages in response to a client's request. After sending all its messages, the server's status details (status code and optional status message) and optional trailing metadata are sent to the client. This completes processing on the server side. The client completes once it has all the server's messages.</p> <p><code>NotificationStreamResponse</code> message represents a notification stream response that contains one or more notifications. The <code>Notification</code> message contains one of the <code>subscription_types</code> notifications, which will be set in accordance to what notifications were subscribed by the agent.</p> <p>In our example, we sent <code>ConfigSubscriptionRequest</code> inside the <code>NotificationRegisterRequest</code>, hence the notifications that we will get back for that <code>stream_id</code> will contain <code>ConfigNotification</code> messages inside <code>Notification</code> of a <code>NotificationStreamResponse</code>.</p>"},{"location":"ndk/guide/architecture/#handling-notifications","title":"Handling notifications","text":"<p>The agent handles the stream of notifications by analyzing which concrete type of notification was read from the stream. The Server streaming RPC will provide notifications till the last available one; the agent then reads out the incoming notifications and handles the messages contained within them.</p> <p>The handling of notifications is done when the last notification is sent by the server. At this point, the agent may perform some work on the received data and, if needed, update the agent's state if it has one.</p>"},{"location":"ndk/guide/architecture/#updating-agents-state-data","title":"Updating agent's state data","text":"<p>Each agent may keep state and configuration data modeled in YANG. When an agent needs to set/update its own state data (for example, when it made some calculations based on received notifications), it needs to use <code>SdkMgrTelemetryService</code> and a corresponding client.</p> Fig 5. Updating agent's state flow <p>The state that an agent intends to have will be available for gNMI telemetry, CLI access, and JSON-RPC retrieval, as it essentially becomes part of the SR Linux state.</p> <p>Updating or initializing agent's state with data is done with <code>TelemetryAddOrUpdate</code> RPC that has a request of type <code>TelemetryUpdateRequest</code> that encloses a list of <code>TelemetryInfo</code> messages. Each <code>TelemetryInfo</code> message contains a <code>key</code> field that points to a subtree of agent's YANG model that needs to be updated with the JSON data contained within <code>data</code> field.</p>"},{"location":"ndk/guide/architecture/#exiting-gracefully","title":"Exiting gracefully","text":"<p>When an agent needs to stop its operation and be removed from the SR Linux system, it needs to be unregistered by invoking <code>AgentUnRegister</code> RPC of the <code>SdkMgrService</code>. The gRPC connection to the NDK server needs to be closed.</p> <p>When unregistered, the agent's state data will be removed from SR Linux system and will no longer be accessible to any of the management interfaces.</p> <ol> <li> <p>For example, here you will find the auto-generated documentation for the latest NDK version at the moment of this writing.\u00a0\u21a9</p> </li> <li> <p><code>sdk_mgr</code> is the name of the application that implements NDK gRPC server and runs on SR Linux OS.\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/guide/dev/go/","title":"Developing agents with NDK in Go","text":"<p>This guide explains how to consume the NDK service when developers write the agents in a Go<sup>1</sup> programming language.</p> <p>Note</p> <p>This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter.</p> <p>Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations.</p> <p>In addition to the publicly available protobuf files, which define the NDK Service, Nokia also provides generated Go bindings for data access classes of NDK in a <code>nokia/srlinux-ndk-go</code> repo.</p> <p>The <code>github.com/nokia/srlinux-ndk-go</code> package provided in that repository enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Go package themselves.</p>"},{"location":"ndk/guide/dev/go/#establish-grpc-channel-with-ndk-manager-and-instantiate-an-ndk-client","title":"Establish gRPC channel with NDK manager and instantiate an NDK client","text":"<p> Additional information</p> <p>To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux.</p> <p>This is done by passing the NDK server address - <code>localhost:50053</code> - to <code>grpc.Dial()</code> as follows:</p> <pre><code>import (\n\"google.golang.org/grpc\"\n)\nconn, err := grpc.Dial(\"localhost:50053\", grpc.WithInsecure())\nif err != nil {\n...\n}\ndefer conn.Close()\n</code></pre> <p>Once the gRPC channel is setup, we need to instantiate a client (often called stub) to perform RPCs. The client is obtained using the <code>NewSdkMgrServiceClient</code> method provided.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\nclient := ndk.NewSdkMgrServiceClient(conn)\n</code></pre>"},{"location":"ndk/guide/dev/go/#register-the-agent-with-the-ndk-manager","title":"Register the agent with the NDK manager","text":"<p> Additional information</p> <p>Agent must be first registered with SR Linux by calling the <code>AgentRegister</code> method available on the returned <code>SdkMgrServiceClient</code> interface. The initial agent state is created during the registration process.</p>"},{"location":"ndk/guide/dev/go/#agents-context","title":"Agent's context","text":"<p>Go context is a required parameter for each RPC service method. Contexts provide the means of enforcing deadlines and cancellations as well as transmitting metadata within the request.</p> <p>During registration, SR Linux will be expecting a key-value pair with the <code>agent_name</code> key and a value of the agent's name passed in the context of an RPC. The agent name is defined in the agent's YAML file.</p> <p>Warning</p> <p>Not including this metadata in the agent <code>ctx</code> would result in an agent registration failure. SR Linux would not be able to differentiate between two agents both connected to the same NDK manager.</p> <pre><code>ctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\n// appending agent's name to the context metadata\nctx = metadata.AppendToOutgoingContext(ctx, \"agent_name\", \"ndkDemo\")\n</code></pre>"},{"location":"ndk/guide/dev/go/#agent-registration","title":"Agent registration","text":"<p><code>AgentRegister</code> method takes in the context <code>ctx</code> that is by now has agent name as its metadata and an <code>AgentRegistrationRequest</code>.</p> <p><code>AgentRegistrationRequest</code> structure can be passed in with its default values for a basic registration request.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\nr, err := client.AgentRegister(ctx, &amp;ndk.AgentRegistrationRequest{})\nif err != nil {\nlog.Fatalf(\"agent registration failed: %v\", err)\n}\n</code></pre> <p><code>AgentRegister</code> method returns <code>AgentRegistrationResponse</code> and an error. Response can be additionally checked for status and error description.</p>"},{"location":"ndk/guide/dev/go/#register-notification-streams","title":"Register notification streams","text":"<p> Additional information</p>"},{"location":"ndk/guide/dev/go/#create-subscription-stream","title":"Create subscription stream","text":"<p>A subscription stream needs to be created first before any of the subscription types can be added. <code>SdkMgrServiceClient</code> first creates the subscription stream by executing <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> only field <code>Op</code> set to a value of <code>const NotificationRegisterRequest_Create</code>. This effectively creates a stream which is identified with a <code>StreamID</code> returned inside the <code>NotificationRegisterResponse</code>.</p> <p><code>StreamId</code> must be associated when subscribing/unsubscribing to certain types of router notifications.</p> <pre><code>req := &amp;ndk.NotificationRegisterRequest{\nOp: ndk.NotificationRegisterRequest_Create,\n}\nresp, err := client.NotificationRegister(ctx, req)\nif err != nil {\nlog.Fatalf(\"Notification Register failed with error: %v\", err)\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\nr.log.Fatalf(\"Notification Register failed with status %d\", resp.GetStatus())\n}\nlog.Debugf(\"Notification Register was successful: StreamID: %d SubscriptionID: %d\", resp.GetStreamId(), resp.GetSubId())\n</code></pre>"},{"location":"ndk/guide/dev/go/#add-notification-subscriptions","title":"Add notification subscriptions","text":"<p>Once the <code>StreamId</code> is acquired, a client can register notifications of a particular type to be delivered over that stream.</p> <p>Different types of notifications types can be subscribed to by calling the same <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> having <code>Op</code> field set to <code>NotificationRegisterRequest_AddSubscription</code> and certain <code>SubscriptionType</code> selected.</p> <p>In the example below we would like to receive notifications from the <code>Config</code> service, hence we specify <code>NotificationRegisterRequest_Config</code> subscription type.</p> <pre><code>subType := &amp;ndk.NotificationRegisterRequest_Config{ // This is unique to each notification type (Config, Intf, etc.).\nConfig: &amp;ndk.ConfigSubscriptionRequest{},\n}\nreq := &amp;ndk.NotificationRegisterRequest{\nStreamId:          resp.GetStreamId(), // StreamId is retrieved from the NotificationRegisterResponse\nOp:                ndk.NotificationRegisterRequest_AddSubscription,\nSubscriptionTypes: subType,\n}\nresp, err := r.mgrStub.NotificationRegister(r.ctx, req)\nif err != nil {\nlog.Fatalf(\"Agent could not subscribe for config notification\")\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\nlog.Fatalf(\"Agent could not subscribe for config notification with status  %d\", resp.GetStatus())\n}\nlog.Infof(\"Agent was able to subscribe for config notification with status %d\", resp.GetStatus())\n</code></pre>"},{"location":"ndk/guide/dev/go/#streaming-notifications","title":"Streaming notifications","text":"<p> Additional information</p> <p>Actual streaming of notifications is a task for another service - <code>SdkNotificationService</code>. This service requires developers to create its own client, which is done with <code>NewSdkNotificationServiceClient</code> function.</p> <p>The returned <code>SdkNotificationServiceClient</code> interface has a single method <code>NotificationStream</code> that is used to start streaming notifications.</p> <p><code>NotificationsStream</code> is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request.</p> <p>To tell the server to start streaming notifications that were subscribed to before the <code>NewSdkNotificationServiceClient</code> executes <code>NotificationsStream</code> method where <code>NotificationStreamRequest</code> struct has its <code>StreamId</code> field set to the value that was obtained at subscription stage.</p> <pre><code>req := &amp;ndk.NotificationStreamRequest{\nStreamId: resp.GetStreamId(),\n}\nstreamResp, err := notifClient.NotificationStream(ctx, req)\nif err != nil {\nlog.Fatal(\"Agent failed to create stream client with error: \", err)\n}\n</code></pre>"},{"location":"ndk/guide/dev/go/#handle-the-streamed-notifications","title":"Handle the streamed notifications","text":"<p> Additional information</p> <p>Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is the pseudocode that illustrates the flow:</p> <pre><code>func HandleNotifications(stream ndk.SdkNotificationService_NotificationStreamClient) {\nfor { // loop until stream returns io.EoF\nnotification stream response (nsr) := stream.Recv()\nfor notif in nsr.Notification { // nsr.Notification is a slice of `Notification`\nif notif.GetConfig() is not nil {\n1. config notif = notif.GetConfig()\n2. handle config notif\n} else if notif.GetIntf() is not nil {\n1. intf notif = notif.GetIntf()\n2. handle intf notif\n} ... // Do this if statement for every notification type the agent is subscribed to\n}\n}\n}\n</code></pre> <p><code>NotificationStream</code> method of the <code>SdkNotificationServiceClient</code> interface will return a stream client <code>SdkNotificationService_NotificationStreamClient</code>.</p> <p><code>SdkNotificationService_NotificationStreamClient</code> contains a <code>Recv()</code> to retrieve notifications one by one. At the end of a stream <code>Rev()</code> will return <code>io.EOF</code>.</p> <p><code>Recv()</code> returns a <code>*NotificationStreamResponse</code> which contains a slice of <code>Notification</code>.</p> <p><code>Notification</code> struct has <code>GetXXX()</code> methods defined which retrieve the notification of a specific type. For example: <code>GetConfig</code> returns <code>ConfigNotification</code>.</p> <p>Note</p> <p><code>ConfigNotification</code> is returned only if <code>Notification</code> struct has a certain subscription type set for its <code>SubscriptionType</code> field. Otherwise, <code>GetConfig</code> returns <code>nil</code>.</p> <p>Once the specific <code>XXXNotification</code> has been extracted using the <code>GetXXX()</code> method, users can access the fields of the notification and process the data contained within the notification using <code>GetKey()</code> and <code>GetData()</code> methods.</p>"},{"location":"ndk/guide/dev/go/#exiting-gracefully","title":"Exiting gracefully","text":"<p>Agent needs to handle SIGTERM signal that is sent when a user invokes <code>stop</code> command via SR Linux CLI. The following is the required steps to cleanly stop the agent:</p> <ol> <li>Remove any agent's state if it was set using <code>TelemetryDelete</code> method of a Telemetry client.</li> <li>Delete notification subscriptions stream <code>NotificationRegister</code> method with <code>Op</code> set to <code>NotificationRegisterRequest_Delete</code>.</li> <li>Invoke use <code>AgentUnRegister()</code> method of a <code>SdkMgrServiceClient</code> interface.</li> <li>Close gRPC channel with the <code>sdk_mgr</code>.</li> </ol>"},{"location":"ndk/guide/dev/go/#logging","title":"Logging","text":"<p>To debug an agent, the developers can analyze the log messages that the agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at <code>/var/log/srlinux/stdout/</code> directory.</p> <p>The default SR Linux debug messages are found in the messages directory <code>/var/log/srlinux/buffer/messages</code>; check them when something went wrong within the SR Linux system (agent registration failed, IDB server warning messages, etc.).</p> <p>Logrus is a popular structured logger for Go that can log messages of different levels of importance, but developers are free to choose whatever logging package they see fit.</p> <ol> <li> <p>Make sure that you have set up the dev environment as explained on this page. Readers are also encouraged to first go through the gRPC basic tutorial to get familiar with the common gRPC workflows when using Go.\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/guide/dev/python/","title":"Developing agents with NDK in Python","text":"<p>This guide explains how to consume the NDK service when developers write the agents using Python<sup>1</sup>.</p> <p>Note</p> <p>This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter.</p> <p>Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations.</p> <p>In addition to the publicly available protobuf files, which define the NDK Service, Nokia also provides generated Python bindings for data access classes of NDK the <code>nokia/srlinux-ndk-py</code> repo. The generated module enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Python package themselves.</p>"},{"location":"ndk/guide/dev/python/#establish-grpc-channel-with-ndk-manager-and-instantiate-an-ndk-client","title":"Establish gRPC channel with NDK manager and instantiate an NDK client","text":"<p> Additional information</p> <p>To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux.</p> <p>This is done by passing the NDK server address - <code>localhost:50053</code> - to <code>grpc.Dial()</code> as follows:</p> <pre><code>import grpc\nchannel = grpc.insecure_channel(\"localhost:50053\")\n</code></pre> <p>Once the gRPC channel is setup, we need to instantiate a client (often called stub) to perform RPCs. The <code>sdk_common_pb2_grpc.SdkMgrServiceStub</code> method returns a <code>SdkMgrService</code> object</p> <pre><code>from ndk.sdk_common_pb2_grpc import SdkMgrServiceStub\nsdk_mgr_client = SdkMgrServiceStub(channel)\n</code></pre>"},{"location":"ndk/guide/dev/python/#register-the-agent-with-the-ndk-manager","title":"Register the agent with the NDK manager","text":"<p> Additional information</p> <p>Agent must be first registered with SR Linux by calling the <code>AgentRegister</code> method available on the returned <code>SdkMgrService</code> interface. The initial agent state is created during the registration process.</p>"},{"location":"ndk/guide/dev/python/#agents-metadata","title":"Agent's Metadata","text":"<p>During registration, SR Linux will be expecting a list of tuples with the <code>agent_name</code> item and value of the agent's name as the other item of the tuple. The agent name is defined in the agent's YAML file.</p> <pre><code>metadata = [(\"agent_name\", agent_name)]\n</code></pre>"},{"location":"ndk/guide/dev/python/#agent-registration","title":"Agent registration","text":"<p>The <code>AgentRegister</code> method takes two named arguments <code>request</code> and <code>metadata</code>. The <code>request</code> argument takes a <code>AgentRegistrationRequest</code> object and the metadata argument uses the previously defined metadata.</p> <pre><code>from ndk.sdk_service_pb2 import AgentRegistrationRequest\nfrom ndk.sdk_common_pb2 import SdkMgrStatus\nregister_request = AgentRegistrationRequest()\nregister_request.agent_liveliness = keepalive_interval # Optional\nresponse = sdk_mgr_client.AgentRegister(request=register_request, metadata=metadata)\nif response.status == SdkMgrStatus.kSdkMgrSuccess:\n# Agent has been registered successfully\npass\nelse:\n# Agent registration failed error string available as response.error_str\npass\n</code></pre> <p>The <code>AgentRegister</code> method returns a <code>AgentRegistrationResponse</code> object containing the status of the request as a <code>SdkMgrStatus</code> object, error message (if request failed) as a string and the app id as a integer.</p>"},{"location":"ndk/guide/dev/python/#register-notification-streams","title":"Register notification streams","text":"<p> Additional information</p>"},{"location":"ndk/guide/dev/python/#create-subscription-stream","title":"Create subscription stream","text":"<p>A subscription stream needs to be created first before any of the subscription types can be added. <code>SdkMgrService</code> first creates the subscription stream by executing <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> only field <code>op</code> set to a value of <code>NotificationRegisterRequest.Create</code>. This effectively creates a stream which is identified with a <code>stream_id</code> returned inside the <code>NotificationRegisterResponse</code>.</p> <p><code>stream_id</code> must be associated when subscribing/unsubscribing to certain types of router notifications.</p> <pre><code>from ndk.sdk_service_pb2 import NotificationRegisterRequest\nrequest = NotificationRegisterRequest(op=NotificationRegisterRequest.Create)\nresponse = sdk_mgr_client.NotificationRegister(request=request, metadata=metadata)\nif response.status == sdk_status.kSdkMgrSuccess:\n# Notification Register successful\nstream_id = response.stream_id\npass\nelse:\n# Notification Register failed, error string available as response.error_str\npass\n</code></pre> <p><code>stream_id</code> will be used in the Streaming notifications section.</p>"},{"location":"ndk/guide/dev/python/#add-notification-subscriptions","title":"Add notification subscriptions","text":"<p>Once the <code>stream_id</code> is acquired, a client can register notifications of a particular type to be delivered over that stream.</p> <p>Different types of notifications types can be subscribed to by calling the same <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> having <code>op</code> field set to <code>NotificationRegisterRequest.AddSubscription</code> and the correct name argument for the configuration type being added (<code>NotificationRegisterRequest</code> fields for the named arguments).</p> <p>In the example below we would like to receive notifications from the <code>Config</code> service, hence we specify the <code>config</code> argument with a <code>ConfigSubscriptionRequest</code> object.</p> <pre><code>from ndk.config_service_pb2 import ConfigSubscriptionRequest\nrequest = NotificationRegisterRequest(\nstream_id=stream_id,\nop=NotificationRegisterRequest.AddSubscription,\nconfig=ConfigSubscriptionRequest(),\n)\nresponse = sdk_mgr_client.NotificationRegister(request=request, metadata=metadata)\nif response.status == sdk_status.kSdkMgrSuccess:\n# Successful registration\npass\nelse:\n# Registration failed, error string available as response.error_str\npass\n</code></pre> <p>Info</p> <p>It is possible to register for multiple different types of notifications at the same time by passing different subscription requests to the same <code>NotificationRegisterRequest</code>.</p>"},{"location":"ndk/guide/dev/python/#streaming-notifications","title":"Streaming notifications","text":"<p> Additional information</p> <p>Actual streaming of notifications is a task for another service - <code>SdkNotificationService</code>. This service requires developers to create its own client, which is done with <code>SdkNotificationServiceStub</code> function.</p> <p>The returned <code>SdkNotificationService</code> has a single method <code>NotificationStream</code> that is used to start streaming notifications.</p> <p><code>NotificationsStream</code> is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request.</p> <p>The <code>stream_id</code> that was returned in the Create subscription stream is used to tell the server to included the notifications that were created between when the <code>SdkNotificationService</code> was created and when its <code>NotificationsStream</code> method is invoked.</p> <pre><code>stream_request = NotificationStreamRequest(stream_id=stream_id)\nstream_response = sdk_notification_client.NotificationStream(\nrequest=stream_request, metadata=metadata\n)\nfor response in stream_response:\nfor notification in response.notification:\n# Handle notifications\npass\n</code></pre>"},{"location":"ndk/guide/dev/python/#handle-the-streamed-notifications","title":"Handle the streamed notifications","text":"<p> Additional information</p> <p>Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is a method that checks for all notification types and delegates handling to helper methods.</p> <pre><code>from ndk.sdk_service_pb2 import Notification\ndef handle_notification(notification: Notification) -&gt; None:\n# Field names are available on the Notification documentation page\nif notification.HasField(\"config\"):\nhandle_ConfigNotification(notification.config)\nif notification.HasField(\"intf\"):\nhandle_InterfaceNotification(notification.intf)\nif notification.HasField(\"nw_inst\"):\nhandle_NetworkInstanceNotification(notification.nw_inst)\nif notification.HasField(\"lldp_neighbor\"):\nhandle_LldpNeighborNotification(notification.lldp_neighbor)\nif notification.HasField(\"bfd_session\"):\nhandle_BfdSessionNotification(notification.bfd_session)\nif notification.HasField(\"route\"):\nhandle_IpRouteNotification(notification.route)\nif notification.HasField(\"appid\"):\nhandle_AppIdentNotification(notification.appid)\nif notification.HasField(\"nhg\"):\nhandle_NextHopGroupNotification(notification.nhg)\n</code></pre> <p>A <code>Notification</code> object has a <code>HasField()</code> method that allows to check if the field contains a notification. Once it is confirmed that <code>XXXXX</code> field is present we can access it as attribute of the notification (<code>notification.XXXXX</code>) this will return a notification of the associated type (for example accessing <code>notification.config</code> returns a <code>ConfigNotification</code>).</p> <p>Note</p> <p>It is essential to verify if the notification has a given field with the <code>HasField()</code> method as accessing an invalid field will give an empty notification. The value will not be <code>None</code> and the accessing the invalid field will not throw an Exception.</p>"},{"location":"ndk/guide/dev/python/#exiting-gracefully","title":"Exiting gracefully","text":"<p>Agent needs to handle SIGTERM signal that is sent when a user invokes <code>stop</code> command via SR Linux CLI. The following is the required steps to cleanly stop the agent:</p> <ol> <li>Remove any agent's state if it was set using <code>TelemetryDelete</code> method of a Telemetry client.</li> <li>Delete notification subscriptions stream using <code>NotificationRegisterRequest</code> with <code>op</code> set to <code>Delete</code></li> <li>Invoke use <code>AgentUnRegister()</code> method of the <code>SdkMgrService</code> object.</li> <li>Close gRPC channel with the <code>sdk_mgr</code> (<code>channel.close()</code>).</li> </ol>"},{"location":"ndk/guide/dev/python/#logging","title":"Logging","text":"<p>To debug an agent, the developers can analyze the log messages that the agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at <code>/var/log/srlinux/stdout/</code> directory.</p> <p>The default SR Linux debug messages are found in the messages directory <code>/var/log/srlinux/buffer/messages</code>; check them when something went wrong within the SR Linux system (agent registration failed, IDB server warning messages, etc.).</p> <ol> <li> <p>Make sure that you have set up the dev environment as explained on this page.\u00a0\u21a9</p> </li> </ol>"},{"location":"ndk/guide/env/go/","title":"Go Development Environment","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide recommendations for a Go toolchain setup suitable for the development and build of NDK applications.</p>"},{"location":"ndk/guide/env/go/#environment-components","title":"Environment components","text":"<p>The toolchain that can be used to develop and build Go-based NDK apps consists of the following components:</p> <ol> <li>Go programming language - Go compiler, toolchain, and standard library</li> <li>Go NDK bindings - generated data access classes for gRPC based NDK service.</li> <li>Goreleaser - Go-focused build &amp; release pipeline runner. Packages nFPM to produce rpm packages that can be used to install NDK agents.</li> </ol>"},{"location":"ndk/guide/env/go/#project-structure","title":"Project structure","text":"<p>It is recommended to use Go modules when developing applications with Go. Go modules allow for better dependency management and can be placed outside the <code>$GOPATH</code> directory.</p> <p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 .goreleaser.yml          # Goreleaser config file\n\u251c\u2500\u2500 main.go                  # Package main that calls agent logic\n\u251c\u2500\u2500 go.mod                   # Go mod file\n\u251c\u2500\u2500 go.sum                   # Go sum file\n</code></pre>"},{"location":"ndk/guide/env/go/#ndk-language-bindings","title":"NDK language bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Go program the language bindings have to be generated from the source proto files.</p> <p>Nokia not only provides the proto files for the SR Linux NDK service but also NDK Go language bindings.</p> <p>With the provided Go bindings, the NDK can be imported in a Go project like that:</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/ndk\"\n</code></pre>"},{"location":"ndk/guide/env/python/","title":"Python Development Environment","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide some recommendations for a Python toolchain setup suitable for the development of NDK applications.</p>"},{"location":"ndk/guide/env/python/#environment-components","title":"Environment components","text":"<p>The toolchain that can be used to develop Python-based NDK apps consists of the following components:</p> <ol> <li>Python programming language - Python interpreter, toolchain, and standard library. Python2 is not supported.</li> <li>Python NDK bindings - generated data access classes for gRPC based NDK service.</li> </ol>"},{"location":"ndk/guide/env/python/#project-structure","title":"Project structure","text":"<p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 main.py                  # Package main that calls agent logic\n\u251c\u2500\u2500 requirements.txt         # Python packages required by the app logic\n</code></pre>"},{"location":"ndk/guide/env/python/#ndk-language-bindings","title":"NDK language bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. The language bindings have to be generated from the source proto files to use gRPC services in a Python program.</p> <p>Nokia provides both the proto files for the SR Linux NDK service and also NDK Python language bindings.</p> <p>With the provided Python bindings, the NDK can be installed with <code>pip</code></p> <pre><code># it is a good practice to use virtual env\nsudo python3 -m venv /opt/myApp/venv\n\n# activate the newly created venv\nsource /opt/myApp/venv/bin/activate\n\n# update pip/setuptools in the venv\npip3 install -U pip setuptools\n\n# install the latest pip package of the NDK\npip install srlinux-ndk # (1)\n</code></pre> <ol> <li>To install a specific version of the NDK check the NDK install instructions on the NDK github repo.</li> </ol> <p>Once installed, NDK services are imported in a Python project like that:</p> <pre><code>from ndk import appid_service_pb2 # (1)\n</code></pre> <ol> <li>The example is provided for <code>appid_service_pb2</code> service but every service is imported the same way.</li> </ol>"},{"location":"programmability/","title":"Programmability","text":"<ul> <li> <p> YANG</p> <p>Nokia SR Linux was ground-up designed with YANG data models taking a central role. Full YANG coverage for every component of SR Linux<sup>1</sup> powers model-driven management interfaces and enables unprecedented automation capabilities.</p> <p> Continue</p> </li> <li> <p> NetOps Development Kit (NDK)</p> <p>The NDK enables operators to integrate their own and third-party applications into the system with all the same benefits as Nokia applications.</p> <p> Reference</p> </li> <li> <p> Customizable CLI</p> <p>An advanced, Python-based CLI provides a flexible framework for accessing the system\u2019s underlying data models.</p> <p>Operators can leverage CLI plugins to customize the way CLI looks, feels, and reacts.</p> <p> Article coming soon...</p> </li> <li> <p> Go API</p> <p>Up your automation workflows using Go API<sup>2</sup> for SR Linux and leverage type hinting, compile-time validation and full schema conformance.</p> <p>Time to leave confguration templating at runtime in the past!</p> <p> Experiment</p> </li> <li> <p> Event Handler</p> <p>Event handler is a framework that enables SR Linux to react to specific system events, using programmable logic to define the actions taken in response to the events.</p> <p>Couple the fully-modelled configuration and state datastores with a versatile and simple Python language and you get a powerful automation execution framework running on the NOS.</p> <p> Checkout examples</p> </li> <li> <p> NAPALM</p> <p>Familiar with a multi-vendor network automation interface that spits fire?</p> <p>With <code>napalm-srlinux</code> community driver we plug in the NAPALM ecosystem using gNMI as the underlying management interface.</p> <p> Try yourself</p> </li> <li> <p> Ansible</p> <p>Does your networking team rely on Ansible for network automation, and you'd rather continue using it with SR Linux fabric? We have you covered with Ansible collection developed for SR Linux fully modelled management interfaces.</p> <p> Documentation</p> </li> <li> <p> Screen scraping</p> <p>Fully modeled, structured, and performant interfaces is not particularly your cup of tea? No worries, we added plugins for the two most popular screen craping libraries:</p> <ul> <li>Scrapli (py, go)</li> <li>Netmiko</li> </ul> <p> Tutorials coming soon...</p> </li> </ul> <ol> <li> <p>Including NDK applications written by users.\u00a0\u21a9</p> </li> <li> <p>Generated with openconfig/ygot.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Learning by doing is not only the most effective method but also an extremely fun one.</p> <p>The hands-on tutorials we provide in this section are designed in such a way that anyone can launch them</p> <ul> <li>at absolutely no cost</li> <li>whenever they want it</li> <li>whatever machine they have</li> <li>and run it for as long as required</li> </ul> <p>The tutorials use the open-source containerlab project to deploy the lab environment with all the needed components. This ensures that both the tutorial authors and the readers work in the same environment. No more second-guessing why the tutorial's outputs differ from yours!</p>"},{"location":"tutorials/infrastructure/kne/","title":"SR Linux with KNE","text":"<p>For easy-to-spin personal network labs, we have open-sourced containerlab project, which many companies and individuals use with and without SR Linux. The simplicity and user-friendliness of containerlab, while being the key ingredients of its success, also bear some limitations. For example, multi-node topologies are not yet possible with containerlab, which means that your lab size is limited by the resources your containerlab host has.</p> <p>Today, Kubernetes is often seen as a de facto standard container orchestration system that enables horizontal scaling of applications. Thanks to the KNE (Kubernetes Network Emulation) project, it is now possible to leverage Kubernetes'es extensibility, programmability, and scalability and deploy networking labs using the Kubernetes backend.</p> <p>Following this tutorial, you will learn how to deploy the Nokia SR Linux node using KNE in different deployment scenarios.</p>","tags":["kne","openconfig"]},{"location":"tutorials/infrastructure/kne/installation/","title":"Installation","text":"<p>To start deploying labs orchestrated by KNE a user needs to install <code>kne</code> command line utility and have a k8s cluster available. Follow KNE setup instructions to install <code>kne</code> and its dependencies.</p> <p>Versions</p> <p>We used the following components and their versions in this tutorial:</p> <ul> <li><code>kne v0.1.9</code><sup>1</sup></li> <li><code>kind v0.17.0</code><sup>2</sup></li> </ul> <p>By following the setup instructions, you should have the following utilities successfully installed:</p> knekind <pre><code>\u276f kne help\nKubernetes Network Emulation CLI.  Works with meshnet to create\nlayer 2 topology used by containers to layout networks in a k8s\nenvironment.\n\nUsage:\nkne [command]\n--snip--\n</code></pre> <pre><code>\u276f kind version\nkind v0.17.0 go1.19.2 linux/amd64\n</code></pre>"},{"location":"tutorials/infrastructure/kne/installation/#cluster-deployment","title":"Cluster deployment","text":"<p>Once the necessary utilities are installed, proceed with the KNE cluster installation. KNE cluster consists of the following high-level components:</p> <ul> <li>Kind cluster: A kind-based k8s cluster to allow automated deployment.</li> <li>Load Balancer service: An Load Balancer service used in the KNE cluster to allow for external access to the nodes. Supported LB services: MetalLB.</li> <li>CNI: configuration of a CNI plugin used in the KNE cluster to layout L2 links between the network nodes deployed in a cluster. Supported CNI plugins: meshnet-cni.</li> <li>External controllers: an optional list of external controllers that manage custom resources.</li> </ul> <p>KNE provides a cluster manifest file (aka \"deployment file\") along with the command to install cluster components using <code>kne deploy</code> command<sup>3</sup>.</p> <p>Warning</p> <p>Deployment file contains <code>controllers</code> section that enables automated installation of external controllers, such as srl-controller. KNE pins particular versions of external controllers to guarantee compatibility between the KNE and controller layers. For example, KNE v0.1.9 deploys srl-controller v0.5.0. If a user wants to use a different version of a controller, they need to remove the controller from the <code>controllers</code> list and install it manually.</p> <p>Using <code>kne deploy</code> and following the cluster deployment instructions, cluster installation boils down to a single command:</p> <pre><code>kne deploy deploy/kne/kind-bridge.yaml\n</code></pre> <p>The deployment process should finish without errors, stating that every component of a KNE cluster has been deployed successfully. At this point, it is helpful to check that the cluster and its components are healthy.</p> kind clusterCNILoad Balancer <p>Ensure that a kind cluster named <code>kne</code> is active.</p> <pre><code>\u276f kind get clusters\nkne\n</code></pre> <p>Check that <code>kubectl</code> is configured to work with <code>kne</code> cluster:</p> <pre><code>\u276f kubectl config current-context\nkind-kne\n</code></pre> <p>Ensure that <code>meshnet</code> CNI is running as a daemonset:</p> <pre><code>\u276f kubectl get daemonset -n meshnet\nNAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR              AGE\nmeshnet   1         1         1       1            1           kubernetes.io/arch=amd64   8m55s\n</code></pre> <p>Ensure that MetalLB Load Balancer is running controller deployment and speaker daemonset:</p> <pre><code>\u276f kubectl get pod -n metallb-system\nNAME                          READY   STATUS    RESTARTS   AGE\ncontroller-55d86f5f7c-bl9kx   1/1     Running   0          12m\nspeaker-zsj29                 1/1     Running   0          11m\n</code></pre>"},{"location":"tutorials/infrastructure/kne/installation/#sr-linux-controller","title":"SR Linux controller","text":"<p>SR Linux controller manages SR Linux containers deployment on top of the KNE clusters and provides the necessary APIs for KNE to deploy SR Linux nodes as part of the network topology. It is automatically installed by the KNE CLI tool.</p> Installing SR Linux controller manually <p>SR Linux controller is an open-source project hosted at  srl-labs/srl-controller repository and can be easily installed on a k8s cluster as per its installation instructions, for example to test a version that was not yet releases or adopted by KNE:</p> <pre><code>kubectl apply -k https://github.com/srl-labs/srl-controller/config/default\n</code></pre> <p>Additional controllers can be installed by following the respective installation instructions provided in the KNE documentation.</p> <p>When <code>srl-controller</code> is installed successfully, it can be seen in its namespace as a deployment:</p> <pre><code>\u276f kubectl get deployments -n srlinux-controller\nNAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\nsrlinux-controller-controller-manager   1/1     1            1           12m\n</code></pre>"},{"location":"tutorials/infrastructure/kne/installation/#license","title":"License","text":"<p>If a user intends to run a topology with chassis-based SR Linux nodes<sup>4</sup>, they must install a valid license.</p> <p>The same lab can be used with unlicensed IXR-D/H variants; to adapt the lab to unlicensed SR Linux variants users need to:</p> <ol> <li>delete <code>model: \"ixr6e\"</code> string from the KNE topology file</li> <li> <p>remove the openconfig configuration blob from the startup-config file</p> remove this blob<pre><code>\"management\": {\n\"srl_nokia-openconfig:openconfig\": {\n\"admin-state\": \"enable\"\n}\n}\n</code></pre> </li> </ol>"},{"location":"tutorials/infrastructure/kne/installation/#image-load","title":"Image load","text":"<p>In the case of a <code>kind</code> cluster, it is advised to load container images to the kind cluster preemptively. Doing so will ensure that necessary images are present in the cluster when KNE creates network topologies.</p> <p>To load srlinux container image to the kind cluster:</p> <pre><code>kind load docker-image ghcr.io/nokia/srlinux:22.11.2 --name kne\n</code></pre> <ol> <li> <p>The tutorial is based on this particular release, but newer releases might work as well.\u00a0\u21a9</p> </li> <li> <p>For this tutorial, we leverage kind (Kubernetes in Docker) to stand up a personal k8s installation. Using kind is not a hard requirement but merely an easy and quick way to get a personal k8s cluster.\u00a0\u21a9</p> </li> <li> <p>Users are free to install cluster components manually. <code>kne deploy</code> aims to automate the prerequisites installation using the tested configurations.\u00a0\u21a9</p> </li> <li> <p>Hardware types ixr6/10, ixr-6e/10e\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/infrastructure/kne/topology/","title":"KNE Topology","text":"<p>Everything is ready for KNE users to create network topologies when installation steps are out of the way. KNE offers a declarative way of defining network topologies using a topology file that captures the state of a topology. Topology message in the <code>topo.proto</code> file defines the schema that the KNE topology follows. Consult with the schema to see which fields a topology can have.</p> <p>KNE topology file can be provided in the following formats:</p> <ol> <li>Prototext<sup>1</sup> - original format of a KNE topology.</li> <li>YAML file - an additional format supported by KNE which is converted to prototext.</li> </ol> <p>Both prototext and YAML files offer the same functionality; given the dominance of the prototext format in the kne repository, we will use this format in the tutorial.</p> <p>Tip</p> <p>The parts of the topology used in this section are taken from the <code>2node-srl-ixr6-with-oc-services.pbtxt</code> topology file hosted at the kne repository.</p>"},{"location":"tutorials/infrastructure/kne/topology/#topology-name","title":"Topology Name","text":"<p>As with most configuration elements, a network topology is identified by a <code>name</code> property that must be unique for each deployed lab, as it will create a namespace lab's resources.</p> <pre><code>name: \"2-srl-ixr6\"\n</code></pre> <p>The name is an arbitrary string.</p>"},{"location":"tutorials/infrastructure/kne/topology/#node","title":"Node","text":"<p>The main constituents of a KNE topology are nodes and links between them. In the topology file, each node is defined as a repeated element of the <code>nodes</code> message:</p> nodes definition in a topology file (prototext format)<pre><code>nodes: {\n// first node parameters\n}\nnodes: {\n// second node parameters\n}\n</code></pre> <p>Node definition has quite some parameters<sup>2</sup>. We will cover the most common of them.</p>"},{"location":"tutorials/infrastructure/kne/topology/#name","title":"Name","text":"<p>Each node must have a name, which is a free-formed string:</p> <pre><code>nodes: {\nname: \"srl1\"\n// other node parameters snipped for brevity\n}\n</code></pre>"},{"location":"tutorials/infrastructure/kne/topology/#vendor","title":"Vendor","text":"<p>The vendor field is provided to let KNE know which vendor is defined within the node section. KNE supports several vendors; the full list is provided in the topo.proto file.</p> <pre><code>nodes: {\nvendor: NOKIA // (1)!\n// other node parameters snipped for brevity\n}\n</code></pre> <ol> <li>Note, the vendor value needs to be provided exactly as defined in the Vendor enum field of the proto file. Without quotes.</li> </ol> <p>Note</p> <p>Some KNE examples may utilize the <code>type</code> parameter with a value of <code>NOKIA_SRL</code> or similar. This field will be deprecated in favor of the separate fields: <code>vendor</code>/<code>model</code>/<code>os</code>/<code>version</code>.</p> <p>The vendor field must be set to <code>NOKIA</code> when working with Nokia SR Linux nodes.</p>"},{"location":"tutorials/infrastructure/kne/topology/#model","title":"Model","text":"<p>With the <code>model</code> field a user indicates which particular model of a given Vendor should be used by the node. In the context of Nokia SR Linux, the <code>model</code> field drives the hardware variant that an SR Linux container will emulate.</p> <pre><code>nodes: {\nmodel: \"ixrd3l\"\n// other node parameters snipped for brevity\n}\n</code></pre>"},{"location":"tutorials/infrastructure/kne/topology/#config","title":"Config","text":"<p>Parameters that configure the way a k8s pod representing a network node is deployed are grouped under the Config message.</p>"},{"location":"tutorials/infrastructure/kne/topology/#image","title":"Image","text":"<p>The essential field under the Config block is <code>image</code>. It sets the container image name that k8s will use in the pod specification.</p> <pre><code>nodes: {\nconfig:{\nimage: \"ghcr.io/nokia/srlinux:22.6.4\"\n}\n// other node parameters snipped for brevity\n}\n</code></pre> <p>Note</p> <ol> <li>In the <code>2node-srl-ixr6-with-oc-services.pbtxt</code> the image is omitted, as the intention there to use the latest available image, which is a default value for the <code>image</code> field.</li> <li>When <code>kind</code> cluster is used, users might want to load the container image before creating the topologies.</li> </ol>"},{"location":"tutorials/infrastructure/kne/topology/#file","title":"File","text":"<p>Often it is desired to deploy a node with a specific startup configuration applied. KNE enables this use case by using the <code>file</code> parameter of a <code>Config</code> message. A path to the startup configuration file is provided using the path relative to the topology file.</p> <pre><code>nodes: {\nconfig:{\nfile: \"my-startup-config.json\"\n}\n// other node parameters snipped for brevity\n}\n</code></pre> <p>In the snippet above, the <code>my-stratup-config.json</code> is expected to be found next to the topology file.</p> <p>For SR Linux nodes, the startup file must contain the full configuration of a node in a JSON format as found in <code>/etc/opt/srlinux/config.json</code> on SR Linux filesystem.</p>"},{"location":"tutorials/infrastructure/kne/topology/#tls-certificates","title":"TLS Certificates","text":"<p>KNE lets users indicate if they want the network nodes to generate self-signed certificates upon boot. The following configuration blob instructs a node to generate a self-signed certificate with a name <code>kne-profile</code> and a key size of <code>4096</code>.</p> <pre><code>nodes: {\nconfig: {\ncert: {\nself_signed: {\ncert_name: \"kne-profile\",\nkey_name: \"N/A\",\nkey_size: 4096,\n}\n}\n}\n}\n</code></pre> <p>Under the hood, SR Linux node will execute the <code>tools system tls generate-self-signed</code> command with the appropriate key size and save the TLS artifacts under the TLS server-profile context.</p> <p>Note</p> <p>Since on SR Linux it is possible to embed TLS artifacts in the config file itself, you may often see labs where the startup-config files are already populated with the TLS configuration.</p>"},{"location":"tutorials/infrastructure/kne/topology/#services","title":"Services","text":"<p>Applications deployed on Kubernetes are not accessible outside the cluster until an Ingress or Load Balancer service is configured to enable that connectivity. Consequently, network elements deployed by KNE have their management services available internally within the cluster, but not from the outside. For the users of the virtual network labs, it is imperative to have external connectivity to the management services running on the nodes to manage the virtual network. In KNE external network connectivity is enabled by the MetalLB Load Balancer service and a particular configuration block in the Node specification.</p> <pre><code>nodes: {\n// other node parameters snipped for brevity\nservices:{\nkey: 22\nvalue: {\nname: \"ssh\"\ninside: 22\noutside: 22\n}\n}\nservices:{\nkey: 9339\nvalue: {\nname: \"gnmi\"\ninside: 57400 // (1)!\noutside: 9339\n}\n}\n}\n</code></pre> <ol> <li>gNMI service running on port <code>57400</code> will be accessible externally by port <code>9339</code> as specified in the <code>outside</code> field.</li> </ol> <p>The above snippet enables SSH and gNMI services to be available outside the k8s cluster.</p> <p>The <code>key</code> field is a unique integer identifier in the map of services; it is typically set to the outside port of the exposed service.</p> <p>Within the <code>value</code> block, a user specifies the service parameters:</p> <ul> <li><code>name</code>: a free-formed string describing the service.</li> <li><code>inside</code>: a port number that the service is running on a network node.</li> <li><code>outside</code>: a port number, which will be configured on a Load Balancer and mapped to the <code>internal</code> port. This mapping effectively enables external access to the service.</li> </ul> <p>Courtesy of the MetalLB Load Balancer service, the defined services will be exposed using the IP addresses from your cluster network using the port mappings as defined in the <code>services</code> portion of the node specification.</p> Management services exposed by Load Balancer<pre><code>\u276f kubectl get svc -n 2-srl-ixr6\nNAME           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE\nservice-srl1   LoadBalancer   10.96.72.99     172.18.0.50   22:30281/TCP,57400:30333/TCP   2m7s # (1)!\nservice-srl2   LoadBalancer   10.96.135.142   172.18.0.51   57400:31443/TCP,22:30266/TCP   2m6s\n</code></pre> <ol> <li>SSH service of <code>srl1</code> node is accessible externally via <code>172.18.0.50:22</code>     gNMI service of <code>srl1</code> node is accessible externally via <code>172.18.0.50:57400</code></li> </ol>"},{"location":"tutorials/infrastructure/kne/topology/#links","title":"Links","text":"<p>With <code>links</code> object of a topology, users wire up the nodes together. The link is defined as a pair <code>a_node/a_int &lt;--&gt; z_int/z_node</code>.</p> <pre><code>links: {\na_node: \"srl1\"\na_int: \"e1-1\"\nz_node: \"srl2\"\nz_int: \"e1-1\"\n}\n</code></pre> <p>The above <code>links</code> object creates a Layer2 virtual wire between the nodes <code>srl1</code> and <code>srl2</code> using the interface names <code>e1-1</code> on both ends.</p> <pre><code>  a_node                       z_node\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 a_int          z_int\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502      \u251c\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2524      \u2502\n \u2502 srl1 \u2502e1-1 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524e1-1 \u2502 srl2 \u2502\n \u2502      \u251c\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2524      \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <p>Pay attention to the interface name specified for SR Linux nodes. Containerized SR Linux node uses <code>eX-Y</code> notation for its network interfaces where <code>X</code> - linecard number <code>Y</code> - port number</p> <p>Example: <code>e1-1</code> interface is mapped to <code>ethernet-1/1</code> interface of SR Linux which is a first port on a first linecard.</p>"},{"location":"tutorials/infrastructure/kne/topology/#interfaces","title":"Interfaces","text":"<p>The link name provided in the links section of the topology defines a name of a Linux interface created in the network namespace of a particular pod. However, this name rarely matches the interface name used by the Network OS.</p> <p>For example, for Nokia SR Linux, the Linux interface notation must follow <code>eX-Y</code> schema, but when configuring these interfaces over any management protocol, users should use <code>ethernet-X/Y</code> form. Since such mapping is different between vendors, KNE users can provide the mapping in the topology file to let external systems know which Linux interface name maps to which internal name.</p> <pre><code>nodes: {\n// other node parameters snipped for brevity\ninterfaces: {\nkey: \"e1-1\"\nvalue: {\nname: \"ethernet-1/1\"\n}\n}\n</code></pre> <p>Note</p> <p>It is not mandatory to provide interface mapping information if no external system that needs to know this mapping will be used.</p> <ol> <li> <p>https://developers.google.com/protocol-buffers/docs/text-format-spec \u21a9</p> </li> <li> <p>full specification of a Node element is contained in the topology proto file.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/","title":"SR Linux with Openconfig services","text":"Summary Tutorial name SR Linux with KNE Lab components 2 Nokia SR Linux nodes Resource requirements  2 vCPU  4 GB Lab kne/examples/srlinux/2node-srl-ixr6-with-oc-services.pbtxt Main ref documents kne documentation Version information<sup>1</sup> <code>kne v0.1.9</code>, <code>srlinux:22.11.2</code>, <code>srl-controller:0.5.0</code>, <code>kind:0.17.0</code> Authors Roman Dodin  <p>KNE repository contains a set of example topologies that aim to help new users get started with using KNE to orchestrate virtual network labs. SR Linux team maintains several examples, which include SR Linux nodes.</p> <p>This chapter explains the details behind the <code>2node-srl-ixr6-with-oc-services.pbtxt</code> example topology.</p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#topology-diagram","title":"Topology diagram","text":"<p>The lab topology aims to introduce KNE users to labs with Nokia SR Linux nodes and acquaint them with Openconfig services running on SR Linux. Two Nokia SR Linux nodes connected over their <code>ethernet-1/1</code> interfaces form a topology of this lab.</p> <pre><code>  a_node                       z_node\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 a_int          z_int\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502      \u251c\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2524      \u2502\n \u2502 srl1 \u2502e1-1 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524e1-1 \u2502 srl2 \u2502\n \u2502      \u251c\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2524      \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Both nodes are configured to emulate IXR-6e chassis-based hardware and run SR Linux v22.6.3.</p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#deployment","title":"Deployment","text":"<p>To deploy this topology, users should complete the following pre-requisite steps:</p> <ol> <li>Install KNE</li> <li>Install SR Linux controller</li> <li>Install SR Linux license<sup>2</sup></li> </ol> <p>Once prerequisites are satisfied, topology deployment is just a single command:</p> <pre><code>kne create examples/nokia/srlinux-services/2node-srl-ixr6-with-oc-services.pbtxt\n</code></pre> <p>When the topology creation succeeds, the final log message <code>Topology \"2-srl-ixr6\" created</code> is displayed.</p> <p>A Kubernetes namespace is created matching the lab name <code>2-srl-ixr6</code>, and lab components are placed in that namespace. To verify lab deployment status, a user can invoke the following command and ensure that the pods are in running state.</p> <pre><code>\u276f kubectl get pods -n 2-srl-ixr6 NAME   READY   STATUS    RESTARTS   AGE\nsrl1   1/1     Running   0          15h\nsrl2   1/1     Running   0          15h\n</code></pre> <p>The above command confirms that the two nodes specified in the topology files are in running state.</p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#configuration","title":"Configuration","text":"<p>Topology file utilizes startup configuration provided in a separate file. This startup configuration contains configuration for essential management and Openconfig services.</p> <p>As a result of this startup config, the nodes come up online with these services in an already operational state.</p> How to configure Openconfig services via CLI <p>In order to configure the below mentioned Openconfig services users can spin up a single-node SR Linux topology with containerlab and enter the follwoing commands in the CLI:</p> <pre><code>enter candidate\n/system tls\nreplace \"server-profile clab-profile\" with \"server-profile kne-profile\"\nset / system gnmi-server network-instance mgmt tls-profile kne-profile\nset / system json-rpc-server network-instance mgmt https tls-profile kne-profile\nset / system management openconfig admin-state enable\nset / system gnmi-server network-instance mgmt yang-models openconfig\nset / system gribi-server admin-state enable network-instance mgmt admin-state enable tls-profile kne-profile\nset / network-instance mgmt protocols gribi admin-state enable\nset / system p4rt-server admin-state enable network-instance mgmt admin-state enable tls-profile kne-profile\n</code></pre>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#tls-certificate","title":"TLS certificate","text":"<p>A generated TLS profile is present in the configuration and can be found by <code>/system tls server-profile kne-profile</code> path. This server profile named <code>kne-profile</code> contains a TLS certificate and a key. This server profile is used by a number of SR Linux management services that require TLS-enabled security.</p> <p>Note, that the certificate present in a lab is shared between both nodes and contains invalid CN and SAN values. Therefore, it won't be possible to verify the certificate offered by the lab nodes, and tools should skip certificate verification.</p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#services","title":"Services","text":"<p>Essential management and Openconfig services are provided in the startup configuration file utilized by this lab. In the following sections, we explain how to verify the operational status of those services.</p> <p>Services enabled on SR Linux nodes running in this lab are made available externally by the MetalLB Load Balancer and the corresponding services configuration blob in the topology file.  </p> <p>Tip</p> <p>To list ports that available externally use:</p> <p><pre><code>\u276f kubectl get svc -n 2-srl-ixr6 NAME           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE\nservice-srl1   LoadBalancer   10.96.72.99     172.18.0.50   22:30281/TCP,57400:30333/TCP   15h\nservice-srl2   LoadBalancer   10.96.135.142   172.18.0.51   57400:31443/TCP,22:30266/TCP   15h\n</code></pre> Access to the services is done via <code>External-IP</code> and the corresponding port number. For example, SSH service on <code>srl1</code> node is available by the <code>172.18.0.50:22</code> socket.</p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#ssh","title":"SSH","text":"<p>An SSH service is enabled on both SR Linux nodes and is exposed via port <code>22</code>. Users can access SSH using the <code>External-IP</code> for a matching service and port <code>22</code>.</p> Example <p>Credentials: <code>admin:NokiaSrl1!</code> <pre><code>\u276f ssh admin@172.18.0.50\nWarning: Permanently added '172.18.0.50' (ECDSA) to the list of known hosts.\n................................................................\n:                  Welcome to Nokia SR Linux!                  :\n:              Open Network OS for the NetOps era.             :\n:                                                              :\n:    This is a freely distributed official container image.    :\n:                      Use it - Share it                       :\n:                                                              :\n: Get started: https://learn.srlinux.dev                       :\n: Container:   https://go.srlinux.dev/container-image          :\n: Docs:        https://doc.srlinux.dev/22-6                    :\n: Rel. notes:  https://doc.srlinux.dev/rn22-6-1                :\n: YANG:        https://yang.srlinux.dev/v22.6.1                :\n: Discord:     https://go.srlinux.dev/discord                  :\n: Contact:     https://go.srlinux.dev/contact-sales            :\n................................................................\n\nadmin@172.18.0.50's password:\n</code></pre></p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#console","title":"Console","text":"<p>To get console-like access to SR Linux NOS users should leverage <code>kubectl exec</code> command and start the <code>sr_cli</code> process:</p> <pre><code>kubectl -n 2-srl-ixr6 exec -it srl1 -- sr_cli # (1)!\n</code></pre> <ol> <li> <ul> <li>Namespace <code>2-srl-ixr6</code> matches the lab name set in the topology file  </li> <li><code>srl1</code> container name matches the node name set in the topology file.</li> </ul> </li> </ol> Example <pre><code>\u276f kubectl -n 2-srl-ixr6 exec -it srl1 -- sr_cli\nDefaulted container \"srl1\" out of: srl1, init-srl1 (init)\nUsing configuration file(s): ['/etc/opt/srlinux/srlinux.rc']\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl1#\n</code></pre>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#openconfig","title":"Openconfig","text":"<p> Openconfig docs</p> <p>By default, Nokia SR Linux uses native YANG models. Openconfig YANG models are already enabled in the configuration file used in this lab.</p> <p>For completeness, the below section shows how to enable Openconfig via different management interfaces.</p> CLIConfig file <pre><code>--{ running }--[  ]--\nA:srl# enter candidate\n\n--{ candidate shared default }--[  ]--\nA:srl# system management openconfig admin-state enable\n\n--{ * candidate shared default }--[  ]--\nA:srl# commit stay \nAll changes have been committed. Starting new transaction.\n</code></pre> <pre><code>\"srl_nokia-system:system\": {\n\"management\": {\n\"srl_nokia-openconfig:openconfig\": {\n\"admin-state\": \"enable\"\n}\n},\n// other system containers\n}\n</code></pre>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#gnmi","title":"gNMI","text":"<p> gNMI docs</p> <p>gNMI service is enabled over port <code>57400</code> in the configuration file used with this lab and exposed by the cluster's LoadBalancer over <code>9339</code> port for external connectivity.</p> <p>By default, gNMI instance configured in the <code>mgmt</code> network instance uses native YANG models. This is driven by the default configuration value of the <code>/system/gnmi-server/network-instance[name=mgmt]/yang-models</code> leaf and selects which models are going to be used when gNMI paths are provided without the <code>origin</code> information in the path.</p> <p>Startup configuration file used in this lab has the <code>yang-models</code> leaf set to <code>openconfig</code>. This makes the paths without the <code>origin</code> value to be treated as openconfig paths.</p> Example <p>gNMI service can be tested using gnmic cli client.</p> CapabilitiesGet using native YANG modelsGet using Openconfig YANG models <pre><code>\u276f gnmic -a 172.18.0.50:9339 -u admin -p NokiaSrl1! --skip-verify capabilities\ngNMI version: 0.7.0\nsupported models:\n  - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2022-06-30\n  - urn:srl_nokia/aaa-password:srl_nokia-aaa-password, Nokia, 2022-06-30\n  - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2021-11-30\n  - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2022-06-30\n-- snip --\n</code></pre> <p>Since the default schema is set ot Openconfig in the startup configuration file, to perform gNMI requests using the native SR Linux models users have to specify the <code>native</code> origin. At the time of this wrigin, origin can be provided via Path Prefixes, and soon will be available for paths as well. <pre><code>\u276f gnmic -a 172.18.0.50:9339 -u admin -p NokiaSrl1! --skip-verify -e JSON_IETF \\\nget --prefix 'native:' --path '/system/information/version'\n[\n{\n\"source\": \"172.18.0.50:9339\",\n    \"timestamp\": 1678615621140266475,\n    \"time\": \"2023-03-12T11:07:01.140266475+01:00\",\n    \"updates\": [\n{\n\"Path\": \"srl_nokia-system:system/srl_nokia-system-info:information/version\",\n        \"values\": {\n\"srl_nokia-system:system/srl_nokia-system-info:information/version\": \"v22.11.2-116-gf3be2e95f2\"\n}\n}\n]\n}\n]\n</code></pre></p> <p>With the configuration leaf <code>/system/gnmi-server/network-instance[name=mgmt]/yang-models</code> set to <code>openconfig</code>, paths without the <code>origin</code> information are assumed to belong to the Openconfig YANG. <pre><code>\u276f gnmic -a 172.18.0.50:9339 -u admin -p NokiaSrl1! --skip-verify -e JSON_IETF \\\nget --path \"/system/state/hostname\"\n[\n{\n\"source\": \"172.18.0.50:9339\",\n    \"timestamp\": 1678615843724905250,\n    \"time\": \"2023-03-12T11:10:43.72490525+01:00\",\n    \"updates\": [\n{\n\"Path\": \"openconfig-system:system/state/hostname\",\n        \"values\": {\n\"openconfig-system:system/state/hostname\": \"srl1\"\n}\n}\n]\n}\n]\n</code></pre></p>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#gnoi","title":"gNOI","text":"<p> gNOI docs</p> <p>On SR Linux, gNOI service is enabled automatically once gNMI service is operational and share the same port <code>57400</code>. Although the same external post could have been used, to integrate with Ondatra test framework, a different service definition named <code>gnoi</code> with a separate <code>outside</code> port has been created.</p> Example <p>gNOI service can be tested using gnoic cli client.</p> <pre><code>\u276f gnoic -a 172.18.0.50:9337 --skip-verify -u admin -p NokiaSrl1! file stat --path /etc/os-release\n+-------------------+-----------------+---------------------------+------------+------------+------+\n|    Target Name    |      Path       |       LastModified        |    Perm    |   Umask    | Size |\n+-------------------+-----------------+---------------------------+------------+------------+------+\n| 172.18.0.50:57400 | /etc/os-release | 2021-09-14T06:32:07+02:00 | -rwxrwxrwx | -----w--w- | 21   |\n+-------------------+-----------------+---------------------------+------------+------------+------+\n</code></pre>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#gribi","title":"gRIBI","text":"<p> gRIBI docs</p> <p>gRIBI server is enabled on a system level and in the <code>mgmt</code> network instance of SR Linux running on port <code>57401</code>. It is exposed to <code>9340</code> port for external connectivity as specified by the services configuration in the topology file.</p> Example <p>gRIBI service can be tested using gribic cli client.</p> <pre><code>\u276f gribic -a 172.18.0.50:9340 -u admin -p NokiaSrl1! --skip-verify get --ns mgmt\nINFO[0000] target 172.18.0.50:9340: final get response:  INFO[0000] got 1 results\nINFO[0000] \"172.18.0.50:9340\":\n</code></pre>"},{"location":"tutorials/infrastructure/kne/srl-with-oc-services/#p4-runtime-p4rt","title":"P4 Runtime (P4RT)","text":"<p> P4RT docs</p> <p>The P4 Runtime server is configured on a system level and in the <code>mgmt</code> network instance of SR Linux running on port <code>9559</code>. The same port is used externally in this lab.</p> <p>Lab users still need to configure interface or device identifiers as per the documentation.</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>License is required to run chassis-based SR Linux systems (models: <code>ixr6e/ixr10e</code>). License-free IXR-D/H systems do not yet have support for Openconfig service; hence they are not suitable for the goals of this lab.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/l2evpn/evpn/","title":"L2 EVPN with SR Linux","text":"<p>Ethernet Virtual Private Network (EVPN), along with Virtual eXtensible LAN (VXLAN), is a technology that allows Layer 2 and Layer 3 traffic to be tunneled across an IP network.</p> <p>The SR Linux EVPN-VXLAN solution enables Layer 2 Broadcast Domains (BDs) in multi-tenant data centers using EVPN for the control plane and VXLAN as the data plane. It includes the following features:</p> <ul> <li>EVPN for VXLAN tunnels (Layer 2), extending a BD in overlay multi-tenant DCs</li> <li>EVPN for VXLAN tunnels (Layer 3), allowing inter-subnet-forwarding for unicast traffic within the same tenant infrastructure</li> </ul> <p>This tutorial is focused on EVPN for VXLAN tunnels Layer 2.</p>"},{"location":"tutorials/l2evpn/evpn/#overview","title":"Overview","text":"<p>EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch.</p> <p>VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently.</p> <p>The following figure demonstrates this concept where servers <code>srv1</code> and <code>srv2</code> are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain.</p> <p>Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable<sup>1</sup>, we can proceed with the VXLAN based EVPN service configuration.</p> <p>While doing that we will cover the following topics:</p> <ul> <li>VXLAN tunnel interface configuration</li> <li>Network instances of type <code>mac-vrf</code></li> <li>Bridged subinterfaces</li> <li>and BGP EVPN control plane configuration</li> </ul>"},{"location":"tutorials/l2evpn/evpn/#ibgp-for-evpn","title":"IBGP for EVPN","text":"<p>Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant.</p> <p>EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches.</p> <p>For that iBGP configuration we will create a group called <code>iBGP-overlay</code> which will have the <code>peer-as</code> and <code>local-as</code> set to <code>100</code> to form an iBGP neighborship. The group will also host the same permissive <code>all</code> routing policy, enabled <code>evpn</code> and disabled ipv4-unicast address families.</p> <p>Then for each leaf we add a new BGP neighbor addressed by the remote <code>system0</code> interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config:</p> leaf1leaf2 <pre><code>enter candidate\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n    neighbor 10.0.0.2 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.1\n        }\n    }\ncommit now\n</code></pre> <pre><code>enter candidate\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n    neighbor 10.0.0.1 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.2\n        }\n    }\ncommit now\n</code></pre> <p>Ensure that the iBGP session is established before proceeding any further:</p> <pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [0/0/0]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> <p>Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above.</p>"},{"location":"tutorials/l2evpn/evpn/#access-interfaces","title":"Access interfaces","text":"<p>Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches:</p> <p>Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be <code>bridged</code>, this makes the interfaces only attachable to a network instance of <code>mac-vrf</code> type with MAC learning and layer-2 forwarding enabled.</p> <p>The following config is applied to both leaf switches:</p> <pre><code>enter candidate\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <p>As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers.</p>"},{"location":"tutorials/l2evpn/evpn/#tunnelvxlan-interface","title":"Tunnel/VXLAN interface","text":"<p>After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric.</p> <p>The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name <code>vxlan&lt;N&gt;</code> where <code>N = 0..255</code>.</p> <p>A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI).</p> <ul> <li>The index can be a number in the range 0-4294967295.</li> <li>The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed).</li> <li>The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of <code>1..16777215</code>.   The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes.   SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets.</li> </ul> <p>Note</p> <p>The source IP used in the vxlan-interfaces is the IPv4 address of subinterface <code>system0.0</code> in the default network-instance.</p> <p>The above information translates to a configuration snippet which is applicable both to <code>leaf1</code> and <code>leaf2</code> nodes.</p> <pre><code>enter candidate\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\ncommit now\n</code></pre> <p>To verify the tunnel interface configuration:</p> <pre><code>A:leaf2# show tunnel-interface vxlan-interface brief\n---------------------------------------------------------------------------------\nShow report for vxlan-tunnels\n---------------------------------------------------------------------------------\n+------------------+-----------------+---------+-------------+------------------+\n| Tunnel Interface | VxLAN Interface |  Type   | Ingress VNI | Egress source-ip |\n+==================+=================+=========+=============+==================+\n| vxlan1           | vxlan1.1        | bridged | 1           | 10.0.0.2/32      |\n+------------------+-----------------+---------+-------------+------------------+\n---------------------------------------------------------------------------------\nSummary\n1 tunnel-interfaces, 1 vxlan interfaces\n0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip\n---------------------------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/l2evpn/evpn/#mac-vrf","title":"MAC-VRF","text":"<p>Now it is a turn of MAC-VRF to get configured.</p> <p>The network-instance type <code>mac-vrf</code> functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration.</p> <p>By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance:</p> <pre><code>enter candidate\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n    }\ncommit now\n</code></pre>"},{"location":"tutorials/l2evpn/evpn/#server-interfaces","title":"Server interfaces","text":"<p>The servers in our fabric do not have any addresses on their <code>eth1</code> interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration.</p> <p>By the end of this section, we will have the following addressing scheme complete:</p> <p>To connect to a shell of a server execute <code>docker exec -it &lt;container-name&gt; bash</code>:</p> srv1srv2 <pre><code>docker exec -it clab-evpn01-srv1 bash\n</code></pre> <pre><code>docker exec -it clab-evpn01-srv2 bash\n</code></pre> <p>Within the shell, configure MAC address<sup>2</sup> and IPv4 address for the <code>eth1</code> interface according to the diagram above, as with this interface the server is connected to the leaf switch.</p> srv1srv2 <pre><code>ip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre> <pre><code>ip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre> <p>Let's try to ping server2 from server1:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2028ms\n</code></pre> <p>That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the <code>srv1</code> interface MAC to get learned by the <code>leaf1</code> mac-vrf network instance:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n----------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n----------------------------------------------------------------------------------------------------------------------\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\n|      Address      |       Destination        |   Dest    |  Type  | Active | Aging |       Last Update        |\n|                   |                          |   Index   |        |        |       |                          |\n+===================+==========================+===========+========+========+=======+==========================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0           | 4         | learnt | true   | 242   | 2021-07-13T17:36:23.000Z |\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    1 Total    1 Active\n----------------------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/l2evpn/evpn/#evpn-in-mac-vrf","title":"EVPN in MAC-VRF","text":"<p>To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our <code>vrf-1</code> network-instance.</p> <p>EVPN configuration under the mac-vrf network instance will require two configuration containers:</p> <ul> <li><code>bgp-vpn</code> - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist.</li> <li><code>bgp-evpn</code> - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to <code>bgp-instance 1</code> is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI).</li> </ul> <p>The following configuration is entered on both leafs:</p> <pre><code>enter candidate\n    /network-instance vrf-1\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\ncommit now\n</code></pre> <p>Once configured, the <code>bgp-vpn</code> instance can be checked to have the RT/RD values set:</p> <pre><code>A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1\n=====================================================================\nNet Instance   : vrf-1\n    bgp Instance 1\n---------------------------------------------------------------------\n        route-distinguisher: 10.0.0.1:111, auto-derived-from-evi\n        export-route-target: target:100:111, manual\n        import-route-target: target:100:111, manual\n=====================================================================\n</code></pre> <p>VNI to EVI mapping</p> <p>Prior to release 21.11, SR Linux used only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI.<sup>3</sup></p> <p>Starting from release 21.11 SR Linux supports an interoperability mode in which SR Linux leaf nodes can be attached to VLAN-aware bundle broadcast domains along with other third-party routers.</p>"},{"location":"tutorials/l2evpn/evpn/#final-configurations","title":"Final configurations","text":"<p>For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration<sup>4</sup> for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks.</p> pastable snippets leaf1leaf2spine1srv1srv2 <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                policy-result accept\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 101\n                router-id 10.0.0.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.2 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.1\n                    }\n                }\n                neighbor 192.168.11.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    /interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.1/30 {\n                }\n            }\n        }\n    }\n    /interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                policy-result accept\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 102\n                router-id 10.0.0.2\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.1 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.2\n                    }\n                }\n                neighbor 192.168.12.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.1/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.2/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                policy-result accept\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/1.0 {\n        }\n        interface ethernet-1/2.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 201\n                router-id 10.0.1.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                }\n                ipv4-unicast {\n                    admin-state enable\n                }\n                neighbor 192.168.11.1 {\n                    peer-as 101\n                    peer-group eBGP-underlay\n                }\n                neighbor 192.168.12.1 {\n                    peer-as 102\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n    /interface ethernet-1/1 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.2/30 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.2/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.1.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv1 bash\n\nip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre></p> <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv2 bash\n\nip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre></p>"},{"location":"tutorials/l2evpn/evpn/#verification","title":"Verification","text":""},{"location":"tutorials/l2evpn/evpn/#evpn-imet-routes","title":"EVPN IMET routes","text":"<p>When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands:</p> <pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [1/1/1]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> <p>The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3).</p> <p>The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose:</p> <ul> <li>Auto-discovery of the remote VTEPs attached to the same EVI</li> <li>Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated</li> </ul> <p>The IMET/RT3 routes can be viewed in summary and detailed modes:</p> RT3 summaryRT3 detailed <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary\n----------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------\nType 3 Inclusive Multicast Ethernet Tag Routes\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n| Status | Route-distinguisher |   Tag-ID   |    Originator-IP    |      neighbor       |      Next-Hop       |\n+========+=====================+============+=====================+=====================+=====================+\n| u*&gt;    | 10.0.0.2:111        | 0          | 10.0.0.2            | 10.0.0.2            | 10.0.0.2            |\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n----------------------------------------------------------------------------------------------------------------\n1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid\n----------------------------------------------------------------------------------------------------------------\n</code></pre> <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail\n-------------------------------------------------------------------------------------\nShow report for the EVPN routes in network-instance  \"default\"\n-------------------------------------------------------------------------------------\nRoute Distinguisher: 10.0.0.2:111\nTag-ID             : 0\nOriginating router : 10.0.0.2\nneighbor           : 10.0.0.2\nReceived paths     : 1\nPath 1: &lt;Best,Valid,Used,&gt;\n    VNI             : 1\n    Route source    : neighbor 10.0.0.2 (last modified 2m3s ago)\n    Route preference: No MED, LocalPref is 100\n    Atomic Aggr     : false\n    BGP next-hop    : 10.0.0.2\n    AS Path         :  i\n    Communities     : [target:100:111, bgp-tunnel-encap:VXLAN]\n    RR Attributes   : No Originator-ID, Cluster-List is []\n    Aggregation     : None\n    Unknown Attr    : None\n    Invalid Reason  : None\n    Tie Break Reason: none\n--------------------------------------------------------------------------------------\n</code></pre> Lets capture those routes? <p>Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers.</p> <p>By capturing on the <code>e1-49</code> interface of the <code>clab-evpn01-leaf1</code> container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the <code>vrf-1</code> network instance to trigger the BGP updates to flow and see them in the live capture.</p> <p>Here is the pcap file with the IMET routes advertisements between <code>leaf1</code> and <code>leaf2</code>.</p> <p>When the IMET routes from <code>leaf2</code> are imported for <code>vrf-1</code> network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 multicast destinations (flooding-list)\n-------------------------------------------------------------------------------\n+--------------+------------+-------------------+----------------------+\n| VTEP Address | Egress VNI | Destination-index | Multicast-forwarding |\n+==============+============+===================+======================+\n| 10.0.0.2     | 1          | 160078821962      | BUM                  |\n+--------------+------------+-------------------+----------------------+\n-------------------------------------------------------------------------------\nSummary\n1 multicast-destinations\n-------------------------------------------------------------------------------\n</code></pre> <p>This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table.</p> <p>As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes.</p>"},{"location":"tutorials/l2evpn/evpn/#vxlan-tunnels","title":"VXLAN tunnels","text":"<p>After receiving EVPN routes from the remote leafs with VXLAN encapsulation<sup>5</sup>, SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the <code>leaf1</code> switch.</p> <pre><code>A:leaf1# /show tunnel vxlan-tunnel all\n----------------------------------------------------------\nShow report for vxlan-tunnels\n----------------------------------------------------------\n+--------------+--------------+--------------------------+\n| VTEP Address |    Index     |       Last Change        |\n+==============+==============+==========================+\n| 10.0.0.2     | 160078821947 | 2021-07-13T21:13:50.000Z |\n+--------------+--------------+--------------------------+\n1 VXLAN tunnels, 1 active, 0 inactive\n----------------------------------------------------------\n</code></pre> <p>The VXLAN tunnel is built between the <code>vxlan</code> interfaces in the MAC-VRF network instances, which internally use <code>system</code> interfaces of the <code>default</code> network instance as a VTEP:</p> <p>Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index<sup>6</sup>, an entry in the tunnel-table is also created for the tunnel.</p> <pre><code>A:leaf1# /show network-instance default tunnel-table all\n-------------------------------------------------------------------------------------------------------\nShow report for network instance \"default\" tunnel table\n-------------------------------------------------------------------------------------------------------\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n| IPv4 Prefix |   Owner   | Type  | Index | Metric | Preference | Fib-prog |       Last Update        |\n+=============+===========+=======+=======+========+============+==========+==========================+\n| 10.0.0.2/32 | vxlan_mgr | vxlan | 1     | 0      | 0          | Y        | 2021-07-13T21:13:43.424Z |\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n-------------------------------------------------------------------------------------------------------\n1 VXLAN tunnels, 1 active, 0 inactive\n</code></pre>"},{"location":"tutorials/l2evpn/evpn/#evpn-macip-routes","title":"EVPN MAC/IP routes","text":"<p>As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n-------------------------------------------------------------------------------\nDestinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nEthernet Segment Destinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nSummary\n0 unicast-destinations, 0 non-es, 0 es\n0 MAC addresses, 0 active, 0 non-active\n</code></pre> <p>This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the <code>vrf-1</code>, we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs<sup>7</sup>.</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n-------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n-------------------------------------------------------------------------------\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    0 Total    0 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    0 Total    0 Active\n-------------------------------------------------------------------------------\n</code></pre> <p>Let's try that ping from <code>srv1</code> towards <code>srv2</code> once again and see what happens:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms\n64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2013ms\nrtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms\n</code></pre> <p>Much better! The dataplane works and we can check that the MAC table in the <code>vrf-1</code> network-instance has been populated with local and EVPN-learned MACs:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n---------------------------------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n---------------------------------------------------------------------------------------------------------------------------------------------\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\n|      Address      |            Destination             |   Dest    |   Type    | Active | Aging |            Last Update             |\n|                   |                                    |   Index   |           |        |       |                                    |\n+===================+====================================+===========+===========+========+=======+====================================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0                     | 4         | learnt    | true   | 240   | 2021-07-18T14:22:55.000Z           |\n| 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1           | 160078821 | evpn      | true   | N/A   | 2021-07-18T14:22:56.000Z           |\n|                   | vtep:10.0.0.2 vni:1                | 962       |           |        |       |                                    |\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    1 Total    1 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    2 Total    2 Active\n---------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>When traffic is exchanged between <code>srv1</code> and <code>srv2</code>, the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2). The MAC/IP routes are imported, and the MACs programmed in the mac-table.</p> <p>The below output shows the MAC/IP EVPN route that <code>leaf1</code> received from its neighbor. The NLRI information contains the MAC of the <code>srv2</code>:</p> <pre><code>A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType 2 MAC-IP Advertisement Routes\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n| Statu |     Route-     |  Tag-ID   |   MAC-address    |   IP-address   |    neighbor    |    Next-Hop    |      VNI       |              ESI              |  MAC Mobility  |\n|   s   | distinguisher  |           |                  |                |                |                |                |                               |                |\n+=======+================+===========+==================+================+================+================+================+===============================+================+\n| u*&gt;   | 10.0.0.2:111   | 0         | 00:C1:AB:00:00:0 | 0.0.0.0        | 10.0.0.2       | 10.0.0.2       | 1              | 00:00:00:00:00:00:00:00:00:00 | -              |\n|       |                |           | 2                |                |                |                |                |                               |                |\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n1 MAC-IP Advertisement routes 1 used, 1 valid\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n---------------------------------------------------------------------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\nDestinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n+--------------+------------+-------------------+-----------------------------+\n| VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) |\n+==============+============+===================+=============================+\n| 10.0.0.2     | 1          | 160078821962      | 1(1/0)                      |\n+--------------+------------+-------------------+-----------------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------\nEthernet Segment Destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------\nSummary\n1 unicast-destinations, 1 non-es, 0 es\n1 MAC addresses, 1 active, 0 non-active\n-------------------------------------------------------------------------------\n</code></pre> <p>packet capture</p> <p>The following pcap was captured a moment before <code>srv1</code> started to ping <code>srv2</code> on <code>leaf1</code> interface <code>e1-49</code>.</p> <p>It shows how:</p> <ol> <li>ARP frames were first exchanged using the multicast destination, </li> <li>next the first ICMP request was sent out by <code>leaf1</code> again using the BUM destination, since RT2 routes were not received yet </li> <li>and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane.</li> <li>after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes.</li> </ol> <p>This concludes the verification steps, as we have a working data plane connectivity between the servers.</p> <ol> <li> <p>as was verified before \u21a9</p> </li> <li> <p>containerlab assigns mac addresses to the interfaces with OUI <code>00:C1:AB</code>. We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables.\u00a0\u21a9</p> </li> <li> <p>Per section 5.1.2 of RFC 8365 \u21a9</p> </li> <li> <p>Easily extracted with doing <code>info &lt;container&gt;</code> where <code>container</code> is <code>routing-policy</code>, <code>network-instance *</code>, <code>interface *</code>, <code>tunnel-interface *</code> \u21a9</p> </li> <li> <p>IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference.\u00a0\u21a9</p> </li> <li> <p>If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created.\u00a0\u21a9</p> </li> <li> <p>We did try to ping from <code>srv1</code> to <code>srv2</code> in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/l2evpn/fabric/","title":"L2 EVPN with SR Linux","text":"<p>Prior to configuring EVPN based overlay, a routing protocol needs to be deployed in the fabric to advertise the reachability of all the leaf VXLAN Termination End Point (VTEP) addresses throughout the IP fabric.</p> <p>With SR Linux, the following routing protocols can be used in the underlay:</p> <ul> <li>ISIS</li> <li>OSPF</li> <li>EBGP</li> </ul> <p>We will use a BGP based fabric design as described in RFC7938 due to its simplicity, scalability, and ease of multi-vendor interoperability.</p>"},{"location":"tutorials/l2evpn/fabric/#leaf-spine-interfaces","title":"Leaf-Spine interfaces","text":"<p>Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the <code>192.168.xx.0/30</code> network to address the links, we will implement the following underlay addressing design:</p> <p>On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity.</p> <p>We begin with connecting to the CLI of our nodes via SSH<sup>1</sup>:</p> <pre><code># connecting to leaf1\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration.</p> <p>Let's witness the step by step process of an interface configuration on a <code>leaf1</code> switch with providing the paste-ables snippets for the rest of the nodes</p> <ol> <li> <p>Enter the <code>candidate</code> configuration mode to make edits to the configuration</p> <pre><code>Welcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:leaf1# enter candidate\n</code></pre> </li> <li> <p>The prompt will indicate the changed active mode</p> <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1#                              </code></pre> </li> <li> <p>Enter into the interface configuration context</p> <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1# interface ethernet-1/49      \n</code></pre> </li> <li> <p>Create a subinterface under the parent interface to configure IPv4 address on it</p> <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 ]--\nA:leaf1# subinterface 0                                        \n--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]--\nA:leaf1# ipv4 address 192.168.11.1/30                                         \n</code></pre> </li> <li> <p>Apply the configuration changes by issuing a <code>commit now</code> command. The changes will be written to the running configuration.</p> <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]--\nA:leaf1# commit now                                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre> </li> </ol> <p>Below you will find the relevant configuration snippets<sup>2</sup> for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode.</p> leaf1leaf2spine1 <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.1/30 {\n            }\n        }\n    }\n}\n</code></pre> <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.1/30 {\n            }\n        }\n    }\n}\n</code></pre> <pre><code>interface ethernet-1/1 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.2/30 {\n            }\n        }\n    }\n}\ninterface ethernet-1/2 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.2/30 {\n            }\n        }\n    }\n}\n</code></pre> <p>Once those snippets are committed to the running configuration with <code>commit now</code> command, we can ensure that the changes have been applied by showing the interface status:</p> <pre><code>--{ + running }--[  ]--                             \nA:spine1# show interface ethernet-1/1               \n====================================================\nethernet-1/1 is up, speed 10G, type None\nethernet-1/1.0 is up\n    Network-instance: \n    Encapsulation   : null\n    Type            : routed\n    IPv4 addr    : 192.168.11.2/30 (static, None)\n----------------------------------------------------\n====================================================\n</code></pre> <p>At this moment, the configured interfaces can not be used as they are not yet associated with any network instance. Below we are placing the interfaces to the network-instance <code>default</code> that is created automatically by SR Linux.</p> leaf1 &amp; leaf2spine1 <pre><code>--{ + candidate shared default }--[  ]--\nA:leaf1# network-instance default interface ethernet-1/49.0\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]--\nA:leaf1# commit now                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre> <pre><code>--{ + candidate shared default }--[  ]--\nA:spine1# network-instance default interface ethernet-1/1.0\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]--\nA:spine1# /network-instance default interface ethernet-1/2.0                              \n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]--\nA:spine2# commit now                                                                      \nAll changes have been committed. Leaving candidate mode.\n</code></pre> <p>When interfaces are owned by the network-instance <code>default</code>, we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from <code>spine1</code> to <code>leaf2</code>:</p> <pre><code>--{ + running }--[  ]--                                     \nA:spine1# ping 192.168.12.1 network-instance default        \nUsing network instance default\nPING 192.168.12.1 (192.168.12.1) 56(84) bytes of data.\n64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms\n64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms\n64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms\n64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms\n^C\n--- 192.168.12.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\nrtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms\n</code></pre>"},{"location":"tutorials/l2evpn/fabric/#ebgp","title":"EBGP","text":"<p>Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol.</p> <p>The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering.</p> <p>Let's turn this diagram with the ASN/Router ID allocation into a working configuration:</p> <p>Here is a breakdown of the steps that are needed to configure EBGP on <code>leaf1</code> towards <code>spine1</code>:</p> <ol> <li> <p>Add BGP protocol to network-instance     Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol.  </p> <pre><code>--{ + candidate shared default }--[  ]--       \nA:leaf1# network-instance default protocols bgp\n</code></pre> </li> <li> <p>Assign Autonomous System Number     The ASN is reported to peers when BGP speaker opens a session towards another router.     According to the diagram above, <code>leaf1</code> has ASN 101.  </p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# autonomous-system 101\n</code></pre> </li> <li> <p>Assign Router ID     This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router.     Leaf1 has a router-id of 10.0.0.1.</p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# router-id 10.0.0.1\n</code></pre> </li> <li> <p>Enable AF     Enable all address families that should be enabled globally as a default for all peers of the BGP instance.     When you later configure individual neighbors or groups, you can override the enabled families at those levels.     For the sake of IPv4 loopbacks advertisement, we only need to enable <code>ipv4-unicast</code> address family:</p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# ipv4-unicast admin-state enable\n</code></pre> </li> <li> <p>Create export/import policies     The export/import policy is required for an EBGP peer to advertise and install routes.     The policy named <code>all</code> that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received<sup>4</sup>.  </p> <p>The routing policies are configured at <code>/routing-policy</code> context, so first, we switch to it from the current <code>bgp</code> context:</p> <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# /routing-policy                                                      \n--{ * candidate shared default }--[ routing-policy ]--                        \nA:leaf1#\n</code></pre> <p>Now that we are in the right context, we can paste the policy definition:</p> <pre><code>--{ +* candidate shared default }--[ routing-policy ]--\nA:leaf1# info\n    policy all {\n        default-action {\n            policy-result accept\n        }\n    }\n</code></pre> </li> <li> <p>Create peer-group config     A peer group should include sessions that have a similar or almost identical configuration.     In this example, the peer group is named <code>eBGP-underlay</code> since it will be used to enable underlay routing between the leafs and spines.     New groups are administratively enabled by default.</p> <p>First, we come back to the bgp context from the routing-policy context:</p> <pre><code>--{ * candidate shared default }--[ routing-policy ]--\nA:leaf1# /network-instance default protocols bgp      \n--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1#\n</code></pre> <p>Now create the peer group. The common group configuration includes the <code>peer-as</code> and <code>export-policy</code> statements.</p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# group eBGP-underlay\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# peer-as 201                                                                               \n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# export-policy all\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# import-policy all\n</code></pre> </li> <li> <p>Configure neighbor     Configure the BGP session with <code>spine1</code>. In this example, <code>spine1</code> is reachable through the <code>ethernet-1/49.0</code> subinterface. On this subnet, <code>spine1</code> has the IPv4 address <code>192.168.11.2</code>.     In this minimal configuration example, the only required configuration for the neighbor is its association with the group <code>eBGP-underlay</code> that was previously created.     New neighbors are administratively enabled by default.</p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay\n</code></pre> </li> <li> <p>Commit configuration     It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far.     Regardless of which context you are currently in, you can see the diff against the baseline config by doing <code>diff /</code></p> <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# diff /                                                                                   \n   network-instance default {\n        protocols {\n+             bgp {\n+                 autonomous-system 101\n+                 router-id 10.0.0.1\n+                 group eBGP-underlay {\n+                     export-policy all\n+                     import-policy all\n+                     peer-as 201\n+                 }\n+                 ipv4-unicast {\n+                     admin-state enable\n+                 }\n+                 neighbor 192.168.11.2 {\n+                     peer-group eBGP-underlay\n+                 }\n+             }\n       }\n    }\n+     routing-policy {\n+         policy all {\n+             default-action {\n+                 policy-result accept\n+             }\n+         }\n+     }\n</code></pre> <p>That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration.</p> <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# commit now\n</code></pre> </li> </ol> <p>EBGP configuration on <code>leaf2</code> and <code>spine1</code> is almost a twin of the one we did for <code>leaf1</code>. Here is a copy-paste-able<sup>3</sup> config snippets for all of the nodes:</p> leaf1leaf2spine1 <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 101\n            router-id 10.0.0.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            policy-result accept\n        }\n    }\n}\n</code></pre> <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 102\n            router-id 10.0.0.2\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.12.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            policy-result accept\n        }\n    }\n}\n</code></pre> <p>Spine configuration is a bit different, in a way that <code>peer-as</code> is specified under the neighbor context, and not the group one. <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 201\n            router-id 10.0.1.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.1 {\n                peer-group eBGP-underlay\n                peer-as 101\n            }\n            neighbor 192.168.12.1 {\n                peer-group eBGP-underlay\n                peer-as 102\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            policy-result accept\n        }\n    }\n}\n</code></pre></p>"},{"location":"tutorials/l2evpn/fabric/#loopbacks","title":"Loopbacks","text":"<p>As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces.</p> <p>In the context of the VXLAN data plane, a special kind of a loopback needs to be created - <code>system0</code> interface.</p> <p>Info</p> <p>The <code>system0.0</code> interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes.</p> <p>Configuration of the <code>system0</code> interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to <code>system0</code> interfaces will match the Router-ID of a given BGP speaker.</p> leaf1leaf2spine1 <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre> <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.2/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre> <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.1.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>"},{"location":"tutorials/l2evpn/fabric/#verification","title":"Verification","text":"<p>As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The <code>system0</code> interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that.</p>"},{"location":"tutorials/l2evpn/fabric/#bgp-status","title":"BGP status","text":"<p>The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on <code>leaf1</code>:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n-------------------------------------------------------------\nDefault preferences\n  BGP Local Preference attribute: 100\n  EBGP route-table preference   : 170\n  IBGP route-table preference   : 170\n-------------------------------------------------------------\nWait for FIB install to advertise: True\nSend rapid withdrawals           : disabled\n-------------------------------------------------------------\nIpv4-unicast AFI/SAFI\n    Received routes               : 3\n    Received and active routes    : None\n    Max number of multipaths      : 1, 1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nIpv6-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : 1,1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nEVPN-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : N/A\n    Multipath can transit multi AS: N/A\n-------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/l2evpn/fabric/#bgp-neighbor-status","title":"BGP neighbor status","text":"<p>Equally important is the neighbor summary status that we can observe with the following:</p> <pre><code>--{ + running }--[  ]--\nA:spine1# show network-instance default protocols bgp neighbor\n----------------------------------------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------------------------------------\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n|    Net-Inst    |         Peer          |     Group      | Flag | Peer-AS |    State    |   Uptime    | AFI/SAFI  |    [Rx/Active/Tx]     |\n|                |                       |                |  s   |         |             |             |           |                       |\n+================+=======================+================+======+=========+=============+=============+===========+=======================+\n| default        | 192.168.11.1          | eBGP-underlay  | S    | 101     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 49s         | ast       |                       |\n| default        | 192.168.12.1          | eBGP-underlay  | S    | 102     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 9s          | ast       |                       |\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------\nSummary:\n2 configured neighbors, 2 configured sessions are established,0 disabled peers\n0 dynamic peers\n</code></pre> <p>With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state.</p>"},{"location":"tutorials/l2evpn/fabric/#receivedadvertised-routes","title":"Received/Advertised routes","text":"<p>The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - <code>system0</code> interfaces. In the below output we verify that <code>leaf1</code> advertises the prefix of <code>system0</code> (<code>10.0.0.1/32</code>) interface towards its EBGP <code>spine1</code> peer:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou\ntes ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.11.2, remote AS: 201, local AS: 101\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-------------------------------------------------------------------------------------+\n|    Network        Next Hop       MED     LocPref           AsPath           Origin  |\n+=====================================================================================+\n| 10.0.0.1/32      192.168.11.      -        100 [101]                       i    |\n|                  1                                                                  |\n| 192.168.11.0/3   192.168.11.      -        100 [101]                       i    |\n| 0 1                                                                  |\n+-------------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n2 advertised BGP routes\n-----------------------------------------------------------------------------------------\n</code></pre> <p>On the far end of the fabric, <code>leaf2</code> receives both the <code>leaf1</code> and <code>spine1</code> system interface prefixes:</p> <pre><code>--{ + running }--[  ]--\nA:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route\ns ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.12.2, remote AS: 201, local AS: 102\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-----------------------------------------------------------------------------------+\n|  Status      Network    Next Hop       MED       LocPref     AsPath      Origin   |\n+===================================================================================+\n|    u*&gt;      10.0.0.1/   192.168.1       -          100 [201,           i     |\n|             32          2.2                                 101]                  |\n|    u*&gt;      10.0.1.1/   192.168.1       -          100 [201]           i     |\n|             32          2.2                                                       |\n|    u*&gt;      192.168.1   192.168.1       -          100 [201]           i     |\n|             1.0/30      2.2                                                       |\n|     *       192.168.1   192.168.1       -          100 [201]           i     |\n|             2.0/30      2.2                                                       |\n+-----------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n4 received BGP routes : 3 used 4 valid\n-----------------------------------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/l2evpn/fabric/#route-table","title":"Route table","text":"<p>The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the <code>default</code> network-instance where we expect them to be:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast summary\n-----------------------------------------------------------------------------------------------------------------------------------\nIPv4 Unicast route table of network instance default\n-----------------------------------------------------------------------------------------------------------------------------------\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n|     Prefix      |  ID   | Route Type |     Route Owner      |      Best/Fib-       |  Metric  |  Pref   | Next-hop  | Next-hop  |\n|                 |       |            |                      |     status(slot)     |          |         |  (Type)   | Interface |\n+=================+=======+============+======================+======================+==========+=========+===========+===========+\n| 10.0.0.1/32     | 3     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 10.0.0.2/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 10.0.1.1/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 192.168.11.0/30 | 1     | local      | net_inst_mgr         | True/success         | 0        | 0       | 192.168.1 | ethernet- |\n|                 |       |            |                      |                      |          |         | 1.1       | 1/49.0    |\n|                 |       |            |                      |                      |          |         | (direct)  |           |\n| 192.168.11.1/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 192.168.11.3/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None (bro | None      |\n|                 |       |            |                      |                      |          |         | adcast)   |           |\n| 192.168.12.0/30 | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n-----------------------------------------------------------------------------------------------------------------------------------\n7 IPv4 routes total\n7 IPv4 prefixes with active routes\n0 IPv4 prefixes with active ECMP routes\n-----------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Both <code>leaf2</code> and <code>spine1</code> prefixes are found in the route table of network-instance <code>default</code> and the <code>bgp_mgr</code> is the owner of those prefixes, which means that they have been added to the route-table by the BGP app.</p>"},{"location":"tutorials/l2evpn/fabric/#dataplane","title":"Dataplane","text":"<p>To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay.</p> <p>For that we will use the <code>ping</code> command with src/dst set to loopback addresses:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2\nUsing network instance default\nPING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data.\n64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms\n64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms\n</code></pre> <p>Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration!</p>"},{"location":"tutorials/l2evpn/fabric/#resulting-configs","title":"Resulting configs","text":"<p>Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with <code>info flat</code> command.</p> <p>Note</p> <p><code>enter candidate</code> and <code>commit now</code> commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config.</p> leaf1leaf2spine1 <pre><code>enter candidate\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.1/32\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action policy-result accept\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 101\nset / network-instance default protocols bgp router-id 10.0.0.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.2\nset / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay\ncommit now\n</code></pre> <pre><code>enter candidate\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.2/32\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action policy-result accept\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 102\nset / network-instance default protocols bgp router-id 10.0.0.2\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.12.2\nset / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay\ncommit now\n</code></pre> <pre><code>enter candidate\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/1\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30\nset / interface ethernet-1/2\nset / interface ethernet-1/2 subinterface 0\nset / interface ethernet-1/2 subinterface 0 ipv4\nset / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.1.1/32\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface ethernet-1/2.0\nset / network-instance default interface system0.0\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action policy-result accept\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 201\nset / network-instance default protocols bgp router-id 10.0.1.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.1\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay\nset / network-instance default protocols bgp neighbor 192.168.12.1\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay\ncommit now\n</code></pre> <ol> <li> <p>default SR Linux credentials are <code>admin:NokiaSrl1!</code>.\u00a0\u21a9</p> </li> <li> <p>the snippets were extracted with <code>info interface ethernet-1/x</code> command issued in running mode.\u00a0\u21a9</p> </li> <li> <p>you can paste those snippets right after you do <code>enter candidate</code> \u21a9</p> </li> <li> <p>a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/l2evpn/intro/","title":"L2 EVPN with SR Linux","text":"Tutorial name L2 EVPN-VXLAN with SR Linux Lab components 3 SR Linux nodes Resource requirements  2vCPU  6 GB Containerlab topology file evpn01.clab.yml Lab name evpn01 Packet captures EVPN IMET routes exchange, RT2 routes exchange with ICMP in datapath Main ref documents RFC 7432 - BGP MPLS-Based Ethernet VPNRFC 8365 - A Network Virtualization Overlay Solution Using Ethernet VPN (EVPN)Nokia 7220 SR Linux Advanced Solutions GuideNokia 7220 SR Linux EVPN-VXLAN Guide Version information<sup>1</sup> <code>containerlab:0.34.0</code>, <code>srlinux:22.11.1</code>, <code>docker-ce:20.10.2</code> <p>Ethernet Virtual Private Network (EVPN) is a standard technology in multi-tenant Data Centers (DCs) and provides a control plane framework for many functions. In this tutorial we will configure a VXLAN based Layer 2 EVPN service<sup>3</sup> in a tiny CLOS fabric and at the same get to know SR Linux better!</p> <p>The DC fabric that we will build for this tutorial consists of the two leaf switches (acting as Top-Of-Rack) and a single spine:</p> <p>The two servers are connected to the leafs via an L2 interface. Service-wise the servers will appear to be on the same L2 network by means of the deployed EVPN Layer 2 service.</p> <p>The tutorial will consist of the following major parts:</p> <ul> <li>Fabric configuration - here we will configure the routing protocol in the underlay of a fabric to advertise the Virtual Tunnel Endpoints (VTEP) of the leaf switches.</li> <li>EVPN configuration - this chapter is dedicated to the EVPN service configuration and validation.</li> </ul>","tags":["evpn"]},{"location":"tutorials/l2evpn/intro/#lab-deployment","title":"Lab deployment","text":"<p>To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM:</p> <p>The containerlab file that describes the lab topology is referenced below in full:</p> <pre><code>name: evpn01\ntopology:\nkinds:\nsrl:\nimage: ghcr.io/nokia/srlinux\nlinux:\nimage: ghcr.io/hellt/network-multitool\nnodes:\nleaf1:\nkind: srl\ntype: ixrd2\nleaf2:\nkind: srl\ntype: ixrd2\nspine1:\nkind: srl\ntype: ixrd3\nsrv1:\nkind: linux\nsrv2:\nkind: linux\nlinks:\n# inter-switch links\n- endpoints: [\"leaf1:e1-49\", \"spine1:e1-1\"]\n- endpoints: [\"leaf2:e1-49\", \"spine1:e1-2\"]\n# server links\n- endpoints: [\"srv1:eth1\", \"leaf1:e1-1\"]\n- endpoints: [\"srv2:eth1\", \"leaf2:e1-1\"]\n</code></pre> <p>Save<sup>2</sup> the contents of this file under <code>evpn01.clab.yml</code> name and you are ready to deploy:</p> <pre><code>$ containerlab deploy -t evpn01.clab.yml\nINFO[0000] Parsing &amp; checking topology file: evpn01.clab.yml \nINFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 \nINFO[0000] Creating root CA                             \nINFO[0001] Creating container: srv2                  \nINFO[0001] Creating container: srv1                  \nINFO[0001] Creating container: leaf2                    \nINFO[0001] Creating container: spine1                   \nINFO[0001] Creating container: leaf1                    \nINFO[0002] Creating virtual wire: leaf1:e1-49 &lt;--&gt; spine1:e1-1 \nINFO[0002] Creating virtual wire: srv2:eth1 &lt;--&gt; leaf2:e1-1 \nINFO[0002] Creating virtual wire: leaf2:e1-49 &lt;--&gt; spine1:e1-2 \nINFO[0002] Creating virtual wire: srv1:eth1 &lt;--&gt; leaf1:e1-1 \nINFO[0003] Writing /etc/hosts file                      \n\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |              Image              | Kind  | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| 1 | clab-evpn01-leaf1  | 4b81c65af558 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.7/24 | 2001:172:20:20::7/64 |\n| 2 | clab-evpn01-leaf2  | de000e791dd6 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.8/24 | 2001:172:20:20::8/64 |\n| 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.6/24 | 2001:172:20:20::6/64 |\n| 4 | clab-evpn01-srv1   | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 5 | clab-evpn01-srv2   | fb722453d715 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n</code></pre> <p>A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of <code>leaf1</code>:</p> <pre><code># default credentials admin:NokiaSrl1!\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey!</p> <p>Note</p> <p>We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough<sup>4</sup> details to survive in the configuration waters we are about to get.</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work, but if they don't, please pin the version to the mentioned ones.\u00a0\u21a9</p> </li> <li> <p>Or download it with <code>curl -LO https://github.com/srl-labs/learn-srlinux/blob/master/labs/evpn01.clab.yml</code> \u21a9</p> </li> <li> <p>Per RFC 8365 &amp; RFC 7432 \u21a9</p> </li> <li> <p>For a complete documentation coverage don't hesitate to visit our documentation portal.\u00a0\u21a9</p> </li> </ol>","tags":["evpn"]},{"location":"tutorials/l2evpn/summary/","title":"L2 EVPN with SR Linux","text":"<p>Layer 2 EVPN services with VXLAN dataplane are very common in multi-tenant data centers. In this tutorial we walked through every step that is needed to configure a basic Layer 2 EVPN with VXLAN dataplane service deployed on SR Linux switches:</p> <ul> <li>IP fabric config using eBGP in the underlay</li> <li>EVPN service config on leaf switches with the control and data plane verification</li> </ul> <p>The highly detailed configuration &amp; verification steps helped us achieve the goal of creating an overlay Layer 2 broadcast domain for the two servers in our topology. So that the high level service diagram transformed into a detailed map of configuration constructs and instances.</p> <p>During the verification phases we collected the following packet captures to prove the control/data plane behavior:</p> <ul> <li>Exchange of the IMET/RT3 EVPN routes. IMET/RT3 routes are the starting point in the L2 EVPN-VXLAN services, as they are used to dynamically discover the VXLAN VTEPs participating in the same EVI.</li> <li>Exchange of MAC-IP/RT2 EVPN routes which convey the MAC information of the attached servers. These routes are used to create unicast tunnel destinations that the dataplane frames will use.</li> </ul> <p>Info</p> <p>The more advanced EVPN topics listed below will be covered in separate tutorials:</p> <ul> <li>EVPN L2 multi-homing</li> <li>MAC mobility</li> <li>MAC duplication and loop protection</li> </ul>"},{"location":"tutorials/mpls/mpls-ldp/intro/","title":"LDP-based MPLS with SR Linux","text":"Tutorial name LDP-based MPLS core Lab components 3 Nokia SR Linux nodes Resource requirements  2vCPU  4 GB Containerlab topology file mpls-ldp.clab.yml Packet captures \u00b7 LDP neighborship\u00b7 MPLS encapsulation Main ref documents \u00b7 RFC 5036 - LDP Specification\u00b7 Nokia SR Linux MPLS Guide Version information<sup>1</sup> <code>containerlab:0.24.1</code>, <code>srlinux:21.11.2</code>, <code>docker-ce:20.10.2</code> <p>Multiprotocol Label Switching (MPLS) is a label switching technology that provides the ability to set up connection-oriented paths over a connection-less IP network. MPLS facilitates network traffic flow and provides a mechanism to engineer network traffic patterns independently from routing tables. MPLS sets up a specific path for a sequence of packets. The packets are identified by a label stack inserted into each packet.</p> <p>This short tutorial will guide you through the steps required to build an LDP-based MPLS core consisting of three SR Linux routers. LDP-based MPLS tunnels are commonly used to enable BGP-free core network.</p> <p>The topology we will use for this interactive tutorial is dead simple - three routers connected in a point-to-point fashion:</p> <p>The MPLS-enabled core will be formed with <code>srl1</code> and <code>srl3</code> acting as Label Edge Routers (LER) and <code>srl2</code> as Label Switch Router (LSR). The loopback <code>lo0</code> interfaces configured on LERs will emulate clients talking to each other via an established MPLS tunnel.</p> <p>The tutorial will consist of the following configuration parts:</p> <ul> <li>Core routing - configuring interfaces, network instances and IS-IS IGP protocol.</li> <li>LDP-based MPLS - configuring LDP and running control plane and data plane verification steps.</li> </ul>","tags":["ldp","mpls"]},{"location":"tutorials/mpls/mpls-ldp/intro/#lab-deployment","title":"Lab deployment","text":"<p>The tutorial is augmented with the containerlab-based lab so that you can perform all the steps we do here. The clab file describing the topology looks like follows:</p> <pre><code>name: mpls-ldp\nprefix: \"\"\ntopology:\ndefaults:\nkind: srl\nkinds:\nsrl:\nimage: ghcr.io/nokia/srlinux:21.11.3\ntype: ixr6 # (1)!\nnodes:\nsrl1:\nsrl2:\nsrl3:\nlinks:\n- endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n- endpoints: [\"srl2:e1-2\", \"srl3:e1-1\"]\n</code></pre> <ol> <li>Pay attention to the HW type we specify in the clab file. MPLS is only available on ixr6 and ixr10 platforms at the time of this writing. IXR-6/10 chassis will require a license since 22.3 release of SR Linux.</li> </ol> <p>Save<sup>2</sup> the contents of this file under <code>mpls-ldp.clab.yml</code> name, and you are ready to deploy:</p> <pre><code>$ containerlab deploy -t clab-ldp.clab.yml\n\n# output omitted for brevity\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n| # | Name | Container ID |         Image         | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n| 1 | srl1 | 7ba522099fd9 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n| 2 | srl2 | a86c7ce3db59 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 3 | srl3 | e87ffaf33111 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n</code></pre> <p>A few seconds later, containerlab finishes the deployment by providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers which can be used to reach the nodes. For example to connect to the SSH server of <code>srl1</code>:</p> <pre><code># default credentials admin:admin\nssh admin@srl1\n</code></pre> <p>With the lab deployed, we are ready to embark on our learn-by-doing LDP-based MPLS configuration journey!</p> <p>Note</p> <p>We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough<sup>3</sup> details to survive in the configuration waters we are about to get.</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>Or download it with <code>curl -LO https://github.com/srl-labs/learn-srlinux/blob/main/labs/mpls-ldp/mpls-ldp.clab.yml</code> \u21a9</p> </li> <li> <p>For complete documentation coverage, check the official documentation.\u00a0\u21a9</p> </li> </ol>","tags":["ldp","mpls"]},{"location":"tutorials/mpls/mpls-ldp/ldp/","title":"LDP-based MPLS with SR Linux","text":"<p>LDP is a protocol defined for distributing labels. It is the set of procedures and messages by which Label Switched Routers (LSRs) establish Label Switched Paths (LSPs) through a network by mapping network-layer routing information directly to data-link layer switched paths. These LSPs may have an endpoint at a directly attached neighbor (comparable to IP hop-by-hop forwarding), or may have an endpoint at a network egress node, enabling switching via all intermediary nodes.</p> <p>This chapter focuses on LDP configuration with verification steps to ensure that LSPs are set up and traffic is properly encapsulated.</p>"},{"location":"tutorials/mpls/mpls-ldp/ldp/#mpls-label-manager","title":"MPLS label manager","text":"<p>SR Linux features an MPLS label manager process that shares the MPLS label space among client applications that require MPLS labels; these applications include static MPLS forwarding and LDP.</p> <p>LDP must be configured with a reference to a predefined range of labels, called a label block. A label block configuration includes a start-label value and an end-label value. LDP requires a dynamic, non-shared label block.</p> <p>Although it is absolutely fine to configure the same label block on all the nodes, we will configure each device with a distinctive range for readability.</p> srl1srl2srl3 <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 100\nset / system mpls label-ranges dynamic D1 end-label 199\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 200\nset / system mpls label-ranges dynamic D1 end-label 299\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 300\nset / system mpls label-ranges dynamic D1 end-label 399\n\ncommit save\n</code></pre>"},{"location":"tutorials/mpls/mpls-ldp/ldp/#ldp-neighbor-discovery","title":"LDP neighbor discovery","text":"<p>LDP neighbor discovery allows SR Linux to discover and connect to LDP peers without manually specifying the peers. SR Linux supports basic LDP discovery for discovering LDP peers, using multicast UDP hello messages.</p> <p>At a minimum, you should enable the LDP process in the network-instance and specify LDP-enabled interfaces.</p> srl1srl2srl3 <pre><code>enter candidate\n\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\n\nset / network-instance default protocols ldp discovery\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\nset / network-instance default protocols\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\nset / network-instance default protocols ldp discovery\n\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0 ipv4 admin-state enable\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\n\nset / network-instance default protocols ldp discovery\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\ncommit save\n</code></pre> <p>Once enabled, LDP neighborship will establish over the specified interfaces.</p> neighborssessions <pre><code>--{ running }--[  ]--                                                                            \nA:srl2# show network-instance default protocols ldp neighbor                                     \n=================================================================================================\nNet-Inst default LDP neighbors\n-------------------------------------------------------------------------------------------------\n+------------------------------------------------------------------------------------------+\n| Interface    Peer LDP     Nbr          Local        Proposed     Negotiated   Remaining  |\n|              ID           Address      Address      Holdtime     Holdtime     Holdtime   |\n+==========================================================================================+\n| ethernet-1   10.0.0.1:0   10.1.2.1     10.1.2.2     15           15           11         |\n| /1.0                                                                                     |\n| ethernet-1   10.0.0.3:0   10.2.3.2     10.2.3.1     15           15           14         |\n| /2.0                                                                                     |\n+------------------------------------------------------------------------------------------+\n=================================================================================================\n</code></pre> <pre><code>A:srl2# /show network-instance default protocols ldp session                                    \n================================================================================================\nNet-Inst default LDP Sessions\n------------------------------------------------------------------------------------------------\n+-------------------------------------------------------------------------------------------+\n| Peer LDP ID                State           Msg Sent   Msg Recv   Last Oper State Change   |\n+===========================================================================================+\n| 10.0.0.1:0                 operational     24         24         2022-03-14T08:08:28.000Z |\n| 10.0.0.3:0                 operational     24         24         2022-03-14T08:08:24.000Z |\n+-------------------------------------------------------------------------------------------+\nNo. of sessions: 2\n</code></pre> <p>Tip</p> <p>This packet capture sniffed on srl2's <code>e1-1</code> shows the LDP multicast hello messages as well as the subsequent Initialization and Notification messages.</p>"},{"location":"tutorials/mpls/mpls-ldp/ldp/#fec","title":"FEC","text":"<p>It is necessary to precisely specify which packets may be mapped to each LSP. This is done by providing a FEC specification for each LSP. The FEC identifies the set of IP packets that may be mapped to that LSP.</p> <p>Each FEC is specified as a set of one or more FEC elements. Each FEC element identifies a set of packets that may be mapped to the corresponding LSP.</p> <p>By default, SR Linux supports /32 IPv4 FEC resolution using IGP routes. For example, on <code>srl2</code> we see four FECs have been received, and four FECs have been advertised. These FECs were created for <code>system0</code> interface IP addresses advertised via IGP.</p> <pre><code>A:srl2# /show network-instance default protocols ldp ipv4 fec                                     \n==================================================================================================\nNet-Inst default LDP IPv4: All FEC prefixes table\n==================================================================================================\nReceived FEC prefixes\n--------------------------------------------------------------------------------------------------\n+--------------------------------------------------------------------------------------------+\n| FEC prefix           Peer LDP ID                 Label                Ingress   Used in    |\n|                                                                       LSR       Forwarding |\n+============================================================================================+\n| 10.0.0.1/32          10.0.0.1:0                  100                  true      true       |\n| 10.0.0.1/32          10.0.0.3:0                  304                  true      false      |\n| 10.0.0.3/32          10.0.0.1:0                  109                  true      false      |\n| 10.0.0.3/32          10.0.0.3:0                  300                  true      true       |\n+--------------------------------------------------------------------------------------------+\n--------------------------------------------------------------------------------------------------\nAdvertised FEC prefixes\n--------------------------------------------------------------------------------------------------\n+--------------------------------------------------------------------------------------------+\n| FEC prefix           Peer LDP ID                 Label                Label        Egress  |\n|                                                                       Status       LSR     |\n+============================================================================================+\n| 10.0.0.1/32          10.0.0.3:0                  206                               false   |\n| 10.0.0.2/32          10.0.0.1:0                  204                               true    |\n| 10.0.0.2/32          10.0.0.3:0                  204                               true    |\n| 10.0.0.3/32          10.0.0.1:0                  205                               false   |\n+--------------------------------------------------------------------------------------------+\n--------------------------------------------------------------------------------------------------\nTotal received FEC prefixes  : 4 (2 used in forwarding)\nTotal advertised FEC prefixes: 4\n</code></pre> <p>The successful labels exchange leads to a populated tunnel table on each MPLS-enabled router. For instance, the tunnel table on <code>srl1</code> lists two tunnels for remote loopbacks of <code>srl2</code> and <code>srl3</code>:</p> <pre><code>--{ running }--[  ]--                                                                                                   \nA:srl1# show network-instance default tunnel-table all                                                                  \n------------------------------------------------------------------------------------------------------------------------\nIPv4 tunnel table of network-instance \"default\"\n------------------------------------------------------------------------------------------------------------------------\n+-------------+------------+------------+-----------+-----+--------+------------+------------+------------+------------+\n| IPv4 Prefix |   Encaps   |   Tunnel   | Tunnel ID | FIB | Metric | Preference |    Last    |  Next-hop  |  Next-hop  |\n|             |    Type    |    Type    |           |     |        |            |   Update   |   (Type)   |            |\n+=============+============+============+===========+=====+========+============+============+============+============+\n| 10.0.0.2/32 | mpls       | ldp        | 65548     | Y   | 10     | 9          | 2022-03-14 | 10.1.2.2   | ethernet-1 |\n|             |            |            |           |     |        |            | T08:08:29. | (mpls)     | /1.0       |\n|             |            |            |           |     |        |            | 207Z       |            |            |\n| 10.0.0.3/32 | mpls       | ldp        | 65549     | Y   | 20     | 9          | 2022-03-14 | 10.1.2.2   | ethernet-1 |\n|             |            |            |           |     |        |            | T08:08:29. | (mpls)     | /1.0       |\n|             |            |            |           |     |        |            | 219Z       |            |            |\n+-------------+------------+------------+-----------+-----+--------+------------+------------+------------+------------+\n------------------------------------------------------------------------------------------------------------------------\n2 LDP tunnels, 2 active, 0 inactive\n------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The label operations can be seen in the mpls route-table report. For example, our LSR <code>srl2</code> performs swap operations for labels it assigned for remote FECs and it pops a label <code>204</code> as it was assigned to its own <code>10.0.0.2</code> FEC.</p> <pre><code>A:srl2# /show network-instance default route-table mpls\n+---------+-----------+-------------+-----------------+------------------------+----------------------+------------------+\n| Label   | Operation | Type        | Next Net-Inst   | Next-hop IP (Type)     | Next-hop             | Next-hop MPLS    |\n|         |           |             |                 |                        | Subinterface         | labels           |\n+=========+===========+=============+=================+========================+======================+==================+\n| 204     | POP       | ldp         | default         |                        |                      |                  |\n| 205     | SWAP      | ldp         | N/A             | 10.2.3.2 (mpls)        | ethernet-1/2.0       | 300              |\n| 206     | SWAP      | ldp         | N/A             | 10.1.2.1 (mpls)        | ethernet-1/1.0       | 100              |\n+---------+-----------+-------------+-----------------+------------------------+----------------------+------------------+\n</code></pre>"},{"location":"tutorials/mpls/mpls-ldp/ldp/#testing-mpls-dataplane","title":"Testing MPLS dataplane","text":"<p>The tunnels established for <code>system0</code> loopback FECs cannot be tested as is because they resolve to the existing IGP routes, and thus plain IPv4 transport is used. To test the MPLS dataplane we would need to create another pair of loopbacks on <code>srl1</code>/<code>srl3</code> nodes and create an iBGP session exchanging these loopbacks; only this time, we will leverage a specific BGP knob asking to resolve the nexthops for these prefixes via LDP tunnel only.</p> <p>In the following snippets we configure <code>lo0</code> loopbacks following with iBGP peering setup to advertise them.</p> srl1srl3 <pre><code>enter candidate\n\n# configuring loopback interface\nset / interface lo0\nset / interface lo0 admin-state enable\nset / interface lo0 subinterface 0\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4\nset / interface lo0 subinterface 0 ipv4 address 192.168.99.1/32\n\nset / network-instance default interface lo0.0\n\n# configuring export policy to advertise loopbacks via BGP\nset / routing-policy\nset / routing-policy prefix-set LOOPBACK\nset / routing-policy prefix-set LOOPBACK prefix 192.168.99.1/32 mask-length-range exact\nset / routing-policy policy EXPORT_LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match family ipv4-unicast\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match prefix-set LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action accept\n\n# configuring iBGP\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp router-id 10.0.0.1\nset / network-instance default protocols bgp group IBGP\nset / network-instance default protocols bgp group IBGP export-policy EXPORT_LOOPBACK\nset / network-instance default protocols bgp group IBGP ipv4-unicast\nset / network-instance default protocols bgp group IBGP ipv4-unicast admin-state enable\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution mode require\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution allowed-tunnel-types [ ldp ]\nset / network-instance default protocols bgp neighbor 10.0.0.3\nset / network-instance default protocols bgp neighbor 10.0.0.3 admin-state enable\nset / network-instance default protocols bgp neighbor 10.0.0.3 peer-as 65001\nset / network-instance default protocols bgp neighbor 10.0.0.3 peer-group IBGP\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\n# configuring loopback interface\nset / interface lo0\nset / interface lo0 admin-state enable\nset / interface lo0 subinterface 0\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4\nset / interface lo0 subinterface 0 ipv4 address 192.168.99.3/32\n\nset / network-instance default interface lo0.0\n\n# configuring export policy to advertise loopbacks via BGP\nset / routing-policy\nset / routing-policy prefix-set LOOPBACK\nset / routing-policy prefix-set LOOPBACK prefix 192.168.99.3/32 mask-length-range exact\nset / routing-policy policy EXPORT_LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match family ipv4-unicast\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match prefix-set LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action accept\n\n# configuring iBGP\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp router-id 10.0.0.3\nset / network-instance default protocols bgp group IBGP\nset / network-instance default protocols bgp group IBGP export-policy EXPORT_LOOPBACK\nset / network-instance default protocols bgp group IBGP ipv4-unicast\nset / network-instance default protocols bgp group IBGP ipv4-unicast admin-state enable\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution mode require\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution allowed-tunnel-types [ ldp ]\nset / network-instance default protocols bgp neighbor 10.0.0.1\nset / network-instance default protocols bgp neighbor 10.0.0.1 admin-state enable\nset / network-instance default protocols bgp neighbor 10.0.0.1 peer-as 65001\nset / network-instance default protocols bgp neighbor 10.0.0.1 peer-group IBGP\n\ncommit save\n</code></pre> <p>The iBGP peering should establish and both <code>srl1</code> and <code>srl3</code> nodes should receive loopback prefixes and install them in the routing table. From <code>srl1</code> point of view it received the remote loopback over BGP:</p> <pre><code>A:srl1# /show network-instance default protocols bgp neighbor 10.0.0.3 received-routes ipv4                                                        \n---------------------------------------------------------------------------------------------------------------------------------------------------\nPeer        : 10.0.0.3, remote AS: 65001, local AS: 65001\nType        : static\nDescription : None\nGroup       : IBGP\n---------------------------------------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+----------------------------------------------------------------------------------------------------------------------------------------------+\n| Status         Network                Next Hop             MED          LocPref                       AsPath                       Origin    |\n+==============================================================================================================================================+\n|  u*&gt;     192.168.99.3/32        10.0.0.3                    -             100                                                         i      |\n+----------------------------------------------------------------------------------------------------------------------------------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------------\n1 received BGP routes : 1 used 1 valid\n</code></pre> <p>And installed it in the routing table. The notable difference here is that the nexthop (<code>10.0.0.3</code>) is indirect, as it is being resolved via mpls/ldp tunnel. We can even see which label will be pushed on the stack<sup>1</sup>.</p> <pre><code>A:srl1# /show network-instance default route-table ipv4-unicast prefix 192.168.99.3/32 detail                                                      \n---------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n---------------------------------------------------------------------------------------------------------------------------------------------------\nDestination   : 192.168.99.3/32\nID            : 0\nRoute Type    : bgp\nRoute Owner   : bgp_mgr\nMetric        : 0\nPreference    : 170\nActive        : true\nLast change   : 2022-03-14T09:05:46.076Z\nResilient hash: false\n---------------------------------------------------------------------------------------------------------------------------------------------------\nNext hops: 1 entries\n10.0.0.3 (indirect) resolved by tunnel to 10.0.0.3/32 (ldp)\n  via 10.1.2.2 (mpls) via [ethernet-1/1.0]\n      pushed MPLS labels : [205]\n---------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Now it is time to test the datapath with a ping between newly created loopbacks<sup>2</sup>.</p> <pre><code>--{ + running }--[  ]--                                                                                                                            \nA:srl1# ping network-instance default 192.168.99.3 -I 192.168.99.1                                                                                 \nUsing network instance default\nPING 192.168.99.3 (192.168.99.3) from 192.168.99.1 : 56(84) bytes of data.\n64 bytes from 192.168.99.3: icmp_seq=1 ttl=64 time=15.3 ms\n64 bytes from 192.168.99.3: icmp_seq=2 ttl=64 time=9.32 ms\n64 bytes from 192.168.99.3: icmp_seq=3 ttl=64 time=16.7 ms\n64 bytes from 192.168.99.3: icmp_seq=4 ttl=64 time=14.9 ms\n^C\n--- 192.168.99.3 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 9.316/14.063/16.727/2.823 ms\n</code></pre> <p>Yay! It works. Now let's see if we indeed had MPLS encapsulation used for that packet exchange. To quickly test this you can run a tcpdump on any node of the topology filtering mpls packets. For instance, let's connect to <code>srl1</code> shell via <code>docker exec</code> command and start listening for mpls packets on e1-1 interface:</p> <pre><code>docker exec -it srl1 bash\n[root@srl1 /]# tcpdump -nnvi e1-1 mpls\ntcpdump: listening on e1-1, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n09:54:03.700378 MPLS (label 205, exp 0, [S], ttl 64) # (1)!\nIP (tos 0x0, ttl 64, id 50746, offset 0, flags [DF], proto ICMP (1), length 84)\n192.168.99.1 &gt; 192.168.99.3: ICMP echo request, id 52902, seq 1, length 64\n09:54:03.707775 MPLS (label 100, exp 0, [S], ttl 63) # (2)!\nIP (tos 0x0, ttl 64, id 63961, offset 0, flags [none], proto ICMP (1), length 84)\n192.168.99.3 &gt; 192.168.99.1: ICMP echo reply, id 52902, seq 1, length 64\n09:54:04.701840 MPLS (label 205, exp 0, [S], ttl 64)\nIP (tos 0x0, ttl 64, id 50797, offset 0, flags [DF], proto ICMP (1), length 84)\n192.168.99.1 &gt; 192.168.99.3: ICMP echo request, id 52902, seq 2, length 64\n09:54:04.707592 MPLS (label 100, exp 0, [S], ttl 63)\nIP (tos 0x0, ttl 64, id 64000, offset 0, flags [none], proto ICMP (1), length 84)\n192.168.99.3 &gt; 192.168.99.1: ICMP echo reply, id 52902, seq 2, length 64\n</code></pre> <ol> <li>MPLS frame with label <code>205</code> encapsulates ICMP echo request sourced from srl1</li> <li>MPLS frame with label <code>100</code> encapsulates ICMP echo reply coming from srl2</li> </ol> <p>This evidence clearly shows the MPLS encapsulation in play. In addition to that, we have captured a pcap on <code>srl2:e-1</code> interface for ICMP packets in case you would like to look at the entire packet encapsulation and framing.</p> <p></p>"},{"location":"tutorials/mpls/mpls-ldp/ldp/#complete-lab","title":"Complete lab","text":"<p>It is great to follow the tutorial doing all the steps yourself. But maybe not every single time  For those who just want to get a looksee at the LDP-in-action we created complete config snippets for the nodes so that they can boot with everything pre-provisioned and ready.</p> <p>You can fetch the config snippets with <code>curl</code>: <pre><code>curl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl1.cfg\ncurl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl2.cfg\ncurl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl3.cfg\n</code></pre></p> <p>Put the downloaded config files next to the topology file and make sure to set up startup-config for each node:</p> <pre><code>name: mpls-ldp\nprefix: \"\"\ntopology:\ndefaults:\nkind: srl\nkinds:\nsrl:\nimage: ghcr.io/nokia/srlinux:21.11.3\ntype: ixr6\nnodes:\nsrl1:\nstartup-config: srl1.cfg\nsrl2:\nstartup-config: srl2.cfg\nsrl3:\nstartup-config: srl3.cfg\nlinks:\n- endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n- endpoints: [\"srl2:e1-2\", \"srl3:e1-1\"]\n</code></pre> <p>Deploy the lab as usual, and you should have everything ready once the lab is deployed.</p> <ol> <li> <p>the label number (205) indicates that this label comes from <code>srl2</code>, as we configured 200-299 label range block on srl2 device.\u00a0\u21a9</p> </li> <li> <p>one could also introduce linux clients to the topology, connect them to srl\u2153 nodes and test the connectivity that way.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/mpls/mpls-ldp/routing/","title":"LDP-based MPLS with SR Linux","text":"<p>Prior to any MPLS configuration, we need to set up routing in the network core. Configuration of interfaces and IGP is the core task explained in this section.</p>"},{"location":"tutorials/mpls/mpls-ldp/routing/#interfaces","title":"Interfaces","text":"<p>Let's start with basic interfaces configuration following this diagram:</p> <p>The below config snippets configure regular <code>Ethernet-1/1</code>, <code>Ethernet-1/2</code> and a special loopback <code>system0</code> interfaces.</p> srl1srl2srl3 <pre><code>enter candidate # (1)!\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.1.2.1/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.1/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre> <ol> <li>config snippets contain <code>enter candidate</code> command to switch to configuration context. At the bottom of the snippet <code>commit save</code> command will perform a <code>commit</code> operation followed by saving the running config to a startup config file.</li> </ol> <pre><code>enter candidate\n\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.1.2.2/30\n\nset / interface ethernet-1/2\nset / interface ethernet-1/2 admin-state enable\nset / interface ethernet-1/2 subinterface 0\nset / interface ethernet-1/2 subinterface 0 admin-state enable\nset / interface ethernet-1/2 subinterface 0 ipv4\nset / interface ethernet-1/2 subinterface 0 ipv4 address 10.2.3.1/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.2/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface ethernet-1/2.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre> <pre><code>enter candidate\n\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.2.3.2/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.3/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre> <p>When the interface config is committed<sup>1</sup>, routers should be able to ping each neighbor's interface address.</p> srl1 pings srl2srl2 pings srl3 <pre><code>--{ running }--[  ]--\nA:srl1# ping network-instance default 10.1.2.2\nUsing network instance default\nPING 10.1.2.2 (10.1.2.2) 56(84) bytes of data.\n64 bytes from 10.1.2.2: icmp_seq=1 ttl=64 time=49.7 ms\n</code></pre> <pre><code>--{ running }--[  ]--\nA:srl2# ping network-instance default 10.2.3.2\nUsing network instance default\nPING 10.2.3.2 (10.2.3.2) 56(84) bytes of data.\n64 bytes from 10.2.3.2: icmp_seq=1 ttl=64 time=0.033 ms\n</code></pre>"},{"location":"tutorials/mpls/mpls-ldp/routing/#igp","title":"IGP","text":"<p>With interfaces config done, proceed with configuring an IGP protocol to redistribute the loopback address information among all routers. In this tutorial, we will use IS-IS routing protocol to achieve this goal.</p> srl1srl2srl3 <pre><code>enter candidate\n\nset / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0001.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\ncommit save\n</code></pre> <pre><code>set / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0002.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 level 2\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\ncommit save\n</code></pre> <pre><code>set / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0003.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\ncommit save\n</code></pre> <p>All routers now should have enabled IS-IS adjacency with their respective neighbors, and the routing table should contain respective <code>system0.0</code> loopback addresses. A view from <code>srl2</code> side:</p> AdjacencyRouting table <pre><code>--{ running }--[  ]--\nA:srl2# show  /network-instance default protocols isis adjacency\n-----------------------------------------------------------------------------------------------------------------------\nNetwork Instance: default\nInstance        : ISIS\n+----------------+----------------+---------------+------------+--------------+-------+---------------+---------------+\n| Interface Name |    Neighbor    |   Adjacency   | Ip Address | Ipv6 Address | State |     Last      |   Remaining   |\n|                |   System Id    |     Level     |            |              |       |  transition   |   holdtime    |\n+================+================+===============+============+==============+=======+===============+===============+\n| ethernet-1/1.0 | 0000.0000.0001 | L2            | 10.1.2.1   | ::           | up    | 2022-03-13T14 | 23            |\n|                |                |               |            |              |       | :15:57.500Z   |               |\n| ethernet-1/2.0 | 0000.0000.0003 | L2            | 10.2.3.2   | ::           | up    | 2022-03-13T14 | 21            |\n|                |                |               |            |              |       | :25:50.100Z   |               |\n+----------------+----------------+---------------+------------+--------------+-------+---------------+---------------+\nAdjacency Count: 2\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The below output verifies that <code>srl2</code> has successfully received loopbacks prefixes from <code>srl1/3</code> nodes. <pre><code>--{ running }--[  ]--\nA:srl2# /show network-instance default route-table all | grep isis\n| 10.0.0.1/32 | 0    | isis      | isis_mgr            | True/success        | 10      | 18     | 10.1.2 | ethern |\n| 10.0.0.3/32 | 0    | isis      | isis_mgr            | True/success        | 10      | 18     | 10.2.3 | ethern |\n</code></pre></p> <p>With IGP setup is done, we can proceed with LDP configuration.</p> <ol> <li> <p>for instance, with <code>commit save</code> command executed from within configuration context.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/event-handler/oper-group/lab/","title":"Event Handler-based Oper Groups tutorial","text":"<p>As always, this tutorial will be backed up by a lab that readers can effortlessly deploy on their machine and follow along. Oper-group lab is contained within srl-labs/oper-group-lab repository and features:</p> <ol> <li>A Clos based fabric with 4 leaves and 2 spines, forming the fabric</li> <li>Two dual-homed clients emulated with linux containers and running <code>iperf</code> software to generate traffic</li> <li>L2 EVPN service<sup>1</sup> configured across the leaves of the fabric</li> <li>A telemetry stack to demonstrate oper-group operations in action.</li> </ol>"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#physical-topology","title":"Physical topology","text":"<p>On a physical layer topology interconnections are layed down as follows:</p> <p>Each client is dual-homed to corresponding leaves; To achieve that, interfaces <code>eth1</code> and <code>eth2</code> are formed into a <code>bond0</code> interface. On the leaves side, the access interface `Ethernet-1/1`` is part of a LAG interface that is \"stretched\" between a pair of leaves, forming a logical construct similar to MC-LAG.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#fabric-underlay","title":"Fabric underlay","text":"<p>In the underlay of a fabric leaves and spines run eBGP protocol to enable leaves to exchange reachability information for their <code>system0</code> interfaces.</p> <p>eBGP peerings are formed between each leaf and spine pair.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#fabric-overlay","title":"Fabric overlay","text":"<p>To support BGP EVPN service, in the overlay iBGP peerings with EVPN address family are established from each leaf to each spine, with spines acting as route reflectors.</p> <p>From the EVPN service standpoint, the mac-vrf instance named <code>vrf-1</code> is created on leaves and <code>ES-1</code> ethernet segment is formed from a LAG interface.</p> <p>Ethernet segments are configured to be in an all-active mode to make sure that every access link is utilized in the fabric.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#telemetry-stack","title":"Telemetry stack","text":"<p>We have enhanced the lab with a telemetry stack featuring gnmic, prometheus, and grafana - our famous GPG stack. Nothing beats real-time visualization, especially when we want to correlate events happening in the network.</p> Element Address Grafana https://localhost:3000 Prometheus https://localhost:9090"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#lab-deployment","title":"Lab deployment","text":"<p>Start with cloning lab's repository</p> <pre><code>git clone https://github.com/srl-labs/opergroup-lab.git &amp;&amp; cd opergroup-lab\n</code></pre> <p>Lab repository contains startup configuration files for the fabric nodes, as well as necessary files for the telemetry stack to come up online operational. To deploy the lab:</p> <pre><code>containerlab deploy -t opergroup.clab.yml\n</code></pre> <p>This will stand up a lab with an already pre-configured fabric using startup configs contained within <code>configs</code> directory.</p> <p>The deployed lab starts up in a pre-provisioned step, where underlay/overlay configuration has already been done. We proceed with oper-group use case exploration in the next chapter of this tutorial.</p> <ol> <li> <p>Check L2 EVPN tutorial to get the basics of L2 EVPN service configuration.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/","title":"Event Handler-based Oper Groups tutorial","text":"<p>Now that we are aware of a potential traffic blackholing that may happen in the all-active EVPN-based fabrics it is time to meet one of the remediation tactics.</p> <p>What would have helped to prevent traffic to get blackholed is to not let it be forwarded to a leaf that has no active uplinks in the first place. This may be achieved by disabling links connected to workloads as soon as uplinks become operationally disabled. This is what oper-group can do and what is depicted below.</p> <p>In this section, we will look into how a particular flavor of the oper-group feature is realized using the Event Handler framework.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#event-handler-based-oper-group","title":"Event Handler-based Oper Group","text":"<p>Starting with the 22.6.1 release SR Linux comes equipped with the Event Handler framework that allows users to write custom Python scripts and has these scripts be called in the event state paths change the value. Event Handler enables SR Linux operators to add programmable logic to handle events that happen in a system.</p> <p>The following sequence<sup>1</sup> captures the core logic of the Event-Handler framework:</p> <ol> <li>A user configures the Event Handler instance with a set of objects to monitor. The objects are referenced by their path provided in a CLI notation.</li> <li>In addition to the paths, a user may configure arbitrary static options that will parametrize a script. script</li> <li>Whenever there is a state change for any of the monitored paths, Event Handler executes a script with a single argument - a JSON string that consists of:<ul> <li>the current value of the monitored paths</li> <li>options provided by a user</li> <li>persistent data if it was set by a script</li> </ul> </li> </ol> <p>One of the first features that leverage Event Handler capability is Oper Group. As was explained in the introduction section, oper-group feature allows changing the operational status of selected ports based on the operational status of another, related, group of ports.</p> <p>Event Handler is supported in SR Linux by the <code>event_mgr</code> process which exposes configuration and state via a container at <code>.system.event-handler</code>. Within this container, a list of event handling instances can be configured at <code>.system.event-handler.instance{}</code> with a user-defined name.</p> <pre><code>--{ * candidate shared default }--[ system event-handler ]--\nA:leaf1# info\n    instance opergroup { #(1)!\n}\n</code></pre> <ol> <li>creation of <code>opergroup</code> Event Handler instance</li> </ol> <p>In this tutorial we will touch upon the most crucial configuration options:</p> <ul> <li><code>paths</code> - to select paths for monitoring state changes</li> <li><code>options</code> - to provide optional parameters to a script</li> <li><code>upython-script</code> - a path to a MicroPython script that contains the automation logic</li> </ul>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#monitored-paths","title":"Monitored paths","text":"<p>The oper-group feature requires users to define a set of uplinks that are crucial for a working service. By monitoring the state of these selected uplinks oper-group decides if the downlinks' operational state should be changed.</p> <p>In the context of this tutorial, on <code>leaf1</code> two uplink interfaces <code>ethernet-1/49</code> and <code>ethernet-1/50</code> should be put under monitoring to avoid blackholing of traffic in case their oper-state will change to a down state.</p> <p>Event Handler configuration contains <code>paths</code> leaf-list for users to select objects for monitoring. Paths should be provided in a CLI format.</p> <p>To monitor the operational state of a given interface the <code>interface &lt;interface-name&gt; oper-state</code> leaf shall be used; in our case, this requirement will translate to the following configuration:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    paths [\n\"interface ethernet-1/{49..50} oper-state\" #(1)!\n]\n</code></pre> <ol> <li>Paths can use range expansion and wildcards</li> </ol> <p>With this configuration Event Handler will subscribe to state changes for <code>oper-state</code> leaf of the two interfaces.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#options","title":"Options","text":"<p>By just monitoring the operational state of certain uplinks we don't gain much. There needs to be a coupling between the monitored uplinks and the downlinks.</p> <ul> <li>Which access links should react to state changes of the uplinks?</li> <li>How many uplinks must be \"healthy\" before we bring down access links?</li> </ul> <p>To answer these questions we need to provide additional parameters to the Event Handler and this is done via <code>options</code>.</p> <p>Options are a user-defined set of parameters that will be passed to a script along with the state of the monitored paths. For the oper-group feature we are going to define two options, that help us parametrize</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info options\n    options {\nobject down-links {\nvalues [\nethernet-1/1\n            ]\n}\nobject required-up-uplinks {\nvalue 1\n}\n}\n</code></pre> <p>To define which links should follow the state of the uplinks we provide the <code>down-links</code> option. This option is defined as a list of values to accommodate for potential support of many access links, but since our lab only has single access lint, the list.</p> <p>Note</p> <p>Values defined in options are free-formed strings and may or may not follow any particular syntax. For <code>down-links</code> option, we choose to use a CLI-compatible value of an interface since this will make it easier to create an action in the script body. But we could use any other form of the interface name.</p> <p>The second option - <code>required-up-uplinks</code> - conveys the number of uplinks we want to have in operation before we put access links down. When a leaf has more than 1 uplink, we may want to tolerate it losing a single uplink. In this tutorial, we pass a value of <code>1</code> which means that at a minimum we want to have at least one uplink to be up. In the script body, we will implement the logic of calculating the number of uplinks in an operational state, and the option is needed to provide the required boundary.</p> <p>We will also add a third option that will indicate to our script that it should print the value of certain script variables as explained later in the debug section. This option will help us explain script operations when we reach Oper group in action chapter.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#script","title":"Script","text":"<p>Event-Handler is a programmable framework that doesn't enforce any particular logic when it comes to handling events occurring in a system. Instead, users are free to create their scripts and thus program the handling of events.</p> <p>As part of the event handler instance configuration, users have to provide a path to a MicroPython script:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    admin-state enable\nupython-script opergroup.py # (1)!\n--snip--\n</code></pre> <ol> <li>A file named <code>opergroup.py</code> will be looked up in the following directories:<ul> <li><code>/etc/opt/srlinux/eventmgr/</code> for user-provided scripts</li> <li><code>/opt/srlinux/eventmgr</code> for Nokia-provided scripts.</li> </ul> </li> </ol> <p>This script will be called each time a state of any monitored paths changes.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#resulting-configuration","title":"Resulting configuration","text":"<p>When paths, options and script location are put together the Event Handler instance config takes the following shape:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    admin-state enable\nupython-script opergroup.py #(4)!\npaths [\n\"interface ethernet-1/{49..50} oper-state\" #(1)!\n]\noptions {\nobject debug { #(5)!\nvalue true\n}\nobject down-links { #(2)!\nvalues [\nethernet-1/1\n            ]\n}\nobject required-up-uplinks { #(3)!\nvalue 1\n}\n}\n</code></pre> <ol> <li>Monitor the operational state of these uplinks.</li> <li>The following links we consider \"access\" links, their operational state will depend on the state of the uplinks when processed by a script.</li> <li>Required number of uplinks to be in the oper-up state before putting down downlinks.</li> <li>Path to the script file which defines the logic of the event-handling routine using the state changes of the monitored paths and provided options.</li> <li>Debug option to indicate to a scrip that it should print additional debugging information.</li> </ol> <p>Now when the configuration is done, it is time to dive into the MicroPython code itself; at the end of the day, it is the core component of the framework.</p> <ol> <li> <p>see the sequence diagram for additional details.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-intro/","title":"Event Handler-based Oper Groups tutorial","text":"","tags":["event handler"]},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-intro/#introduction-to-oper-group","title":"Introduction to oper group","text":"Tutorial name Oper Groups with Event Handler Lab components 6 Nokia SR Linux nodes, 2 Linux nodes Resource requirements  4 vCPU  6 GB Lab srl-labs/opergroup-lab Main ref documents Event Handler Guide Version information<sup>1</sup> <code>containerlab:0.26.1</code>, <code>srlinux:22.3.2</code>, <code>docker-ce:20.10.2</code> <p>One of the most common use cases that can be covered with the Event Handler framework is known as \"Operational group\" or \"Oper-group\" for short. An oper-group feature covers several use cases, but in essence, it creates a relationship between logical elements of a network node so that they become aware of each other - forming a logical group.</p> <p>In the data center space oper-group feature can tackle the problem of traffic black-holing when leaves lose all connectivity to the spine layer. Consider the following simplified Clos topology where clients are multi-homed to leaves:</p> <p>With EVPN all-active multihoming enabled in fabric traffic from <code>client1</code> is load-balanced over the links attached to the upstream leaves and propagates via fabric to its destination.</p> <p>Since all links of a client' bond interface are active, traffic is hashed to each of the constituent links and thus utilizes all available bandwidth. A problem occurs when a leaf looses connectivity to all upstream spines, as illustrated below:</p> <p>When <code>leaf1</code> loses its uplinks, traffic from <code>client1</code> still gets sent to it since the client is not aware of any link loss problems happening on the leaf. This results in traffic blackholing on <code>leaf1</code>.</p> <p>To remedy this particular failure scenario an oper-group can be used. The idea here is to make a logical grouping between certain uplink and downlink interfaces on the leaves so that downlinks would share fate with uplink status. In our example, oper-group can be configured in such a way that leaves will shutdown their downlink interfaces should they detect that uplinks went down. This operational group's workflow depicted below:</p> <p>When a leaf loses its uplinks, the oper-group gets notified about that fact and reacts accordingly by operationally disabling the access link towards the client. Once the leaf's downlink transitions to a <code>down</code> state, the client's bond interface stops using that particular interface for hashing, and traffic moves over to healthy links. In our example, the client stops sending to <code>leaf1</code> and everything gets sent over to <code>leaf2</code>.</p> <p>In this tutorial, we will see how SR Linux's Event Handler framework enables oper-group capability.</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> </ol>","tags":["event handler"]},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/","title":"Event Handler-based Oper Groups tutorial","text":""},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#start-up","title":"Start up","text":"<p>When the Event Handler instance is configured and administratively enabled an initial sync of the monitored paths state is done. As a result of that initial sync, Event Handler immediately attempts to execute a script as it receives the state for the monitored paths.</p> <p>Users can check the status of a particular event handler instance by querying the state datastore:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# info from state /system event-handler instance opergroup\nsystem {\nevent-handler {\ninstance opergroup {\nadmin-state enable\nupython-script opergroup.py\noper-state up\nlast-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"up\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"up\\\"}],\\\"options\\\":{\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-uplinks\\\":\\\"1\\\",\\\"required-up-uplins\\\":\\\"1\\\"}}\"\nlast-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"up\\\"}}]}\"\nlast-stdout-stderr \"\"\npath [\n\"interface ethernet-1/{49..50} oper-state\"\n]\noptions {\nobject down-links {\nvalues [\nethernet-1/1\n]\n}\nobject required-up-uplinks {\nvalue 1\n}\n}\nstatistics {\nexecution-duration 0\nlast-execution \"20 minutes ago\"\ntotal-execution-duration 0\nexecution-count 2\nexecution-successes 2\n}\n}\n}\n}\n</code></pre> <p>Notable leaves in the state definition of the instance:</p> <ul> <li><code>oper-state</code> - the operational state of the instance. In case of any errors in the script and/or configuration, the state will be <code>down</code>.</li> <li><code>oper-reason</code> and <code>oper-reason-detail</code> - these leaves will contain info on the reasoning behind the event handler instance to be rendered operationally down.</li> <li><code>last-input</code> - input json string that was used during the last execution.</li> <li><code>last-stdout-stderr</code> - here you will find outputs from your script such as print statements and log messages.</li> <li><code>last-output</code> - output json string that was produced by a script during the last execution.</li> <li><code>statistics</code> - statistical information about the execution process.</li> </ul> <p>The state dump above captures the state of the <code>opergroup</code> event handler instance after the second successful run.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#running-mode","title":"Running mode","text":"<p>Let's get back to our running fabric and once again verify that we have opergroup instance configured and running before we start manipulating the uplink's state.</p> checking opergroup config <p>```js A:leaf1# info from running /system event-handler instance opergroup     system {         event-handler {             instance opergroup {                 admin-state enable                 upython-script opergroup.py                 path [                     \"interface ethernet-1/{49..50} oper-state\"                 ]                 options {                     object down-links {                         values [                             ethernet-1/1                         ]                     }                     object required-up-uplinks {                         value 1                     }                 }             }         }     }</p> <p>```</p> ensuring opergroup is running <p>```js A:leaf1# info  from state /system event-handler instance opergroup oper-state     system {         event-handler {             instance opergroup {                 oper-state up             }         }     }</p> <p>```</p>"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#disabling-one-uplink","title":"Disabling one uplink","text":"<p>Let's start first putting down a single uplink with leaving the other one operational. Our oper-group is configured in such a way that unless we lose both uplinks nothing should happen to the downstream ethernet-1/1 interface. Time to put this to test.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 60 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 60\n</code></pre> </li> <li> <p>At ~T=45s disable <code>ethernet-1/49</code> uplink interface by putting it administratively down with the following command</p> <pre><code>bash set-uplinks.sh leaf1 49 disable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p>What you should see has to resemble the following picture:</p> <p></p> <p>Initially, traffic is nicely balanced between two leaves and then even more through each leaf's uplinks. When we disable <code>ethernet-1/49</code> interface a single stream that was flowing through it got rerouted to <code>ethernet-1/50</code> and nothing impacted our streams. See how steady that line is on both access interfaces of our leaves.</p> <p>As for the Event Handler, it should've been executed a script once again, because, remember, the script runs every time any of the monitored objects change. Fetch the state of our opergroup instance and see for yourself:</p> <pre><code>A:leaf1# info from state /system event-handler instance opergroup\nsystem {\nevent-handler {\ninstance opergroup {\nadmin-state enable\nupython-script opergroup.py\noper-state up\nlast-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"down\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"up\\\"}],\\\"options\\\":{\\\"debug\\\":\\\"true\\\",\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-uplinks\\\":\\\"1\\\",\\\"required-up-uplins\\\":\\\"1\\\"}}\"\nlast-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"up\\\"}}]}\"\nlast-stdout-stderr \"num of required up uplinks = 1\ndetected num of up uplinks = 1\ndownlinks new state = up\n\"\n--snip--\n</code></pre> <p>Note, that <code>last-input</code> leaf has a <code>down</code> value for the oper-state of <code>ethernet-1/49</code>. At the same time, the <code>last-output</code> leaf that contains the output structure passed by our script indicates the <code>up</code> state for the access <code>ethernet-1/1</code> interface. That is because we have met our condition of having at least one uplink operational before putting down access links.</p> <p>The <code>last-stdout-stderr</code> leaf will show the debug statements we print out in our script to help us see which variables had which values during the script execution.</p> <ul> <li><code>num of required up uplinks = 1</code>: this value we configured via options is a constant.</li> <li><code>detected num of up uplinks = 1</code>: this is a calculated number of operational uplinks that our script performs using the input JSON string passed by Event Handler. Since one of the interfaces was down, the number of operational ones is <code>1</code>.</li> <li><code>downlinks new state = up</code>: since we met our condition and the number of operational interfaces is not less than the configured number of required active uplinks, the access interface must be operational.</li> </ul>"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#disabling-all-uplinks","title":"Disabling all uplinks","text":"<p>So far so good, now let's have a look at a case where a <code>leaf1</code> loses its second uplink. This is where we expect Event Handler to enforce and put the access interface down to prevent traffic blackholing.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 60 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 60\n</code></pre> </li> <li> <p>At ~T=30s disable <code>ethernet-1/50</code> uplink interface by putting it administratively down with the following command</p> <pre><code>bash set-uplinks.sh leaf1 50 disable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p></p> <p>That is Event Handler-based oper-group feature in action! As the annotations explain, the event of <code>ethernet-1/50</code> going down gets noticed by the Event Handler and it disables <code>leaf1</code> access link to prevent traffic from blackholing.</p> <p>All the streams that were served by <code>leaf1</code> moves to <code>leaf2</code> and no disruption is made to the TCP sessions. Iperf client reports that there were a few retransmits for the two streams that switched to the <code>leaf2</code> mid-flight, but that's it:</p> <pre><code>[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    0             sender\n[  5]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[  7]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    0             sender\n[  7]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[  9]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    3             sender\n[  9]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[ 11]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    1             sender\n[ 11]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[SUM]   0.00-60.00  sec  5.92 MBytes   828 Kbits/sec    4             sender\n[SUM]   0.00-60.00  sec  5.92 MBytes   828 Kbits/sec                  receiver\n\niperf Done.\n</code></pre> <p>On the Event Handler site we will see the following picture:</p> <pre><code>A:leaf1# info from state /system event-handler instance opergroup\nsystem {\nevent-handler {\ninstance opergroup {\nadmin-state enable\nupython-script opergroup.py\noper-state up\nlast-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"down\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"down\\\"}],\\\"options\\\":{\\\"debug\\\":\\\"true\\\",\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-link\\\":\\\"1\\\",\\\"required-up-uplinks\\\":\\\"1\\\"}}\"\nlast-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"down\\\"}}]}\"\nlast-stdout-stderr \"num of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\n\"\n</code></pre> <p>First, in the <code>last-input</code> we see that Event Handler rightfully passes the current state of both uplinks, which is <code>down</code>. Next, in the <code>last-stdout-stderr</code> field we see that the script correctly calculated that no uplinks are operational and the desired state for the downlinks is <code>down</code>. Finally, the <code>last-output</code> now lists <code>set-ephemeral-path</code> with <code>down</code> value for the access interface. This will effectively get processed by the Event Handler and put down the <code>ethernet-1/1</code> interface.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#enabling-interfaces","title":"Enabling interfaces","text":"<p>In the reverse order, let's bring both uplinks up and see what happens.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 100 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 100\n</code></pre> </li> <li> <p>At ~T=30s bring both uplinks up</p> <pre><code>bash set-uplinks.sh leaf1 \"{49..50}\" enable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p></p> <p>We started with all streams taking <code>leaf2</code> route, granted that <code>leaf1</code> access interface was operationally <code>down</code> as a result of Event Handler operation.</p> <p>Then when we brought uplinks up, Event Handler enabled access interface <code>ethernet-1/1</code> on <code>leaf1</code> and strange things happened. Instead of seeing traffic moving over to <code>leaf1</code>, we see how it moves away from <code>leaf2</code>, but doesn't pass through <code>leaf1</code>.</p> <p>The reason is that <code>leaf1</code> although got its uplinks back in an operational state, wasn't able to establish iBGP sessions and get its EVPN routes yet. Thus, traffic was getting stuck. Then iBGP sessions came up, but at this point, TCP sessions were in backoff retry mode, so they were not immediately passing through <code>leaf1</code>.</p> <p>Eventually, closer to the end of the test we see how TCP streams managed to get back in shape and spiked in bitrate to meet the bitrate goal.</p> <p>This is quite an interesting observation, because it is evident that it might not be optimal to bring the access interface up when uplinks get operational, instead, we may want to improve our oper-group script to enable the access interface only when iBGP sessions are ready, or even EVPN routes are received and installed.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/","title":"Event Handler-based Oper Groups tutorial","text":"<p>Before we meet the Event Handler framework of SR Linux and leverage it to configure oper-group feature, it is crucial to understand the problem at hand. As was mentioned in the introduction, without oper-group feature traffic loss can occur should any leaf lose all its uplinks. Let's lab a couple of scenarios that highlight a problem that oper-group is set to remedy.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#healthy-fabric-scenario","title":"Healthy fabric scenario","text":"<p>The startup configuration that our lab is equipped with gets our fabric to a state where traffic can be exchanged between clients. Users can verify that by running a simple iperf-based traffic test.</p> <p>In our lab, <code>client2</code> runs iperf3 server, while <code>client1</code> acts as a client. With the following command we can run a single stream of TCP data with a bitrate of 200 Kbps:</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K\n</code></pre> <p>Once invoked, <code>client1</code> starts to send data towards <code>client2</code> for 10 seconds, providing a report by the end of a test.</p> <p><pre><code>Connecting to host 192.168.100.2, port 5201\n[  5] local 192.168.100.1 port 55166 connected to 192.168.100.2 port 5201\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.00   sec   107 KBytes   880 Kbits/sec    0   26.9 KBytes       \n[  5]   1.00-2.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   2.00-3.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   3.00-4.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   4.00-5.00   sec   128 KBytes  1.05 Mbits/sec    0   31.1 KBytes       \n[  5]   5.00-6.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   6.00-7.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   7.00-8.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   8.00-9.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   9.00-10.00  sec   128 KBytes  1.05 Mbits/sec    0   35.4 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec   363 KBytes   298 Kbits/sec    0             sender\n[  5]   0.00-10.00  sec   363 KBytes   298 Kbits/sec                  receiver\n</code></pre> In addition to iperf results, users can monitor the throughput of `leaf\u00bd`` links using grafana dashboard: </p> <p>This visualization tells us that <code>client1</code> hashed its single stream<sup>1</sup> over <code>client1:eth2</code> interface that connects to <code>leaf2:e1-1</code>. On the \"Leaf2 e1-1 throughput\" panel in the bottom right we see incoming traffic that indicates data is flowing in via this interface.</p> <p>Next, we see that <code>leaf2</code> used its <code>e1-50</code> interface to send data over to a spine layer, through which it reaches <code>client2</code> side<sup>2</sup>.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#load-balancing-on-the-client-side","title":"Load balancing on the client side","text":"<p>Next, it is interesting to verify that client can utilize both links in its <code>bond0</code> interface since our L2 EVPN service uses an all-active multihoming mode for the ethernet segment. To test that we need to tell iperf to use at least two parallel streams; that is what <code>-P</code> flag is for.</p> <p>With the following command we start two parallel streams, 200 Kbps bitrate each, and this time for 20 seconds.</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P2 -t 20\n</code></pre> <p>Our telemetry visualization makes it clear that client-side load balancing is indeed happening as both leaves receive traffic on their <code>e-1/1</code> interface.</p> <p></p> <p><code>leaf1</code> and <code>leaf2</code> both chose to use their <code>e1-49</code> interface to send the traffic to the spine layer.</p> Load balancing in the fabric? <p>You may have noticed that when we sent two parallel streams client hashed two streams over two links in its bond interface. But then leaves used a single uplink interface towards the fabric. This is due to the fact that each leaf got a single \"stream\" and thus a single uplink interface was utilized.</p> <p>We can see ECMP in the fabric happening if we send more streams, for example, eight of them: <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 8 -t 20\n</code></pre></p> <p>That way leaves will have more streams to handle and they will load balance the streams nicely as shown in this picture.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#traffic-loss-scenario","title":"Traffic loss scenario","text":"<p>Now to the interesting part. What happens if one of the leaves suddenly loses all its uplinks while traffic is mid-flight? Will traffic be re-routed to healthy leaf? Will it be dropped? Let's lab it out.</p> <p>We will send 4 streams for 40 seconds long and somewhere in the middle we will execute <code>set-uplinks.sh</code> script which administratively disables uplinks on a given leaf:</p> <ol> <li>Start the traffic generators     <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 40\n</code></pre></li> <li>Wait ~20s for graphs to form shape</li> <li>Put down both uplinks on <code>leaf1</code> <pre><code>bash set-uplinks.sh leaf1 \"{49..50}\" disable\n</code></pre></li> <li>Monitor the traffic distribution</li> </ol> <p>Here is a video demonstrating this workflow:</p> <p>Let's see what exactly is happening there. </p> <ul> <li>[00:00 - 00:15] We started four streams 200Kbps bitrate each, summing up to 800Kbps. Those for streams were evenly distributed over the two links of a bond interface of our <code>client1</code>.     Both leaves report 400 Kbps of traffic detected on their <code>e1-1</code> interface, so each leaf handles two streams each.     Leaves then load balance these two streams over their two uplinks. We see that both <code>e1-49</code> and <code>e1-50</code> report outgoing bitrate to be ~200Kbps, which is a bitrate of a single stream we configured. That way every uplink on our leaves is utilized and handling a stream of data.</li> <li>[00:34 - 01:00] At this very moment, we execute <code>bash set-uplinks.sh leaf1 disable</code> putting uplinks on <code>leaf1</code> administratively down. The bottom left panel immediately indicates that the operational status of both uplinks went down.     But pay close attention to what is happening with traffic throughput. Traffic rate on <code>leaf1</code> access interface drops immediately, as TCP sessions of the streams it was handling stopped to receive ACKs.     At the same time, <code>leaf2</code> didn't attract any new streams, it has been handling its two streams summing up to 400Kbps all way long. This means, that traffic that was passing through <code>leaf1</code> was \"blackholed\" as <code>client1</code> was not notified in any way that one of the links in its bond interface must not be used.</li> </ul> <p>This scenario opens the stage for oper-group, as this feature provides means to make sure that a client won't use a link that is connected to a leaf that has no means to forward traffic to the fabric.</p> <ol> <li> <p>iperf3 sends data as a single stream, until <code>-P</code> flag is set.\u00a0\u21a9</p> </li> <li> <p>when you start traffic for the first time, you might wonder why a leaf that is not used for traffic forwarding gets some traffic on its uplink interface for a brief moment as shown here. Check out this link to see why is this happening.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/event-handler/oper-group/script/","title":"Event Handler-based Oper Groups tutorial","text":"<p>A MicroPython script is a central piece of the framework. It allows users to create programmable logic to handle events and thus presents a flexible interface for adding custom functionality to the Nokia SR Linux platform.</p> <p>Writing MicroPython scripts for the Event Handler is very much like writing regular Python scripts; a developer just needs to keep in mind a limited set of standard library modules available to them.</p> <p>For testing purposes, users may leverage <code>ghcr.io/srl-labs/upy:1.18</code> container image to execute their scripts against a MicroPython interpreter used in SR Linux. Granted, they add a <code>main()</code> function to their script in addition to the <code>event_hander_main()</code> function required by the framework.</p> <p>VS Code users can create a dev container with the above image to develop inside the container with MicroPython interpreter as demonstrated in opergroup-lab repo.</p> <p>Note</p> <p>Event Handler scripts may exist in two locations:</p> <ol> <li><code>/etc/opt/srlinux/eventmgr/</code> for user-provided scripts</li> <li><code>/opt/srlinux/eventmgr</code> for Nokia-provided scripts.</li> </ol> <p>No other directory hierarchy can be used.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#input","title":"Input","text":"<p>As explained in the official docs Event Handler expects to find and execute a specific function - <code>event_handler_main(in_json_str)</code> - which takes in a json string as its single argument. For the oper-group use case, the input JSON string will consist of the current state of the two uplinks and the provided options. For example, the following JSON is expected to be passed to a function when <code>ethernet-1/49</code> operational state goes to <code>down</code>:</p> <pre><code>{\n\"paths\": [\n{\n\"path\": \"interface ethernet-1/49 oper-state\",\n\"value\": \"down\"\n},\n{\n\"path\": \"interface ethernet-1/50 oper-state\",\n\"value\": \"up\"\n}\n],\n\"options\": {\n\"required-up-uplinks\": \"1\",\n\"down-links\": [\n\"ethernet-1/1\"\n]\n}\n}\n</code></pre>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#script-walkthrough","title":"Script walkthrough","text":"<p>Given the input JSON, let's have a look the script that implements the oper-group feature in its entirety.</p> <pre><code>import sys\nimport json\n# count_up_uplinks returns the number of monitored uplinks that have oper-state=up\ndef count_up_uplinks(paths):\nup_cnt = 0\nfor path in paths:\nif path.get(\"value\", \"down\") == \"up\":\nup_cnt = up_cnt + 1\nreturn up_cnt\n# required_up_uplinks returns the value of the `required-up-uplinks` option\ndef required_up_uplinks(options):\nreturn int(options.get(\"required-up-uplinks\", 1))\n# main entry function for event handler\ndef event_handler_main(in_json_str):\n# parse input json string passed by event handler\nin_json = json.loads(in_json_str)\npaths = in_json[\"paths\"]\noptions = in_json[\"options\"]\nnum_up_uplinks = count_up_uplinks(paths)\ndownlinks_new_state = (\n\"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n)\n# add `debug=\"true\"` option to event-handler configuration to output parsed parameters\nif options.get(\"debug\") == \"true\":\nprint(\nf\"num of required up uplinks = {required_up_uplinks(options)}\\n\\\ndetected num of up uplinks = {num_up_uplinks}\\n\\\ndownlinks new state = {downlinks_new_state}\"\n)\nresponse_actions = []\nfor downlink in options.get(\"down-links\", []):\nresponse_actions.append(\n{\n\"set-ephemeral-path\": {\n\"path\": f\"interface {downlink} oper-state\",\n\"value\": downlinks_new_state,\n}\n}\n)\nresponse = {\"actions\": response_actions}\nreturn json.dumps(response)\n</code></pre>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#parsing-input-json","title":"Parsing input JSON","text":"<p>Starting with the <code>event_handler_main</code> func we parse the incoming JSON string and extracting the relevant portions:</p> <pre><code>in_json = json.loads(in_json_str)\npaths = in_json[\"paths\"]\noptions = in_json[\"options\"]\n</code></pre> <p>Paths and Options are the only objects in the incoming JSON, which we respectfully save in the like-named variables.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#evaluating-the-desired-state-of-downlinks","title":"Evaluating the desired state of downlinks","text":"<p>With the input parsed, we enter the central piece of the script where we make a decision on what state should the access links be in, given the inputs we received.</p> <pre><code>num_up_uplinks = count_up_uplinks(paths)\ndownlinks_new_state = (\n\"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n)\n</code></pre> <p>First, we count the number of uplinks in oper-state up, this is done with <code>count_up_uplinks()</code> function which simply walks through the current state of the uplinks passed into the script by the Event Handler.</p> <pre><code># count_up_uplinks returns the number of monitored uplinks that have oper-state=up\ndef count_up_uplinks(paths):\nup_cnt = 0\nfor path in paths:\nif path.get(\"value\", \"down\") == \"up\":\nup_cnt = up_cnt + 1\nreturn up_cnt\n</code></pre> <p>When we calculated how many uplinks are operationally up, we can decide what state should the downlinks be in. To rule that decision we compare the number of operational uplinks with the required number of uplinks passed via options:</p> <pre><code>downlinks_new_state = (\n\"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n)\n</code></pre> <p>If the required number of operational uplinks is less than the required number of them, we should put down downlinks to prevent traffic blackholing. On the other hand, if the number of operational uplinks is &gt;= the required number of uplinks, we should bring the access links up.</p> <p>The desired state of the downlinks is saved in <code>downlinks_new_state</code> variable.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#debugging","title":"Debugging","text":"<p>It is useful to take a pause here and embed some debugging log outputs for the key variables of a script. In our case, we've added a print statement that dumps important variables of our script.</p> <pre><code># add `debug=\"true\"` option to event-handler configuration to output parsed parameters\nif options.get(\"debug\") == \"true\":\nprint(\nf\"num of required up uplinks = {required_up_uplinks(options)}\\n\\\ndetected num of up uplinks = {num_up_uplinks}\\n\\\ndownlinks new state = {downlinks_new_state}\"\n)\n</code></pre> <p>The debug log will only be present if the <code>debug</code> option will be set to <code>\"true\"</code> in the Event Handler instance config. You will be able to find this log output by using this CLI command:</p> <pre><code>info from state /system event-handler instance opergroup last-stdout-stderr\n</code></pre>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#composing-output","title":"Composing output","text":"<p>At this point, our script is able to define the desired state of the downlinks, based on the state of the user-defined uplinks and the required number of healthy uplinks. For the Event Handler to take any action, the script needs to output a JSON string following the expected format.</p> <pre><code>response_actions = []\nfor downlink in options.get(\"down-links\", []):\nresponse_actions.append(\n{\n\"set-ephemeral-path\": {\n\"path\": f\"interface {downlink} oper-state\",\n\"value\": downlinks_new_state,\n}\n}\n)\nresponse = {\"actions\": response_actions}\nreturn json.dumps(response)\n</code></pre> <p>This code snippet shows the way to create an output JSON, using the calculated <code>downlinks_new_state</code> and the list of downlinks provided via <code>down-links</code> option. We range over the down-links option to append a structure that Event Handler expects to see in output JSON and using <code>set-ephemeral-path</code> action that will set oper state of the downlinks to the desired value (up or down).</p> <p>The output is provided via <code>response</code> dictionary, that we marshal to JSON encoding at the end before returning from the function. This routine will provide a JSON back to the Event Handler and since it is formed in a well-known way, Event Handler will process and execute the actions passed to it.</p> <p>Consequently, by receiving back a list of actions from the script, Event Handler will implement the oper-group feature when a state of a group of downlinks is derived from the state of a group of uplinks.</p>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#summary","title":"Summary","text":"<p>Let's take a few input examples and see which outputs will be generated by the script to better understand the logic of the automation.</p> <p>We start in a healthy state with both uplinks in operation and oper-group event handler configured as per the previous steps.</p> <p>In the event of a single uplink interface going operationally down:</p> Input JSONCalculated parametersOutput JSON <pre><code>{\n\"paths\": [\n{\n\"path\": \"interface ethernet-1/49 oper-state\",\n\"value\": \"down\"\n},\n{\n\"path\": \"interface ethernet-1/50 oper-state\",\n\"value\": \"up\"\n}\n],\n\"options\": {\n\"required-up-uplinks\": \"1\",\n\"down-links\": [\n\"ethernet-1/1\"\n]\n}\n}\n</code></pre> <p>*Number of required uplinks doesn't change as it is an option provided as user input. It is always <code>1</code> in our case. * Detected number of uplinks in operational state equals <code>1</code>, as we range through the <code>paths</code> in the incoming JSON and count paths which have <code>up</code> value for the <code>interface ethernet-* oper-state</code> leaf. * Downlinks' new state should be <code>\"up\"</code>, since we still have a minimum number of operational uplinks = <code>1</code>.</p> <pre><code>{\n\"actions\": [\n{\n\"set-ephemeral-path\": {\n\"path\": \"/interface ethernet-1/1 oper-state\",\n\"value\": \"up\",\n}\n}\n]\n}\n</code></pre> <p>Then let's see what happens if the second uplink goes down.</p> Input JSONCalculated parametersOutput JSON <pre><code>{\n\"paths\": [\n{\n\"path\": \"interface ethernet-1/49 oper-state\",\n\"value\": \"down\"\n},\n{\n\"path\": \"interface ethernet-1/50 oper-state\",\n\"value\": \"down\"\n}\n],\n\"options\": {\n\"required-up-uplinks\": \"1\",\n\"down-links\": [\n\"ethernet-1/1\"\n]\n}\n}\n</code></pre> <p>*Number of required uplinks doesn't change as it is an option provided as user input. It is always <code>1</code> in our case. * Detected number of uplinks in operational state equals <code>0</code>, as we range through the <code>paths</code> in the incoming JSON and count paths which have <code>up</code> value for the <code>interface ethernet-* oper-state</code> leaf. * Downlinks' new state should be <code>\"down\"</code>, since the number of operational uplinks (<code>0</code>) is less than the required number of operational uplinks.</p> <pre><code>{\n\"actions\": [\n{\n\"set-ephemeral-path\": {\n\"path\": \"/interface ethernet-1/1 oper-state\",\n\"value\": \"down\",\n}\n}\n]\n}\n</code></pre>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#off-box-testing","title":"Off-box testing","text":"<p>Although it is absolutely possible to test Event Handler scripts using containerized SR Linux image, it makes a lot of sense to test the script off-box.</p> <p>Since scripts are provided with a known input JSON structure, we can pass it to a script's <code>main()</code> function as if it was provided by the Event Manager itself. Consider the following code snippet that is part of the opergroup.py script we just walked through:</p> <pre><code>def main():\nexample_in_json_str = \"\"\"\n{\n    \"paths\": [\n        {\n            \"path\":\"interface ethernet-1/49 oper-status\",\n            \"value\":\"down\"\n        },\n        {\n            \"path\":\"interface ethernet-1/50 oper-status\",\n            \"value\":\"down\"\n        }\n    ],\n    \"options\": {\n        \"required-up-uplinks\":1,\n        \"down-links\": [\n            \"Ethernet-1/1\",\n            \"Ethernet-1/2\"\n        ],\n        \"debug\": \"true\"\n    }\n}\n\"\"\"\njson_response = event_handler_main(example_in_json_str)\nprint(f\"Response JSON:\\n{json_response}\")\nif __name__ == \"__main__\":\nsys.exit(main())\n</code></pre> <p>Since Event Handler's entrypoint is <code>event_handler_main()</code> func, we can create a <code>main()</code> function that contains a variable with a JSON-encoded string that follows the schema of the input argument; this variable is then passed to the <code>event_handler_main()</code> simulating Event Handler invokation. In essence, we are mocking the Event Handler and provide a hand-crafted input JSON to the <code>event_handler_main()</code> function.</p> <p>Now, we can test our script on any system that has Python/MicroPython installed, for example:</p> Testing with PythonTesting with MicroPyton <p>Given your script doesn't use any non supported by MicroPython libraries, you may use Python3 installed on any system to test your script. For example: <pre><code>\u276f python3 opergroup.py\nnum of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\nResponse JSON:\n{\"actions\": [{\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/1 oper-state\", \"value\": \"down\"}}, {\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/2 oper-state\", \"value\": \"down\"}}]}\n</code></pre></p> <p>Testing with MicroPython is advised, as this will guarantee that the code will work on SR Linux. Feel free to install Unix port of MicroPython or leverage <code>srl-labs/upy:1.18</code> container image:</p> <p><pre><code>docker run -it  -v $(pwd):/workdir ghcr.io/srl-labs/upy:1.18 micropython opergroup.py\nnum of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\nResponse JSON:\n{\"actions\": [{\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/1 oper-state\", \"value\": \"down\"}}, {\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/2 oper-state\", \"value\": \"down\"}}]}\n</code></pre> To pretty print the output, use <code>jq</code>: <pre><code>docker exec -it dc6ded4ed7ff bash -c \"micropython opergroup.py | tail -1 | jq .\"\n</code></pre> <pre><code>{\n\"actions\": [\n{\n\"set-ephemeral-path\": {\n\"path\": \"interface Ethernet-1/1 oper-state\",\n\"value\": \"down\"\n}\n},\n{\n\"set-ephemeral-path\": {\n\"path\": \"interface Ethernet-1/2 oper-state\",\n\"value\": \"down\"\n}\n}\n]\n}\n</code></pre></p>"},{"location":"tutorials/programmability/event-handler/oper-group/script/#script-delivery","title":"Script delivery","text":"<p>Scripts created by users must be delivered to the SR Linux nodes and available by the well-known location. Any file transfer technique can be used to deliver the source files/packages.</p> <p>When using containerlab, users may take advantage of the <code>binds</code> option of a node and bind mount the script to its location. This is exactly how we do it in the opergroup-lab:</p> <pre><code>name: opergroup\ntopology:\nnodes:\nleaf1:\nbinds:\n- opergroup.py:/etc/opt/srlinux/eventmgr/opergroup.py\n</code></pre>"},{"location":"tutorials/programmability/event-handler/oper-group/summary/","title":"Event Handler-based Oper Groups tutorial","text":"<p>Event-driven automation is a popular paradigm in the networks field. One practical implementation of that paradigm is Nokia SR Linux Event Handler framework that allows users to programmatically react to events happening in a network OS.</p> <p>This tutorial covers Event Handler concepts by explaining how they can be used to implement Operational Group feature.</p> <p>Theoretical data is backed by a containerlab-based lab that we exclusively use throughout the tutorial. Readers can therefore repeat every step in their own time.</p> <p>Before explaining how to configure an event handler-based oper-group instance, we first explain what problem oper-group is set to fix.</p> <p>Once the problem statement is set, we proceed with configuration steps for the event handler instance.</p> <p>A key piece of the Event Handler framework is the script that is getting executed every time an event to which users subscribed happens. In the Script chapter we explain how oper-group script is composed.</p> <p>Finally, it is time to see how the Event Handler instance powered by the oper-group script works. We follow through with the various scenarios and capture the behavior of the fabric.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/","title":"Using Ansible's URI module with SR Linux's JSON-RPC Interface","text":"<p>Warning</p> <p>This is an original tutorial that predate <code>nokia.srlinux</code> collection. It uses Ansible's <code>uri</code> module to interact with SR Linux's JSON-RPC interface. It is recommended to use <code>nokia.srlinux</code> collection instead.</p> <p>A new version of this tutorial is available here.</p> Summary Tutorial name JSON-RPC with Ansible Lab components 2 Nokia SR Linux nodes Resource requirements  2 vCPU  4 GB Lab jsonrpc-ansible Main ref documents JSON-RPC Configuration, JSON-RPC ManagementAnsible URI module Version information<sup>1</sup> <code>srlinux:22.11.1</code>, <code>containerlab:0.33.0</code>, <code>ansible:v6.6</code> Authors Roman Dodin  Discussions  Twitter \u00b7  LinkedIn <p>In the JSON-RPC Basics tutorial, we focused on the JSON-RPC interface mechanics and its capabilities. The examples we used there used a well-known <code>curl</code> command-line utility to put the focal point on the JSON-RPC itself and some automation framework.</p> <p>Arguably, using <code>curl</code> for network automation tasks that aren't trivial may be challenging and likely lead to hairy bash scripting. Instead, network ops teams prefer to use home-grown automation that leverages programming languages or configuration management tools like Ansible<sup>2</sup> fitted to the networking purpose.</p> <p>Ansible for network automation?</p> <p>We should mention that using Ansible for network automation might feel like a shortcut to automation nirvana with both infra and network domains automated via a single cfg management tool, but this might be a trap.</p> <p>From our experience using Ansible for network automation may work great when your automation tasks are trivial and do not require advanced configuration or state management. Programming in Ansible is tricky at best, and we advise you to consider using general-purpose languages instead (Python, Go, etc).</p> <p>Still, network teams who have experience with Ansible may work around its limitations and make the tool do the job without falling into a trap of troubleshooting playbooks, variable shadowing and jinja-programming.</p> <p>The topic of this tutorial is exactly this - using Ansible and SR Linux's JSON-RPC interface to automate common network operations. This task-oriented tutorial will help you understand how Ansible can be used to perform day0+ operations on our magnificent Nokia SR Linux Network OS.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#gnmi-or-json-rpc","title":"gNMI or JSON-RPC?","text":"<p>Ansible has been marketing itself as a framework suitable for network automation. We've seen lots of network platforms integrated with Ansible using custom galaxy collections. There is no point in arguing if Ansible is the right tool of choice when network automation branches out as a separate netops discipline. If Ansible does the job for certain netops teams, our task it to help them understand how it can be used with SR Linux.</p> <p>At the time of this writing, SR Linux provides three management interfaces:</p> <ul> <li>gNMI</li> <li>JSON-RPC</li> <li>CLI</li> </ul> <p>We've discussed before how these interfaces have the same visibility, but which one to pick for Ansible?</p> <p>A few years back, Nokia open-sourced the <code>nokia.grpc</code> galaxy collection to add gNMI support to Ansible. Unfortunately, due to the complications in the upstream python-grpcio library<sup>3</sup>, this plugin was not widely used in the context of Ansible. In addition to that limitation, Ansible host has to be provided with gRPC libraries as dependencies, which might be problematic for some users. Having said that, it is still possible to use this collection.</p> <p>In contrast with gNMI, which requires a custom collection to operate, using JSON-RPC with Ansible is easy; the HTTP client is part of the Ansible core URI module and both secured and unsecured transports are possible. Also bear in mind, that the performance that gNMI offers is not of critical importance for Ansible-based network automation stacks. Add to the mix JSON-RPC's ability to call out CLI commands via reliable HTTP transport and it makes it easy to converge on this interface as far as Ansible is concerned.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#lab-deployment","title":"Lab deployment","text":"<p>If this is not your first tutorial on this site, you rightfully expect to get a containerlab-based lab provided so that you can follow along with the provided examples. The lab defines two Nokia SR Linux nodes connected over <code>ethernet-1/1</code> interfaces.</p> <p>To deploy the lab clone the repository and do <code>containerlab deploy</code> from within the repository's directory. Shortly after you should have two SR Linux containers running:</p> Result of containerlab deploy command<pre><code>+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n| # |      Name      | Container ID |             Image             | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n| 1 | clab-2srl-srl1 | cc5ba5f8cc04 | ghcr.io/nokia/srlinux:22.11.1 | srl  | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 2 | clab-2srl-srl2 | aa5f8626ac4b | ghcr.io/nokia/srlinux:22.11.1 | srl  | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#ansible-setup","title":"Ansible setup","text":""},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#inventory","title":"Inventory","text":"<p>The reason our lab has two nodes is to leverage Ansible's inventory. The two nodes that are deployed by containerlab fit nicely in the simplest inventory ever. We use the names of the containers as containeralb reports back to us and shovel them in the YAML format of the inventory:</p> inventory.yml file<pre><code>all:\nhosts:\nclab-2srl-srl1:\ne1_1_ip: 192.168.0.1/24\nclab-2srl-srl2:\ne1_1_ip: 192.168.0.2/24\n</code></pre> <p>We put a variable <code>e1_1_ip</code> for each host, as later we would like to use the values of these variables in the configuration tasks.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#container","title":"Container","text":"<p>Ansible is infamous for breaking things when you least expect it. For that reason we put that beast in a container cage.</p> <p>We built a container image with Ansible v6.6.0 and are going to use it throughout this tutorial via a runner script <code>ansible.sh</code> that simply calls a <code>docker run</code> command with a few args:</p> ansible-in-a-container runner script<pre><code>docker run --rm -it \\\n-v $(pwd):/ansible \\ #(1)!\n-v ~/.ssh:/root/.ssh \\ #(2)!\n-v /etc/hosts:/etc/hosts \\ #(3)!\nghcr.io/hellt/ansible:6.6.0 ansible-playbook -i inventory.yml $@\n</code></pre> <ol> <li><code>/ansible</code> is a working dir for our container image, so we mount the repo's directory to this path.</li> <li>although not needed for this lab, we still mount ssh dir of the host to the container, in case we need key-based ssh access</li> <li>to make sure that Ansible container can reach the nodes deployed by containerlab we mount the <code>/etc/hosts</code> file to it. That way ansible inside the container can resolve node names to IP addresses.</li> </ol> <p>Note</p> <p>With Ansible running in a container connected to the default bridge network and the rest of the nodes running in the <code>clab</code> docker network users may experience communication problems between Ansible and network elements. This stems from the default iptables rules Docker maintains preventing container communications between different networks. To overcome this, consider one of the following methods (one of them, not all):</p> <ol> <li>Install iptables rule to allow packets to <code>docker0</code> network:    <pre><code>sudo iptables -I DOCKER-USER -o docker0 -j ACCEPT -m comment --comment \"allow inter-network comms\"\n</code></pre></li> <li>Instruct containerlab to start nodes in the Docker default network.</li> <li>Run Ansible container in the network that containerlab uses (<code>clab</code> by default)</li> </ol> <p>Using this container image is not required for this tutorial, you still can install Ansible using any of the supported methods.</p> <p>With the container image, we tried to make sure you will have one problem less to worry about.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#uri-module","title":"URI module","text":"<p>One of the biggest advantages of JSON-RPC interface is that it uses HTTP1.1 transport that every automation platform or programming language has native support for. In Ansible, the HTTP client is implemented with the builtin URI module. We will solely use this core module of Ansible to interact with SR Linux's JSON-RPC interface.</p> <p>Here is an example of a playbook with a single play utilizing this module could look like.</p> <pre><code>- name: Get state data from SR Linux\nhosts: all\nconnection: local\ngather_facts: no\ntasks:\n- name: Get hostname and version\nansible.builtin.uri: #(1)!\nurl: http://{{inventory_hostname}}/jsonrpc #(2)!\nurl_username: admin #(3)!\nurl_password: NokiaSrl1!\nmethod: POST\nbody: #(4)!\njsonrpc: \"2.0\"\nid: 1\nmethod: get\nparams:\ndatastore: state\ncommands: #(5)!\n- path: /system/name/host-name\n- path: /system/information/version\n- path: /system/json-rpc-server/network-instance[name=mgmt]/https/tls-profile\ndatastore: running\nbody_format: json #(6)!\nregister: get_result #(7)!\n- ansible.builtin.debug: #(8)!\nmsg: \"Host {{get_result.json.result[0]}} runs {{get_result.json.result[1]}} version\"\n</code></pre> <ol> <li>Fully qualified module name</li> <li>URL to use. See basics tutorial.</li> <li>Credentials for user authentication.</li> <li>Body of the request as per basics tutorial.</li> <li>Commands to use in the RPC. See basics tutorial for additional information.</li> <li><code>body_format=json</code> will encode the body data structure to json string.</li> <li>Registering the result will make it possible to extract the results of the RPC in the subsequent tasks.</li> <li>In case of a successful RPC invocation, the resulting data structure will contain <code>json</code> key which provides access to the <code>results</code> list.</li> </ol>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#tasks","title":"Tasks","text":"<p>Once the lab repository is cloned and the lab is deployed, we are ready to start solving the tasks using Ansible and JSON-RPC. Note, that the tasks will be solved straightforwardly without using clever Ansible features; the goal of this tutorial is to understand how to leverage JSON-RPC interface of SR Linux, and not how to effectively use Ansible in general.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#retrieving-state-and-config","title":"Retrieving state and config","text":"<p>Beginning with the common task of retrieving state and configuration data from SR Linux using model-based paths.</p> <p>Playbook <code>task01/get-state-and-config-data.yml</code>:</p> <pre><code>- name: Get state and config data from SR Linux\nhosts: all\nconnection: local\ngather_facts: no\ntasks:\n- name: Get hostname, version and tls-profile\nansible.builtin.uri:\nurl: http://{{inventory_hostname}}/jsonrpc\nurl_username: admin\nurl_password: NokiaSrl1!\nmethod: POST\nbody:\njsonrpc: \"2.0\"\nid: 1\nmethod: get\nparams:\ndatastore: state\ncommands:\n- path: /system/name/host-name\n- path: /system/information/version\n- path: /system/json-rpc-server/network-instance[name=mgmt]/https/tls-profile\ndatastore: running\nbody_format: json\nregister: get_result\n- ansible.builtin.debug:\nmsg: \"Host {{get_result.json.result[0]}} runs {{get_result.json.result[1]}} version and json-rpc server uses '{{get_result.json.result[2]}}' TLS profile\"\n</code></pre> <p>In the <code>Get hostname, version and tls-profile</code> task we craft the body payload with multiple commands, each of which is targeting a certain leaf. Note, how we provided <code>state</code> datastore on the global level and override it in the 3<sup>rd</sup> command where we needed to use <code>running</code> datastore. Read more on datastores here.</p> <p>To execute the playbook:</p> <pre><code>./ansible.sh task01/get-state-and-config-data.yml\n</code></pre> <p>The result of the playbook will contain the message string per each lab node with the relevant information:</p> <pre><code>TASK [ansible.builtin.debug] ***********************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": \"Host srl1 runs v22.11.1-184-g6eeaa254f7 version and json-rpc server uses 'clab-profile' TLS profile\"\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": \"Host srl2 runs v22.11.1-184-g6eeaa254f7 version and json-rpc server uses 'clab-profile' TLS profile\"\n}\n</code></pre> <p>Of course, your paths might not necessarily point to a leaf, it can be a conatiner, a list, or other YANG element. We used leaves in the example to demonstrate how to access the data in the response.</p> <p>Based on the provided example, users can fetch any configuration or state data available in SR Linux.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#configuration-backup","title":"Configuration backup","text":"<p>Another bestseller task in the network operations category - configuration backup. In SR Linux, the running configuration is what populates the <code>running</code> datastore. We can easily fetch the entire <code>running</code> datastore by using the <code>/</code> path and the Get method of JSON-RPC.</p> <p>Playbook <code>task02/cfg-backup.yml</code> demonstrates how this is can be done.</p> snip from the task02/cfg-backup.yml<pre><code>body:\njsonrpc: \"2.0\"\nid: 1\nmethod: get\nparams:\ndatastore: running\ncommands:\n- path: /\n</code></pre> <p>Once the whole running datastore is fetched we write (using the copy module) it to the ansible host filesystem to the same directory where the playbook is located:</p> <pre><code>- name: Save fetched configs\nansible.builtin.copy:\ncontent: \"{{get_result.json.result[0] | to_nice_json}}\"\ndest: \"{{playbook_dir}}/{{inventory_hostname}}.cfg.json\"\n</code></pre> <p>As a result, running configs of the two nodes are written to the <code>task02</code> directory:</p> <pre><code>PLAY [Configuration backup] **************************************************************************************\n\nTASK [Backup running configuration] ******************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Save fetched configs] **************************************************************************************\nchanged: [clab-2srl-srl1]\nchanged: [clab-2srl-srl2]\n\nPLAY RECAP *******************************************************************************************************\nclab-2srl-srl1             : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\nclab-2srl-srl2             : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <pre><code>\u276f ls task02\ncfg-backup.yml  clab-2srl-srl1.cfg.json  clab-2srl-srl2.cfg.json\n</code></pre> <p>Note</p> <p>Alternative approach to do configuration backup is to transfer the config file that resides on the file system of the Network OS.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#setting-configuration","title":"Setting configuration","text":"<p>Configuration tasks are not the white crows either. Many Ansible operators provision their network devices using popular techniques such as configuration templating.</p> <p>In <code>task03/config.yml</code> playbook we switch to the JSON-RPC's Set method and demonstrate different sourcing of configuration data.</p> <p>The body of our request contains three different commands which demonstrate various ways of changing the configuration on the device.</p> <pre><code>body:\njsonrpc: \"2.0\"\nid: 1\nmethod: set\nparams:\ncommands:\n- action: replace\npath: /interface[name=mgmt0]/description:{{inventory_hostname}} management interface\n- action: update\npath: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\n- action: update\npath: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/iface-cfg.json.j2') }}\"\nbody_format: json\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#path-embedded-value","title":"Path-embedded value","text":"<p>The first command:</p> <pre><code>- action: replace\npath: /interface[name=mgmt0]/description:{{inventory_hostname}} management interface\n</code></pre> <p>is a <code>replace</code> action that embeds the value of the leaf we set in the <code>path</code> field. Note, that templating can be used throughout the playbook, which we leverage to customize the description value.</p> <p>This is the most simple way of setting the configuration for a given leaf.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#value-container","title":"Value container","text":"<p>A little bit more interesting case is shown with the 2<sup>nd</sup> command, which updates several fields under <code>/system/information</code> container:</p> <pre><code>- action: update\npath: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\n</code></pre> <p>We set two leaves - <code>location</code> and <code>contact</code> - by creating the <code>value</code> container which embeds leaves and values we want to set under the <code>/system/information</code> path.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#config-sourced-from-a-file","title":"Config sourced from a file","text":"<p>Quite often, configuration templates that are intended to be pushed to the node are saved on disk. Hence, we would like to show how to use those tempaltes and configure SR Linux.</p> <p>The <code>iface-cfg.json.j2</code> file contains a template for two interfaces - <code>ethernet-1/1</code> and <code>lo0</code>. The <code>ethernet-1/1</code> interface is configured with a sub-interface and IP address sourced from the variable we put in the inventory and is attached to the default network instance.</p> <pre><code>{\n\"interface\": [\n{\n\"name\": \"ethernet-1/1\",\n\"description\": \"ethernet-1/1 interface on {{inventory_hostname}} node\",\n\"admin-state\": \"enable\",\n\"subinterface\": [\n{\n\"index\": 0,\n\"admin-state\": \"enable\",\n\"ipv4\": {\n\"address\": [\n{\n\"ip-prefix\": \"{{e1_1_ip}}\"\n}\n]\n}\n}\n]\n},\n{\n\"name\": \"lo0\",\n\"description\": \"loopback interface on {{inventory_hostname}} node\",\n\"admin-state\": \"enable\"\n}\n],\n\"network-instance\": [\n{\n\"name\": \"default\",\n\"interface\": [\n{\n\"name\": \"ethernet-1/1.0\"\n}\n]\n}\n]\n}\n</code></pre> <p>Since <code>/interface</code> list is a top-level element in our YANG model (as denoted by <code>/interface</code> path), to create a new member of this list, we craft a JSON object with <code>interface</code> list and specify its members (<code>ethernet-1/1</code> and <code>lo0</code>). This JSON object is then updates/merges the <code>/</code> path, thus making two new interfaces. Same with the <code>network-instance</code>.</p> <pre><code>- action: update\npath: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/iface-cfg.json.j2') }}\"\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#error-handling","title":"Error handling","text":"<p>To catch potential errors that might happen during config provisioning a task that fails when error is returned by JSON-RPC is part of the playbook.</p> <pre><code>- name: Stop if request contains error\nansible.builtin.fail:\nmsg: \"Error: {{set_result.json.error.message}}\"\nwhen: set_result.json.error is defined\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#results-validation","title":"Results validation","text":"<p>Run the playbook with <code>./ansible.sh task03/config.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task03/config.yml\n\nPLAY [Configuration] ************************************************************************************************************************\n\nTASK [Various configuration tasks] **********************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Stop if request contains error] *******************************************************************************************************\nskipping: [clab-2srl-srl1]\nskipping: [clab-2srl-srl2]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": [\n        \"mgmt0 description is: clab-2srl-srl1 management interface\",\n        \"location is: the Netherlands\",\n        \"contact is: Roman Dodin\",\n        \"ethernet-1/1 description is: ethernet-1/1 interface on clab-2srl-srl1 node\",\n        \"loopback0 description is: loopback interface on clab-2srl-srl1 node\"\n    ]\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": [\n        \"mgmt0 description is: clab-2srl-srl2 management interface\",\n        \"location is: the Netherlands\",\n        \"contact is: Roman Dodin\",\n        \"ethernet-1/1 description is: ethernet-1/1 interface on clab-2srl-srl2 node\",\n        \"loopback0 description is: loopback interface on clab-2srl-srl2 node\"\n    ]\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre> <p>To ensure that the changes were applied properly the last task fetches state information for the leaves that were touched. Looking at the output we can verify that the config was set correctly.</p> <p>Additionally, since we configured IP addresses over the connected interfaces on both nodes, you should be able to execute a ping between the nodes:</p> <pre><code>--{ + running }--[  ]--\nA:srl1# ping 192.168.0.2 network-instance default  Using network instance default\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=90.3 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=12.6 ms\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#replacing-partial-config","title":"Replacing partial config","text":"<p>To replace portions of a config a Set method with <code>replace</code> operation is used.</p> <p>In <code>task04/replace-partial-cfg.yml</code> playbook we replace everything that was configured in the configuration task with a single leaf <code>admin-state: disable</code>.</p> <pre><code>body:\njsonrpc: \"2.0\"\nid: 1\nmethod: set\nparams:\ncommands:\n- action: replace\npath: /interface[name=ethernet-1/1]\nvalue:\nname: ethernet-1/1\nadmin-state: disable\n- action: delete\npath: /network-instance[name=default]/interface[name=ethernet-1/1.0]\n</code></pre> <p>The replace action will delete everything under <code>/interface[name=ethernet-1/1]</code> and update it with the value specified in the request. We also remove the binding of the subinterface <code>ethernet-1/1.0</code> as it is about to be removed as the result of our replace operation.</p> <p>Run the playbook with <code>./ansible.sh task04/replace-partial-cfg.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task04/replace-partial-cfg.yml\n\nPLAY [Replace operation] ********************************************************************************************************************\n\nTASK [Replace partial config] ***************************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [Stop if request contains error] *******************************************************************************************************\nskipping: [clab-2srl-srl1]\nskipping: [clab-2srl-srl2]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"get_result.json.result\": [\n        {\n            \"admin-state\": \"disable\"\n        }\n    ]\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"get_result.json.result\": [\n        {\n            \"admin-state\": \"disable\"\n        }\n    ]\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#replacing-entire-config","title":"Replacing entire config","text":"<p>One of the common tactics to provision a new box with services after it has been ZTP'd is to replace whatever is there with the golden or node-specific config.</p> <p>In <code>task05/replace-entire-cfg.yml</code> we do just that. The golden config that resides in the task directory has jinja variables that Ansible populates at runtime to add node-specific values. The golden config we used in this example is similar to what we retrieve in the backup task.</p> <pre><code>body:\njsonrpc: \"2.0\"\nid: 1\nmethod: set\nparams:\ncommands:\n- action: replace\npath: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/golden.cfg.json.j2') }}\"\n</code></pre> <p>To replace the entire config we set the path to <code>/</code> value and provide the entire config in the value field.</p> <p>Run the playbook with <code>./ansible.sh task05/replace-entire-cfg.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task05/replace-entire-cfg.yml\n\nPLAY [Replace operation] ********************************************************************************************************************\n\nTASK [Replace partial config] ***************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Stop if request contains error] *******************************************************************************************************\nskipping: [clab-2srl-srl1]\nskipping: [clab-2srl-srl2]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": \"Location from golden config: CONTAINERLAB\"\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": \"Location from golden config: CONTAINERLAB\"\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#collecting-show-commands","title":"Collecting <code>show</code> commands","text":"<p>Another golden record of the netops is to dump some operational <code>show</code> commands for audit, pre/post checks, etc. Using JSON-RPC's CLI method we can execute <code>show</code> and other commands with output format being json, text or table.</p> <p>In <code>task06/fetch-show-cmd-output.yml</code> we collect the output of a few <code>show</code> commands in text outputs and save them in per-node files.</p> <pre><code>- name: Operational commands\n# --snip--\nvars:\ncommands:\n- show version\n- show platform chassis\ntasks:\n- name: Fetch show commands output\nansible.builtin.uri:\n# --snip--\nbody:\njsonrpc: \"2.0\"\nid: 1\nmethod: cli\nparams:\ncommands: \"{{commands}}\"\noutput-format: text\nbody_format: json\nregister: cli_result\n</code></pre> <p>For this playbook we introduce playbook variable - <code>commands</code> - that host a list of show commands we would like to execute remotely. In the body part of the request we use the CLI method and our commands refer to the variable.</p> <p>To save the results of the executed commands we loop over the results array (see CLI method examples for response format explanation) and save each result in its own file with a sanitized name:</p> <pre><code>- name: Save fetched show outputs\nansible.builtin.copy:\ncontent: \"{{item}}\"\ndest: '{{playbook_dir}}/{{inventory_hostname}}.{{ commands[idx] | replace(\" \", \"-\") | regex_replace(\"[^A-Za-z0-9\\-]\", \"\") }}.txt'\nloop: \"{{cli_result.json.result}}\"\nloop_control:\nindex_var: idx\n</code></pre> <p>Run the playbook with <code>./ansible.sh task06/fetch-show-cmd-output.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task06/fetch-show-cmd-output.yml\n\nPLAY [Operational commands] *****************************************************************************************************************\n\nTASK [Fetch show commands output] ***********************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [Stop if request contains error] *******************************************************************************************************\nskipping: [clab-2srl-srl1]\nskipping: [clab-2srl-srl2]\n\nTASK [Save fetched show outputs] ************************************************************************************************************\nchanged: [clab-2srl-srl1] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl1\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:C0:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.394Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nchanged: [clab-2srl-srl2] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl2\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:1E:01:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.390Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nok: [clab-2srl-srl2] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType             : 7220 IXR-D2\nLast Boot type   : normal\nHW MAC address   : 1A:1E:01:FF:00:00\nSlots            : 1\nOper Status      : up\nLast booted      : 2022-12-08T13:55:46.390Z\nLast change      : 2022-12-08T13:55:46.390Z\nPart number      : Sim Part No.\nCLEI code        : Sim CLEI\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nok: [clab-2srl-srl1] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType             : 7220 IXR-D2\nLast Boot type   : normal\nHW MAC address   : 1A:C0:00:FF:00:00\nSlots            : 1\nOper Status      : up\nLast booted      : 2022-12-08T13:55:46.394Z\nLast change      : 2022-12-08T13:55:46.394Z\nPart number      : Sim Part No.\nCLEI code        : Sim CLEI\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=2    changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=2    changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre> <p>As a result of that run, you should get a file per the executed command per the node in your inventory:</p> <pre><code>\u276f ls task06\nclab-2srl-srl1.show-platform-chassis.txt  clab-2srl-srl2.show-platform-chassis.txt  fetch-show-cmd-output.yml\nclab-2srl-srl1.show-version.txt           clab-2srl-srl2.show-version.txt\n\n\u276f cat task06/clab-2srl-srl1.show-version.txt \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl1\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:C0:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.394Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#https","title":"HTTPS","text":"<p>URI module allows users to use secured transport and optionally skip certificate verification. Check the basics tutorial section on https details and how containerlab certificates can help you test the secured connection.</p>"},{"location":"tutorials/programmability/json-rpc/ansible-with-uri/#summary","title":"Summary","text":"<p>While Ansible may not be the best tool for the network automation job due to complicated troubleshooting, weird looping mechanisms, challenging ways to manipulate and extract modelled data - it is still being used by many teams.</p> <p>Our mission was to demonstrate how Ansible can be used in conjunction with SR Linux Network OS and which interface to choose - gNMI or JSON-RPC? Then, through a set of task-oriented exercises, we showed almost all methods of JSON-RPC interface. Our selection criteria was to provide the examples that we see typically in the field and at the same time not overcomplicate them so that everyone can follow along.</p> <p>Do you want us to cover more tasks using Ansible, or any other automation stack? Do let us know in the comments!</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>Or home-grown automation tools leveraging some general purpose programming language.\u00a0\u21a9</p> </li> <li> <p>The library does not allow to use unsecured gRPC transport nor it allows to skip certificate validation process.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/json-rpc/ansible/","title":"Using <code>nokia.srlinux</code> Ansible collection","text":"Summary Tutorial name <code>nokia.srlinux</code> collection with Ansible Lab components 2 Nokia SR Linux nodes Resource requirements  2 vCPU  4 GB Lab jsonrpc-ansible Main ref documents JSON-RPC Configuration, JSON-RPC Management<code>nokia.srlinux</code> collection Version information<sup>1</sup> <code>srlinux:23.3.1</code>, <code>containerlab:0.40.0</code>, <code>ansible-core:2.13</code> Authors Roman Dodin  Discussions  Twitter \u00b7  LinkedIn <p>Note</p> <p>This is an updated tutorial that uses the new <code>nokia.srlinux</code> Ansible collection. The previous version of this tutorial that uses Ansible URI module is deprecated but still can be found here.</p> <p>In the JSON-RPC Basics tutorial, we focused on the JSON-RPC interface mechanics and its capabilities. The examples we used there used a well-known <code>curl</code> command-line utility to put the focal point on the JSON-RPC itself and some automation framework.</p> <p>Arguably, using <code>curl</code> for network automation tasks that aren't trivial may be challenging and likely lead to hairy bash scripting. Instead, network ops teams prefer to use home-grown automation that leverages programming languages or configuration management tools like Ansible<sup>2</sup> fitted to the networking purpose.</p> <p>Ansible for network automation?</p> <p>We should mention that using Ansible for network automation might feel like a shortcut to automation nirvana with both infra and network domains automated via a single cfg management tool, but this might be a trap.</p> <p>From our experience using Ansible for network automation may work great when your automation tasks are trivial and do not require advanced configuration or state management. Programming in Ansible is tricky at best, and we advise you to consider using general-purpose languages instead (Python, Go, etc).</p> <p>Still, network teams who have experience with Ansible may work around its limitations and make the tool do the job without falling into a trap of troubleshooting playbooks, variable shadowing and jinja-programming.</p> <p>The topic of this tutorial is exactly this - using Ansible and SR Linux's JSON-RPC interface to automate common network operations. This task-oriented tutorial will help you understand how Ansible can be used to perform day0+ operations on our magnificent Nokia SR Linux Network OS.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#gnmi-or-json-rpc","title":"gNMI or JSON-RPC?","text":"<p>Ansible has been marketing itself as a framework suitable for network automation. We've seen lots of network platforms integrated with Ansible using custom galaxy collections. There is no point in arguing if Ansible is the right tool of choice when network automation branches out as a separate netops discipline. If Ansible does the job for some netops teams, our task it to help them understand how it can be used with SR Linux.</p> <p>At the time of this writing, SR Linux provides three management interfaces:</p> <ul> <li>gNMI</li> <li>JSON-RPC</li> <li>CLI</li> </ul> <p>We've discussed before how these interfaces have the same visibility, but which one to pick for Ansible?</p> <p>A few years back, Nokia open-sourced the <code>nokia.grpc</code> galaxy collection to add gNMI support to Ansible. Unfortunately, due to the complications in the upstream python-grpcio library<sup>3</sup>, this plugin was not widely used in the context of Ansible. In addition to that limitation, Ansible host has to be provided with gRPC libraries as dependencies, which might be problematic for some users. Having said that, it is still possible to use this collection.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#sr-linux-collection","title":"SR Linux collection","text":"<p>In contrast with gNMI, which requires a custom collection to operate, using HTTP API with Ansible is easy, and lots of Ansible's networking collections use that interface, and so do we.</p> <p>We are pleased to have our <code>nokia.srlinux</code> collection published that empowers Ansible users to automate Nokia SR Linux-based fabrics.</p> <p>The collection is available on Ansible Galaxy and can be installed using the <code>ansible-galaxy</code> command. As part of the collection, we provide a set of Ansible modules that allow you to perform common network operations on SR Linux devices:</p> <ul> <li><code>nokia.srlinux.config</code> - to configure the device in fully model-driven way.</li> <li><code>nokia.srlinux.get</code> - to retrieve the device configuration and operational state.</li> <li><code>nokia.srlinux.validate</code> - to validate changes before applying them to the device.</li> <li><code>nokia.srlinux.cli</code> - to execute arbitrary CLI commands on the device.</li> </ul>"},{"location":"tutorials/programmability/json-rpc/ansible/#lab-deployment","title":"Lab deployment","text":"<p>If this is not your first tutorial on this site, you rightfully expect to get a containerlab-based lab provided so that you can follow along with the provided examples. The lab defines two Nokia SR Linux nodes connected over <code>ethernet-1/1</code> interfaces.</p> <p>To deploy the lab, clone the repository and do <code>containerlab deploy</code> from within the repository's directory. Shortly after you should have two SR Linux containers running:</p> Result of containerlab deploy command<pre><code>+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n| # |      Name      | Container ID |             Image             | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n| 1 | clab-2srl-srl1 | cc5ba5f8cc04 | ghcr.io/nokia/srlinux:23.3.1  | srl  | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 2 | clab-2srl-srl2 | aa5f8626ac4b | ghcr.io/nokia/srlinux:23.3.1  | srl  | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+----------------+--------------+-------------------------------+------+---------+----------------+----------------------+\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#ansible-setup","title":"Ansible setup","text":""},{"location":"tutorials/programmability/json-rpc/ansible/#inventory","title":"Inventory","text":"<p>The two nodes deployed by containerlab fit nicely in a simple Ansible's inventory. We use container's names as containerlab reported back to us and construct the inventory in a  YAML format:</p> inventory.yml file<pre><code>all:\nvars:\nansible_connection: ansible.netcommon.httpapi\nansible_user: admin\nansible_password: NokiaSrl1!\nansible_network_os: nokia.srlinux.srlinux\nhosts:\nclab-2srl-srl1:\ne1_1_ip: 192.168.0.1/24\nclab-2srl-srl2:\ne1_1_ip: 192.168.0.2/24\n</code></pre> <p>We also specify the variables that are common for all hosts and are essential for the <code>nokia.srlinux</code> collection operation. These values are explained in the collection's documentation.</p> <p>We put a variable <code>e1_1_ip</code> for each host, as later we would like to use the values of these variables in the configuration tasks.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#container","title":"Container","text":"<p>Ansible is infamous for breaking things when you least expect it. For that reason, we put caged the beast. We built a container image with Ansible-core v2.13.8 and are going to use it throughout this tutorial via a runner script <code>ansible.sh</code> that calls a <code>docker run</code> command with a few args:</p> ansible-in-a-container runner script<pre><code>docker run --rm -it \\\n-v $(pwd):/ansible \\ #(1)!\n-v ~/.ssh:/root/.ssh \\ #(2)!\n-v /etc/hosts:/etc/hosts \\ #(3)!\nghcr.io/hellt/ansible:2.13.8 ansible-playbook -i inventory.yml $@\n</code></pre> <ol> <li><code>/ansible</code> is a working dir for our container image, so we mount the repo's directory to this path.</li> <li>although not needed for this lab, we still mount ssh dir of the host to the container, in case we need key-based ssh access</li> <li>to make sure that Ansible container can reach the nodes deployed by containerlab we mount the <code>/etc/hosts</code> file to it. That way ansible inside the container can resolve node names to IP addresses.</li> </ol> <p>Note</p> <p>With Ansible running in a container connected to the default bridge network and the rest of the nodes running in the <code>clab</code> docker network users may experience communication problems between Ansible and network elements. This stems from the default iptables rules Docker maintains preventing container communications between different networks. To overcome this, consider one of the following methods (one of them, not all):</p> <ol> <li>Install iptables rule to allow packets to <code>docker0</code> network:    <pre><code>sudo iptables -I DOCKER-USER -o docker0 -j ACCEPT -m comment --comment \"allow inter-network comms\"\n</code></pre></li> <li>Instruct containerlab to start nodes in the Docker default network.</li> <li>Run Ansible container in the network that containerlab uses (<code>clab</code> by default)</li> </ol> <p>Using this container image is not required for this tutorial, you still can install Ansible using any of the supported methods.</p> <p>With the container image, we tried to make sure you will have one problem less to worry about.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#tasks","title":"Tasks","text":"<p>Once the lab repository is cloned and the lab is deployed, we are ready to start solving the tasks using <code>nokia.srlinux</code> Ansible collection.  </p>"},{"location":"tutorials/programmability/json-rpc/ansible/#retrieving-state-and-config","title":"Retrieving state and config","text":"<p>Beginning with the common task of retrieving state and configuration data from SR Linux using model-based paths.</p> <p>Playbook <code>task01/get-state-and-config-data.yml</code>:</p> <pre><code>- name: Get state and config data from SR Linux\nhosts: all\ngather_facts: false\ntasks:\n- name: Get hostname, version and tls-profile\nnokia.srlinux.get:\npaths:\n- path: /system/name/host-name\n- path: /system/information/version\n- path: /system/json-rpc-server/network-instance[name=mgmt]/https/tls-profile\nregister: get_result\n- ansible.builtin.debug:\nmsg: \"Host {{get_result.result[0]}} runs {{get_result.result[1]}} version and json-rpc server uses '{{get_result.result[2]}}' TLS profile\"\n</code></pre> <p>In the <code>Get hostname, version and tls-profile</code> task we leverage the <code>nokia.srlinux.get</code> to fetch three parameters off of the SR Linux node.</p> <p>Each path in the <code>paths</code> list uses an XPATH-like path notation that points to a YANG-modelled node. In this particular example, we retrieve the data from the <code>state</code> datastore, as this is the default value for the datastore parameter of the get module. Read more on datastores here.</p> <p>To execute the playbook:</p> <pre><code>./ansible.sh task01/get-state-and-config-data.yml\n</code></pre> <p>The result of the playbook will contain the message string per each lab node with the relevant information:</p> <pre><code>TASK [ansible.builtin.debug] ***********************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": \"Host srl1 runs v22.11.1-184-g6eeaa254f7 version and json-rpc server uses 'clab-profile' TLS profile\"\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": \"Host srl2 runs v22.11.1-184-g6eeaa254f7 version and json-rpc server uses 'clab-profile' TLS profile\"\n}\n</code></pre> <p>The <code>debug</code> task shows how to access the data returned by the module. Read more on the returned data in the get module documentation.</p> <p>Of course, your paths might not necessarily point to a leaf; it can be a container, a list, or another YANG element. We used leaves in the example to demonstrate how to access the data in the response.</p> <p>Based on the provided example, users can fetch any configuration or state data available in SR Linux.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#configuration-backup","title":"Configuration backup","text":"<p>Another bestseller task in the network operations category - configuration backup. In SR Linux, the running configuration is what populates the <code>running</code> datastore. We can easily fetch the entire <code>running</code> datastore by using the <code>/</code> path with the <code>get</code> module.</p> <p>Playbook <code>task02/cfg-backup.yml</code> demonstrates how this is done.</p> snip from the task02/cfg-backup.yml<pre><code>- name: Configuration backup\nhosts: all\ngather_facts: false\ntasks:\n- name: Backup running configuration\nnokia.srlinux.get:\npaths:\n- path: /\ndatastore: running\nregister: get_result\n</code></pre> <p>Once the whole running datastore is fetched we write (using the copy module) it to the ansible host filesystem to the same directory where the playbook is located:</p> <pre><code>- name: Save fetched configs\nansible.builtin.copy:\ncontent: \"{{get_result.result[0] | to_nice_json}}\"\ndest: \"{{playbook_dir}}/{{inventory_hostname}}.cfg.json\"\n</code></pre> <p>As a result, running configs of the two nodes are written to the <code>task02</code> directory:</p> <pre><code>PLAY [Configuration backup] **************************************************************************************\n\nTASK [Backup running configuration] ******************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Save fetched configs] **************************************************************************************\nchanged: [clab-2srl-srl1]\nchanged: [clab-2srl-srl2]\n\nPLAY RECAP *******************************************************************************************************\nclab-2srl-srl1             : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\nclab-2srl-srl2             : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <pre><code>\u276f ls task02\ncfg-backup.yml  clab-2srl-srl1.cfg.json  clab-2srl-srl2.cfg.json\n</code></pre> <p>Note</p> <p>Alternative approach to do configuration backup is to transfer the config file that resides on the file system of the Network OS.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#setting-configuration","title":"Setting configuration","text":"<p>Configuration tasks are not the white crows either. Many Ansible operators provision their network devices using popular techniques such as configuration templating.</p> <p>In <code>task03/config.yml</code> playbook, we switch to the <code>config</code> module and demonstrate several ways to source the configuration data.</p> <p>The tasks' body highlights module's ability to stuff many configuration operations in a single request. In this case a replace operation is accompanied by two update operations.</p> <pre><code>- name: Configuration\nhosts: all\ngather_facts: false\ntasks:\n- name: Various configuration tasks\nnokia.srlinux.config:\nreplace:\n- path: /interface[name=mgmt0]/description\nvalue: \"{{inventory_hostname}} management interface\"\nupdate:\n- path: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\n- path: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/iface-cfg.json.j2') }}\"\nregister: set_result\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#value-container","title":"Value container","text":"<p>A most common way to provide configuration values is by writing them in the <code>value</code> parameter of an operation. Like in the case of the <code>update</code>'s first operation, we provide values for the <code>/system/information</code> container:</p> <pre><code>- action: update\npath: /system/information\nvalue:\nlocation: the Netherlands\ncontact: Roman Dodin\n</code></pre> <p>We set two leaves - <code>location</code> and <code>contact</code> - by creating the <code>value</code> container, which embeds leaves and values we want to set under the <code>/system/information</code> path.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#config-sourced-from-a-file","title":"Config sourced from a file","text":"<p>Quite often, configuration templates intended to be pushed to the node are saved on disk. Hence, we want to show how to use those templates in your configuration tasks.</p> <p>The <code>iface-cfg.json.j2</code> file contains a template for the two interfaces - <code>ethernet-1/1</code> and <code>lo0</code>. The <code>ethernet-1/1</code> interface is configured with a sub-interface and IP address sourced from the variable we put in the inventory and is attached to the default network instance.</p> <pre><code>{\n\"interface\": [\n{\n\"name\": \"ethernet-1/1\",\n\"description\": \"ethernet-1/1 interface on {{inventory_hostname}} node\",\n\"admin-state\": \"enable\",\n\"subinterface\": [\n{\n\"index\": 0,\n\"admin-state\": \"enable\",\n\"ipv4\": {\n\"address\": [\n{\n\"ip-prefix\": \"{{e1_1_ip}}\"\n}\n]\n}\n}\n]\n},\n{\n\"name\": \"lo0\",\n\"description\": \"loopback interface on {{inventory_hostname}} node\",\n\"admin-state\": \"enable\"\n}\n],\n\"network-instance\": [\n{\n\"name\": \"default\",\n\"interface\": [\n{\n\"name\": \"ethernet-1/1.0\"\n}\n]\n}\n]\n}\n</code></pre> <p>Since <code>/interface</code> list is a top-level element in our YANG model (as denoted by <code>/interface</code> path), to create a new member of this list, we craft a JSON object with <code>interface</code> list and specify its members (<code>ethernet-1/1</code> and <code>lo0</code>). This JSON object is then updates/merges the <code>/</code> path, thus making two new interfaces. Same with the <code>network-instance</code>.</p> <pre><code>- action: update\npath: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/iface-cfg.json.j2') }}\"\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#results-validation","title":"Results validation","text":"<p>Run the playbook with <code>./ansible.sh task03/config.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task03/config.yml\n\nPLAY [Configuration] ************************************************************************************************************************\n\nTASK [Various configuration tasks] **********************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": [\n        \"mgmt0 description is: clab-2srl-srl1 management interface\",\n        \"location is: the Netherlands\",\n        \"contact is: Roman Dodin\",\n        \"ethernet-1/1 description is: ethernet-1/1 interface on clab-2srl-srl1 node\",\n        \"loopback0 description is: loopback interface on clab-2srl-srl1 node\"\n    ]\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": [\n        \"mgmt0 description is: clab-2srl-srl2 management interface\",\n        \"location is: the Netherlands\",\n        \"contact is: Roman Dodin\",\n        \"ethernet-1/1 description is: ethernet-1/1 interface on clab-2srl-srl2 node\",\n        \"loopback0 description is: loopback interface on clab-2srl-srl2 node\"\n    ]\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre> <p>To ensure that the changes were correctly applied, the last task fetches state information for the touched leaves. Looking at the output, we can verify that the config was applied correctly.</p> <p>Additionally, since we configured IP addresses over the connected interfaces on both nodes, you should be able to execute a ping between the nodes:</p> <pre><code>--{ + running }--[  ]--\nA:srl1# ping 192.168.0.2 network-instance default  Using network instance default\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=90.3 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=12.6 ms\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#replacing-partial-config","title":"Replacing partial config","text":"<p>To replace a portion of a config a <code>replace</code> operation of the <code>config</code> module is used with the path pointing to the subtree to be replaced. The value of the <code>replace</code> operation is the new subtree to be replaced with.</p> <p>In <code>task04/replace-partial-cfg.yml</code> playbook we replace everything that was configured in the previous configuration task for <code>ethernet-1/1</code> interface with a container having a single leaf set - <code>admin-state: disable</code>.</p> <pre><code>- name: Replace operation\nhosts: all\ngather_facts: false\ntasks:\n- name: Replace partial config\nnokia.srlinux.config:\nreplace:\n- path: /interface[name=ethernet-1/1]\nvalue:\nname: ethernet-1/1\nadmin-state: disable\ndelete:\n- path: /network-instance[name=default]/interface[name=ethernet-1/1.0]\n</code></pre> <p>The replace action will delete everything under <code>/interface[name=ethernet-1/1]</code> and update it with the value specified in the request. We also remove the binding of the subinterface <code>ethernet-1/1.0</code> from the network instance, since the subinterface is going to be removed by the replace operation and we can't have it referenced in the network instance anymore.</p> <p>Run the playbook with <code>./ansible.sh task04/replace-partial-cfg.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task04/replace-partial-cfg.yml\n\nPLAY [Replace operation] ********************************************************************************************************************\n\nTASK [Replace partial config] ***************************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"get_result.json.result\": [\n        {\n            \"admin-state\": \"disable\"\n        }\n    ]\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"get_result.json.result\": [\n        {\n            \"admin-state\": \"disable\"\n        }\n    ]\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#replacing-entire-config","title":"Replacing entire config","text":"<p>One of the common tactics to provision a new box with services after it has been ZTP'd is to replace whatever is there with the golden or node-specific config.</p> <p>In <code>task05/replace-entire-cfg.yml</code> we do just that. The golden config that resides in the task directory has jinja variables that Ansible populates at runtime to add node-specific values. The golden config we used in this example is similar to what we retrieved in the backup task.</p> <pre><code>- name: Replace operation\nhosts: all\ngather_facts: false\ntasks:\n- name: Replace entire config\nnokia.srlinux.config:\nreplace:\n- path: /\nvalue: \"{{lookup('ansible.builtin.template', '{{playbook_dir}}/golden.cfg.json.j2') }}\"\n</code></pre> <p>To replace the entire config we set the path to <code>/</code> value and provide the entire config in the value field.</p> <p>Run the playbook with <code>./ansible.sh task05/replace-entire-cfg.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task05/replace-entire-cfg.yml\n\nPLAY [Replace operation] ********************************************************************************************************************\n\nTASK [Replace partial config] ***************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [Verify configuration set] *************************************************************************************************************\nok: [clab-2srl-srl1]\nok: [clab-2srl-srl2]\n\nTASK [ansible.builtin.debug] ****************************************************************************************************************\nok: [clab-2srl-srl1] =&gt; {\n    \"msg\": \"Location from golden config: CONTAINERLAB\"\n}\nok: [clab-2srl-srl2] =&gt; {\n    \"msg\": \"Location from golden config: CONTAINERLAB\"\n}\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=3    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#collecting-show-commands","title":"Collecting <code>show</code> commands","text":"<p>Another golden record of the netops is to dump some operational <code>show</code> commands for audit, pre/post checks, etc. Using <code>nokia.srlinux.cli</code> module we can execute <code>show</code> and other commands with output format being <code>json</code>, <code>text</code> or <code>table</code>.</p> <p>In <code>task06/fetch-show-cmd-output.yml</code> we collect the output of a few <code>show</code> commands in text outputs and save them in per-node files.</p> <pre><code>- name: Operational commands\nhosts: all\ngather_facts: false\nvars:\ncommands:\n- show version\n- show platform chassis\ntasks:\n- name: Fetch show commands output\nnokia.srlinux.cli:\ncommands: \"{{commands}}\"\noutput_format: text\nregister: cli_result\n</code></pre> <p>For this playbook we introduce playbook variable - <code>commands</code> - that host a list of show commands we would like to execute remotely.</p> <p>To save the results of the executed commands we loop over the results array and save each result in its own file with a sanitized name:</p> <pre><code>- name: Save fetched show outputs\nansible.builtin.copy:\ncontent: \"{{item}}\"\ndest: '{{playbook_dir}}/{{inventory_hostname}}.{{ commands[idx] | replace(\" \", \"-\") | regex_replace(\"[^A-Za-z0-9\\-]\", \"\") }}.txt'\nloop: \"{{cli_result.result}}\"\nloop_control:\nindex_var: idx\n</code></pre> <p>Run the playbook with <code>./ansible.sh task06/fetch-show-cmd-output.yml</code>:</p> Run output <pre><code>\u276f ./ansible.sh task06/fetch-show-cmd-output.yml\n\nPLAY [Operational commands] *****************************************************************************************************************\n\nTASK [Fetch show commands output] ***********************************************************************************************************\nok: [clab-2srl-srl2]\nok: [clab-2srl-srl1]\n\nTASK [Save fetched show outputs] ************************************************************************************************************\nchanged: [clab-2srl-srl1] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl1\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:C0:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.394Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nchanged: [clab-2srl-srl2] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl2\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:1E:01:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.390Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nok: [clab-2srl-srl2] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType             : 7220 IXR-D2\nLast Boot type   : normal\nHW MAC address   : 1A:1E:01:FF:00:00\nSlots            : 1\nOper Status      : up\nLast booted      : 2022-12-08T13:55:46.390Z\nLast change      : 2022-12-08T13:55:46.390Z\nPart number      : Sim Part No.\nCLEI code        : Sim CLEI\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\nok: [clab-2srl-srl1] =&gt; (item=--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType             : 7220 IXR-D2\nLast Boot type   : normal\nHW MAC address   : 1A:C0:00:FF:00:00\nSlots            : 1\nOper Status      : up\nLast booted      : 2022-12-08T13:55:46.394Z\nLast change      : 2022-12-08T13:55:46.394Z\nPart number      : Sim Part No.\nCLEI code        : Sim CLEI\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n)\n\nPLAY RECAP **********************************************************************************************************************************\nclab-2srl-srl1             : ok=2    changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \nclab-2srl-srl2             : ok=2    changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n</code></pre> <p>As a result of that run, you should get a file per the executed command per the node in your inventory:</p> <pre><code>\u276f ls task06\nclab-2srl-srl1.show-platform-chassis.txt  clab-2srl-srl2.show-platform-chassis.txt  fetch-show-cmd-output.yml\nclab-2srl-srl1.show-version.txt           clab-2srl-srl2.show-version.txt\n\n\u276f cat task06/clab-2srl-srl1.show-version.txt \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nHostname             : srl1\nChassis Type         : 7220 IXR-D2\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:C0:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-08T13:55:46.394Z\nTotal Memory         : 24052875 kB\nFree Memory          : 14955894 kB\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/ansible/#https","title":"HTTPS","text":"<p>All the examples in this tutorial were using HTTP as a transport protocol. However, in production environments, you would want to use HTTPS to secure your communication with the network devices.</p> <p>Using HTTPS with this collection is as simple as adding the <code>ansible_httpapi_use_ssl: true</code> variable. For instance, to the list of variables in the inventory file:</p> <pre><code>all:\nvars:\nansible_connection: ansible.netcommon.httpapi\nansible_user: admin\nansible_password: NokiaSrl1!\nansible_network_os: nokia.srlinux.srlinux\nansible_httpapi_use_ssl: true\nhosts:\n# snip\n</code></pre> <p>To see other options of TLS-secured connections, see the TLS section of the module's docs.</p>"},{"location":"tutorials/programmability/json-rpc/ansible/#summary","title":"Summary","text":"<p>While Ansible may not be the best tool for the network automation job due to complicated troubleshooting, weird looping mechanisms, and challenging ways to manipulate and extract modelled data - it is still being used by many teams.</p> <p>Our mission was to demonstrate how Ansible can be used in conjunction with SR Linux Network OS and which interface to choose - gNMI or JSON-RPC? Then, through a set of task-oriented exercises, we showed how modules in <code>nokia.srlinux</code> collection can help you reach your automation goals with Ansible.</p> <p>Do you want us to cover more tasks using Ansible, or any other automation stack? Do let us know in the comments!</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>Or home-grown automation tools leveraging some general purpose programming language.\u00a0\u21a9</p> </li> <li> <p>The library does not allow to use unsecured gRPC transport nor it allows to skip certificate validation process.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/programmability/json-rpc/basics/","title":"JSON-RPC Basics","text":""},{"location":"tutorials/programmability/json-rpc/basics/#json-rpc-basics","title":"JSON-RPC Basics","text":"Summary Tutorial name JSON-RPC Basics Lab components Single Nokia SR Linux node Resource requirements  2 vCPU  4 GB Lab single-srlinux Main ref documents JSON-RPC Configuration, JSON-RPC Management Version information<sup>1</sup> <code>srlinux:22.11.1</code>, <code>containerlab:0.33.0</code> Authors Roman Dodin  Discussions  Twitter \u00b7  LinkedIn <p>As of release 22.11.1, Nokia SR Linux Network OS employs three fully modeled management interfaces:</p> <ul> <li>gNMI</li> <li>JSON-RPC</li> <li>CLI</li> </ul> <p>Not only these interfaces are modeled, but they all use the same set of models and therefore enable one of the key differentiators of SR Linux - every management interface has access to the state and configuration datastores and provides the same visibility and configuration capabilities<sup>2</sup>.</p> <p> </p> Every management interface is a client of the same core API <p>Every management interface, in essence, uses the same API provided by the management server of SR Linux which makes interfaces equal in access rights and visibility.</p> JSON-RPC as just another client of the same management API <p>In this tutorial, we are going to meet SR Linux's JSON-RPC interface and learn how to achieve basic management tasks using the <code>curl</code> utility. In the subsequent tutorials, our focus will shift from the JSON-RPC towards the different tooling that leverages it; think of Ansible, Postman tools and integrations with programming languages like Go and Python.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#why-json-rpc","title":"Why JSON-RPC?","text":"<p>But first, why even bother using JSON-RPC if SR Linux sports a more performant-on-the-wire and modern gNMI interface? While it is true, that gNMI can be considered more performant on the wire by leveraging HTTP2 multiplexing and protobuf encoding, some well established automation stacks may not be able to offer gRPC/gNMI support just yet. To make SR Linux accessible to non-hyperscalers and network teams who have been using HTTP/JSON-based management tools we offered a JSON-RPC management interface that can be easily integrated with a wide variety of higher-level Network Management Systems (NMS) and automation stacks.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#json-rpc-methods","title":"JSON-RPC methods","text":"<p>Being a custom management interface, JSON-RPC offers both standard methods like <code>get</code> and <code>set</code> to work with the state and configuration datastores of srlinux, as well as custom functions like <code>validate</code> for validating the config and <code>cli</code> to invoke CLI commands on the system. All of that uses JSON-encoded messages exchanged over HTTP transport.</p> <p>As the RPC part of the name suggests, users are able to execute certain remote procedures on SR Linux via JSON-RPC interface. We refer to these procedures as methods; the following table summarizes the JSON-RPC provided methods as stated in the JSON-RPC Management Guide.</p> Method Description Get Used to retrieve configuration and state details from the system. The get method can be used with candidate, running, and state datastores, but cannot be used with the tools datastore. Set Used to set a configuration or run operational transaction. The set method can be used with the candidate and tools datastores. Validate Used to verify that the system accepts a configuration transaction before applying it to the system. CLI Used to run CLI commands. The get and set methods are restricted to accessing data structures via the YANG models, but the cli method can access any commands added to the system via python plugins or aliases. <p>We will introduce all of these methods in detail during the practical section of this tutorial.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#configuring-json-rpc","title":"Configuring JSON-RPC","text":"<p>SR Linux factory configuration doesn't have JSON-RPC management enabled, but it is easy to configure one. JSON-RPC Configuration Guide does a good job explaining all the bits and pieces of interface configuration, so to not repeat ourselves let's look at what containerlab configures automatically for every SR Linux node based on a single-node lab that we use in this tutorial.</p> <pre><code>--{ running }--[  ]--\nA:srl# info /system json-rpc-server\n    system {\n        json-rpc-server {\n            admin-state enable\n            network-instance mgmt {\n                http {\n                    admin-state enable\n                }\n                https {\n                    admin-state enable\n                    tls-profile clab-profile\n                }\n            }\n        }\n    }\n</code></pre> <p>By default, containerlab enables JSON-RPC management interface in the management network instance<sup>4</sup> by configuring <code>json-rpc-server</code> instance running both in secure/https and plain-text/http modes on ports 80 and 443 accordingly. For https endpoint, containerlab uses the <code>tls-profile clab-profile</code> that it generates on lab startup.</p> <p>Note</p> <p>JSON-RPC management interface runs on the <code>/jsonrpc</code> HTTP(S) endpoint of the SR Linux, which means that to access this interface, users should use the following URI:</p> <pre><code>http(s)://&lt;srlinux-address&gt;/jsonrpc #(1)!\n</code></pre> <ol> <li>where <code>srlinux-address</code> is the address of the management interface of SR Linux. The lab used in this tutorial has a deterministic name for the srlinux node - <code>clab-srl01-srl</code> - which we will use as the address.</li> </ol> <p>With this configuration in place, users can leverage JSON-RPC immediately after containerlab finishes deploying the lab.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#requestresponse-structure","title":"Request/response structure","text":"<p>The management interface sends requests<sup>3</sup> to the JSON-RPC server and receives responses. The request/response format is a JSON encoded string and is detailed in the docs. Let's look at the skeleton of the request/response messages as it will help us getting through practical exercises:</p> RequestResponse JSON-RPC request structure<pre><code>{\n\"jsonrpc\": \"2.0\",\n\"id\": 0,\n\"method\": \"get\",\n\"params\": {\n\"commands\": [],\n\"output-format\": \"\" //(1)!\n}\n}\n</code></pre> <ol> <li>Only applicable for CLI method.</li> </ol> <p>where:</p> <ul> <li><code>jsonrpc</code> - selects the version of the management interface and at the moment of this writing should be always set to <code>2.0</code>.</li> <li><code>id</code> - sets the ID of a request which is echoed back in the response to help correlate the message flows<sup>8</sup>.</li> <li><code>method</code> - sets one of the supported RPC methods used for this request.</li> <li><code>params</code> - container for RPC commands. We will cover the contents of this container through the practical exercises.</li> </ul> <pre><code>{\n\"result\": [],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>The response object structure provides a <code>result</code> list that contains the result of the invoked RPC. Additionally, the response object contains the RPC version and request ID.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#authentication","title":"Authentication","text":"<p>JSON-RPC server uses basic authentication for both HTTP and HTTPS transports, which means user information must be provided in a request.</p> User credentials are passed in a request<pre><code>curl -s -X POST 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc'\n</code></pre> <p>In the example above, the user <code>admin</code> with a password <code>NokiaSrl1!</code> is used to authenticate with the JSON-RPC API.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#methods","title":"Methods","text":"<p>Enough with the boring theory, let's have some handson fun firing off requests, and learn how JSON-RPC works in real life. To keep things focused on the JSON-RPC itself, we will be using <code>curl</code> utility as our HTTP client with <code>jq</code> helping format the responses.</p> <p>Tip</p> <p>Keep the JSON-RPC Management Guide tab open, as additional theory is provided there which we won't duplicate in this tutorial.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#get","title":"Get","text":"<p>Starting with the basics, let's see how we can query SR Linux configuration and state datastores using <code>get</code> method of JSON-RPC. How about we start with a simple management task of getting to know the software version we're running in our lab container?</p> RequestResponse <p><code>curl</code> by default uses HTTP POST method, thus we don't explicitly specify it. With <code>-d @- &lt;&lt;EOF</code> argument we pass the heredoc-styled body of the request in a JSON format of this POST request.</p> <p>Our <code>commands</code> list contains a single object with <code>path</code> and <code>datastore</code> values set.</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p><code>jq</code> used in the request command displays the json response in a formatted way.</p> <pre><code>{\n\"result\": [\n\"v22.11.1-184-g6eeaa254f7\"\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>There is something to unpack in the message used in this exchange. First, note that in the <code>params</code> container we specified the <code>commands</code> list. Each element in this list is an object that contains a <code>path</code> and <code>datastore</code> values.  </p>"},{"location":"tutorials/programmability/json-rpc/basics/#path","title":"Path","text":"<p>The <code>path</code> is a string that follows gNMI Path Conventions<sup>5</sup> and used to point to an element that is . It is not hard to spot that the <code>path</code> follows the SR Linux YANG model and allows us to select a certain leaf that contains the version information.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#datastore","title":"Datastore","text":"<p>The <code>datastore</code> value sets the SR Linux datastore we would like to use with our RPC. SR Linux offers four datastores that JSON-RPC users can choose from:</p> Datastore Description Candidate Used to change the configuration of the system with the get, set, and validate methods; default datastore is used if the datastore parameter is not provided. Running Used to retrieve the active configuration with the get method. State Used to retrieve the running (active) configuration along with the operational state. tools Used to perform operational tasks on the system; only supported with the update action command and the set method. <p>By specifying <code>path=/system/information/version</code> and <code>datastore=state</code> in our request, we instruct SR Linux to return the value of the targeted leaf using the <code>state</code> datastore. An equivalent CLI command on SR Linux to retrieve the same would be:</p> <pre><code>--{ running }--[  ]--\nA:srl# info from state system information version  \n    system {\n        information {\n            version v22.11.1-184-g6eeaa254f7\n        }\n    }\n--{ running }--[  ]--\n</code></pre> <p>Note</p> <p>Datastore value can be set either per-command as in the example above, or on the <code>params</code> level. Command-scope datastore value takes precedence over the <code>params</code>-scope value.</p> <p>When datastore value is omitted, <code>running</code> datastore is assumed. For example, repeating the same request without specifying the datastore will error, as <code>running</code> datastore doesn't hold state leaves and thus can't return the <code>version</code> leaf under the <code>/system/information</code> container.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/system/information/version\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>An error is returned since running datastore holds configuration, not state, and <code>version</code> leaf is a state one.</p> <pre><code>{\n\"error\": {\n\"code\": -1,\n\"message\": \"Path not valid - unknown element 'version'. Options are [contact, location]\"\n},\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>The response object contains the same ID used in the request, as well as the list of results. The number of entries in the <code>results</code> list will match the number of <code>commands</code> specified in the request.</p> How to get entire configuration? <p>It is quite easy, actually. Just send the request with the <code>/</code> path:</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>To get rid of the response fields and only get the value of the result, change the jq expression to <code>jq '.result[]'</code>.</p> <p>In the same way, to get the full state of the switch, add <code>state</code> datastore:</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq '.result[]' &gt; /tmp/test.json\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#multiple-commands","title":"Multiple commands","text":"<p>JSON-RPC allows users to batch commands of the same method in the same request. Just add elements to the <code>commands</code> list of the body message. In the following example we query the state datastore for two elements inside the same request:</p> <ol> <li>system version</li> <li>statistics data of the mgmt0 interface</li> </ol> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            },\n            {\n                \"path\": \"/interface[name=mgmt0]/statistics\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>Response message will contain a list of results with results being ordered the same way as the commands in the request.</p> <pre><code>{\n\"result\": [\n\"v22.11.1-184-g6eeaa254f7\",\n{\n\"in-octets\": \"140285\",\n\"in-unicast-packets\": \"1389\",\n\"in-broadcast-packets\": \"0\",\n\"in-multicast-packets\": \"1\",\n\"in-discarded-packets\": \"0\",\n\"in-error-packets\": \"5\",\n\"in-fcs-error-packets\": \"0\",\n\"out-octets\": \"748587\",\n\"out-mirror-octets\": \"0\",\n\"out-unicast-packets\": \"2349\",\n\"out-broadcast-packets\": \"6\",\n\"out-multicast-packets\": \"30\",\n\"out-discarded-packets\": \"0\",\n\"out-error-packets\": \"0\",\n\"out-mirror-packets\": \"0\",\n\"carrier-transitions\": \"1\"\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Note, that when the path in a request points to a leaf (like <code>/system/information/version</code>), then the result entry will be just the value of this leaf. In contrast with that, when the path is pointing to a container, then a JSON object is returned, like in the case of the result for the <code>/interface[name=mgmt0]/statistics</code> path.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#set","title":"Set","text":"<p>JSON-RPC is quite flexible when it comes to creating, updating and deleting configuration on SR Linux<sup>7</sup>. And additionally, Set method allows users to execute operational (aka <code>/tools</code>) commands.</p> <p>When changing configuration with the Set method, the JSON-RPC server creates a private candidate datastore, applies the changes and performs an implicit commit. Thus, changes are commited automatically (if they are valid) for each RPC.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#update","title":"Update","text":"<p>Switching to the 1<sup>st</sup> gear, let's just add a description to our <code>mgmt0</code> interface.</p> RequestResponseVerification <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"update\",\n                \"path\": \"/interface[name=mgmt0]/description:set-via-json-rpc\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Checking that the interface description has been set successfully.</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/interface[name=mgmt0]/description\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>Response: <pre><code>{\n\"result\": [\n\"set-via-json-rpc\"\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre></p>"},{"location":"tutorials/programmability/json-rpc/basics/#action","title":"Action","text":"<p>As you can see, the request message now contains the <code>set</code> method, and in the list of commands we have a new field - <code>action</code>. Action field is only set with <code>set</code> and <code>validate</code> methods and can take the following values:</p> <ul> <li>update - updates a leaf or container with the new value.</li> <li>delete - deletes a leaf or container.</li> <li>replace - replaces configuration with the supplied new configuration blob for a specified path. This is equivalent to a delete+update operation tandem.</li> </ul> <p>Since we wanted to set a description on the interface, the <code>update</code> action was just enough.</p> <p>The response object for a successful Set method contains a single empty JSON object regardless of how many commands were in the request.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#path-value-formats","title":"Path value formats","text":"<p>I bet you noticed the peculiar path value used in the Set request message - <code>\"path\": \"/interface[name=mgmt0]/description:set-via-json-rpc\"</code>. This path notation follows the <code>&lt;path&gt;:&lt;value&gt;</code> schema, where a scalar value of a leaf is provided in the path itself separated by a <code>:</code> char.</p> <p>Alternatively, users can specify the value using the <code>\"value\"</code> field inside the command. This allows to provide structutred values for a certain path. For example, lets set multiple fields under the <code>/system/information</code> container:</p> RequestResponseVerification <p>Set two leaves - <code>location</code> and <code>contact</code> under the <code>/system/information</code> container by using the <code>value</code> field of the command.</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"update\",\n                \"path\": \"/system/information\",\n                \"value\": {\n                  \"location\": \"the Netherlands\",\n                  \"contact\": \"Roman Dodin\"\n                }\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"datastore\": \"state\",\n        \"commands\": [\n            {\n                \"path\": \"/system/information/location\"\n            },\n            {\n                \"path\": \"/system/information/contact\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>Result:</p> <pre><code>{\n\"result\": [\n\"the Netherlands\",\n\"Roman Dodin\"\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#replace","title":"Replace","text":"<p>With the <code>replace</code> action it is possible to replace the entire configuration block for a given path with another configuration blob supplied in the request message. In essense, a replace operation is a combination of <code>delete + update</code> actions for a given path.</p> <p>To demonstrate replace operation in action, we will use the same <code>/system/information</code> container, that by now contains the <code>contact</code> and location leaves:</p> Verify current configuration of /system/information container<pre><code>\u276f docker exec clab-srl01-srl sr_cli info from running /system information\n    system {\ninformation {\ncontact \"Roman Dodin\"\nlocation \"the Netherlands\"\n}\n}\n</code></pre> <p>Let's replace this conatiner with setting just the contact leaf to \"John Doe\" value.</p> RequestResponseVerification <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"replace\",\n                \"path\": \"/system/information\",\n                \"value\": {\n                  \"contact\": \"John Doe\"\n                }\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <pre><code>\u276f docker exec clab-srl01-srl sr_cli info from running /system information\nsystem {\ninformation {\ncontact \"John Doe\"\n}\n}\n</code></pre> <p>Notice, how the verification command proves that the whole configuration under <code>/system/information</code> has been replaced with a single <code>contact</code> leaf value, there is no trace of <code>location</code> leaf.</p> Replacing the whole configuration <p>One of the common management tasks is to replace the entire config with a golden or intended configuration. To do that with JSON-RPC use <code>/</code> path and a file with JSON-formatted config:</p> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF\n{\n\"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n\"commands\": [\n{\n\"action\": \"replace\",\n                \"path\": \"/\",\n                \"value\": $(cat /path/to/config.json)\n}\n]\n}\n}\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#delete","title":"Delete","text":"<p>To delete a configuration region for a certain path use <code>delete</code> action of the Set method. For example, let's delete everything under the <code>/system/information</code> container:</p> Initial stateRequestResponseVerification <p>We start with <code>information</code> container containing <code>contact</code> leaf.</p> <pre><code>\u276f docker exec clab-srl01-srl sr_cli info from running /system information system {\ninformation {\ncontact \"John Doe\"\n}\n}\n</code></pre> <p>Delete the configuration under the <code>/system/information</code> container. <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"delete\",\n                \"path\": \"/system/information\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre></p> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Verify that the container is now empty: <pre><code>\u276f docker exec clab-srl01-srl sr_cli info from running /system information\nsystem {\ninformation {\n}\n}\n</code></pre></p> <p>Note</p> <p>Delete operation will not error when trying to delete a valid but non-existing node.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#multiple-actions","title":"Multiple actions","text":"<p>For advanced configuration management tasks JSON-RPC interface allows to batch different actions in a single RPC. Multiple commands with various actions can be part of an RPC message body; these actions are going to be applied to the same private candidate datastore that JSON-RPC interfaces uses and will be committed together as a single transaction.</p> <p>For example, let's create an RPC that will have all the actions batched together:</p> RequestResponse <p>In this composite request we replace the description for the management interface, then create a new network-instance <code>vrf-red</code> and finally deleteing a login banner. All those actions will be committed together as a single transaction.</p> <p><pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"replace\",\n                \"path\": \"/interface[name=mgmt0]/description:set-via-multi-cmd-json-rpc\"\n            },\n            {\n                \"action\": \"update\",\n                \"path\": \"/network-instance[name=vrf-red]\",\n                \"value\": {\n                    \"name\": \"vrf-red\",\n                    \"description\": \"set-via-json-rpc\"\n                }\n            },\n            {\n                \"action\": \"delete\",\n                \"path\": \"/system/banner/login-banner\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> This example also shows how to create an element of a list (like a new network instance) - specify the key in the <code>path</code> and the content of the list member in the <code>value</code>.</p> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#tools-commands","title":"Tools commands","text":"<p>Set method allows users to invoke operational commands that use a specific <code>tools</code> datastore. These commands are typically RPCs themselves, as they invoke some action on the SR Linux NOS.</p> <p>For example, <code>/tools interface mgmt0 statistics clear</code> command when invoked via CLI will clear stats for <code>mgmt0</code> interface. The same command can be called out using the Set method, as well as using the CLI method. The difference being that with Set method users should specify the modelled path using gNMI path notations, while with the CLI method users use the syntax of the CLI.</p> Initial stateRequestResponseVerification <p>Check the amount of incoming octets for mgmt0 interface. <pre><code>--{ + running }--[  ]--\nA:srl# info from state /interface mgmt0 statistics in-octets  \n    interface mgmt0 {\n        statistics {\n            in-octets 383557\n        }\n    }\n</code></pre></p> <p>Clearing statistics of <code>mgmt0</code> interface by calling the <code>/tools</code> command using the modelled path<sup>6</sup>. <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"set\",\n    \"params\": {\n        \"datastore\": \"tools\",\n        \"commands\": [\n            {\n                \"action\": \"update\",\n                \"path\": \"/interface[name=mgmt0]/statistics/clear\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre></p> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>The following output shows that the stats has been cleared via the tools command executed via JSON-RPC. <pre><code>--{ + running }--[  ]--\nA:srl# info from state /interface mgmt0 statistics in-octets\n    interface mgmt0 {\n        statistics {\n            in-octets 4379\n        }\n    }\n</code></pre></p>"},{"location":"tutorials/programmability/json-rpc/basics/#validate","title":"Validate","text":"<p>One of the infamous fallacies that people associate with gNMI is its inability to work with candidate datastores, do confirmed commits and validate configs. While JSON-RPC interface doesn't let you do incremental updates to an opened candidate datastore with the Set method, it allows you to validate a portion of a config using Validate method.</p> <p>Under the hood, SR Linux executes <code>commit validate</code> command on the provided configuration blob, and no configuration changes are made to the system. The goal of Validate method is to give users a way to ensure that the config they are about to push will be accepted.</p> <p>Validate method works with the same actions as Set method - update, replace and delete. For example, lets take our composite change request from the last exercise and validate it.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"validate\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"replace\",\n                \"path\": \"/interface[name=mgmt0]/description:set-via-multi-cmd-json-rpc\"\n            },\n            {\n                \"action\": \"update\",\n                \"path\": \"/network-instance[name=vrf-red]\",\n                \"value\": {\n                    \"name\": \"vrf-red\",\n                    \"description\": \"set-via-json-rpc\"\n                }\n            },\n            {\n                \"action\": \"delete\",\n                \"path\": \"/system/banner/login-banner\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>The empty result object indicates that changes were successfully validated and no errors were detected. What happens when the changes are not valid? Let's make some errors in our request, for example, let's try setting a description for an invalid interface:</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"validate\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"action\": \"update\",\n                \"path\": \"/interface[name=GigabitEthernet1/0]/description:set-via-json-rpc\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"error\": {\n\"code\": -1,\n\"message\": \"Failed to parse value 'GigabitEthernet1/0' for key 'name' (node 'interface') - Invalid value \\\"GigabitEthernet1/0\\\": Must match the pattern '(mgmt0|mgmt0-standby|system0|lo(0|1[0-9][0-9]|2([0-4][0-9]|5[0-5])|[1-9][0-9]|[1-9])|lif-.*|vhn-.*|enp(0|1[0-9][0-9]|2([0-4][0-9]|5[0-5])|[1-9][0-9]|[1-9])s(0|[1-9]|[1-2][0-9]|3[0-1])f[0-7]|ethernet-([1-9](\\\\d){0,1}(/[abcd])?(/[1-9](\\\\d){0,1})?/(([1-9](\\\\d){0,1})|(1[0-1]\\\\d)|(12[0-8])))|irb(0|1[0-9][0-9]|2([0-4][0-9]|5[0-5])|[1-9][0-9]|[1-9])|lag(([1-9](\\\\d){0,1})|(1[0-1]\\\\d)|(12[0-8])))'\"\n},\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>And SR Linux immediately returns an error explaining where exactly the error was.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#cli","title":"CLI","text":"<p>One of the reasons we ended up having JSON-RPC interface and not RESTCONF was the need to support CLI-formatted operations. At SR Linux, we are big believers in all things modeled, but we can't neglect the fact that transition to model-based world may take time for some teams. In the interim, these teams can effectively accomplish operational tasks using CLI-based automation.</p> <p>With JSON-RPC CLI method we allow users to remotely execute CLI commands while offering HTTP transport reliability and saving users from the burdens of screen scraping.</p> <p>Tip</p> <p>CLI method also allows to call CLI commands that are not modelled, such as aliases or plugins (e.g. <code>show version</code>). But it is not possible to execute interactive commands, e.g. <code>ping</code>, <code>bash</code>, etc.</p> <p>Staring with basics, let's see what it takes to execute a simple <code>show version</code> command using JSON-RPC?</p> RequestResponseExecuted in CLI with JSON formattingExecuted in CLI <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"show version\"\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{\n\"basic system info\": {\n\"Hostname\": \"srl\",\n\"Chassis Type\": \"7220 IXR-D3L\",\n\"Part Number\": \"Sim Part No.\",\n\"Serial Number\": \"Sim Serial No.\",\n\"System HW MAC Address\": \"1A:90:00:FF:00:00\",\n\"Software Version\": \"v22.11.1\",\n\"Build Number\": \"184-g6eeaa254f7\",\n\"Architecture\": \"x86_64\",\n\"Last Booted\": \"2022-12-06T11:38:51.482Z\",\n\"Total Memory\": \"24052875 kB\",\n\"Free Memory\": \"17004746 kB\"\n}\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:srl# show version | as json  {\n\"basic system info\": {\n\"Hostname\": \"srl\",\n\"Chassis Type\": \"7220 IXR-D3L\",\n\"Part Number\": \"Sim Part No.\",\n\"Serial Number\": \"Sim Serial No.\",\n\"System HW MAC Address\": \"1A:90:00:FF:00:00\",\n\"Software Version\": \"v22.11.1\",\n\"Build Number\": \"184-g6eeaa254f7\",\n\"Architecture\": \"x86_64\",\n\"Last Booted\": \"2022-12-06T11:38:51.482Z\",\n\"Total Memory\": \"24052875 kB\",\n\"Free Memory\": \"16858484 kB\"\n}\n}\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:srl# show version\n---------------------------------------------------\nHostname             : srl\nChassis Type         : 7220 IXR-D3L\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:90:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-06T11:38:51.482Z\nTotal Memory         : 24052875 kB\nFree Memory          : 16973972 kB\n---------------------------------------------------\n</code></pre> <p>Okay, there is a lot of output here, focus first on the request message. In the request body, we have <code>cli</code> method set, and the <code>commands</code> list contains a list of strings, where each string is a CLI command as it is seen in the CLI. We have only one command to execute, hence our list has only one element - <code>show version</code>.</p> <p>The response message contains a list of results. Since we had only one command, our results list contains a single element, which matches the output of the <code>show version | as json</code> command when it is invoked in the CLI.</p> <p>Note</p> <p>The peculiar <code>\"basic system info\"</code> key in the response is a special node name that is set in the <code>show version</code> plugin of the CLI as a constant.</p> <p>SR Linux uses a concept of CLI plugins for all its <code>show</code> commands, and each such command has a root node name that has a unique name. For <code>show version</code> command this node name is <code>basic system info</code>.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#output-format","title":"Output format","text":"<p>Alright, we executed a CLI command, but the returned result is formed as JSON, which is a default formatting option for JSON-RPC. Can we influence that? Turns out we can.</p> <p>With <code>output-format</code> field of the request we can choose the formatting of the returned data:</p> <ul> <li>json - the default format option</li> <li>text - textual/ascii output as seen in the CLI</li> <li>table - table view of the returned data</li> </ul> Req with <code>text</code>Resp with <code>text</code>Req with <code>table</code>Resp with <code>table</code> <p><code>jq</code> arguments used in this command filter out the result element and use the raw processing to render newlines. <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq -r '.result[]'\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"show version\"\n        ],\n        \"output-format\": \"text\"\n    }\n}\nEOF\n</code></pre></p> <pre><code>-------------------------------------------------------------\nHostname             : srl\nChassis Type         : 7220 IXR-D3L\nPart Number          : Sim Part No.\nSerial Number        : Sim Serial No.\nSystem HW MAC Address: 1A:90:00:FF:00:00\nSoftware Version     : v22.11.1\nBuild Number         : 184-g6eeaa254f7\nArchitecture         : x86_64\nLast Booted          : 2022-12-06T11:38:51.482Z\nTotal Memory         : 24052875 kB\nFree Memory          : 16414484 kB\n-------------------------------------------------------------\n</code></pre> <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq -r '.result[]'\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"show version\"\n        ],\n        \"output-format\": \"table\"\n    }\n}\nEOF\n</code></pre> <pre><code>+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|    Hostname     |  Chassis Type   |   Part Number   |  Serial Number  |  System HW MAC  |    Software     |  Build Number   |  Architecture   |   Last Booted   |  Total Memory   |   Free Memory   |\n|                 |                 |                 |                 |     Address     |     Version     |                 |                 |                 |                 |                 |\n+=================+=================+=================+=================+=================+=================+=================+=================+=================+=================+=================+\n| srl             | 7220 IXR-D3L    | Sim Part No.    | Sim Serial No.  | 1A:90:00:FF:00: | v22.11.1        | 184-g6eeaa254f7 | x86_64          | 2022-12-06T11:3 | 24052875 kB     | 16466207 kB     |\n|                 |                 |                 |                 | 00              |                 |                 |                 | 8:51.482Z       |                 |                 |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#context-switching","title":"Context switching","text":"<p>When using CLI method, the commands entered one after another work exactly the same as when you enter them in the CLI. This means that current working context changes based on the entered commands. For instance, if you first enters to the interface context and then execute the <code>info</code> command, it will work out nicely, since the context switch is persistent across commands in the same RPC. The next RPC, as expected, will not maintain the context of a previous RPC; by default running datastore is activated and <code>/</code> context is set.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"interface mgmt0\",\n            \"info\"\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{},\n{\n\"name\": \"mgmt0\",\n\"description\": \"set-via-multi-cmd-json-rpc\",\n\"admin-state\": \"enable\",\n\"subinterface\": [\n{\n\"index\": 0,\n\"admin-state\": \"enable\",\n\"ipv4\": {\n\"dhcp-client\": {}\n},\n\"ipv6\": {\n\"dhcp-client\": {}\n}\n}\n]\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Note, how the <code>result</code> list contains two elements matching the number of commands used in the request. The first command - <code>/interface mgmt0</code> - doesn't produce any output, as it just enters the context of an interface. The second command though - <code>info</code> - produces the output as it dumps the configuration items for the interface, and we get its output with json formatting.</p>"},{"location":"tutorials/programmability/json-rpc/basics/#configuration","title":"Configuration","text":"<p>You guessed it right, you can also perform configuration tasks with CLI method and use the CLI format of the configuration to do that. Let's configure an interface using CLI-styled commands in different ways:</p> <p>Note</p> <p>When using CLI method for configuration tasks explicit entering into the candidate datastore and committing is necessary.</p> Contextual commandsFlattened commandsConfig dumpVerification <p>One option to use when executing configuration tasks is to use the commands sequence that an operator would have used. This way every other command respects the present working context.</p> <p>This method is error-prone, since tracking the context changes is tedious. But, still, this is an option.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"enter candidate\",\n            \"/interface ethernet-1/1\",\n            \"description \\\"this is a new interface\\\"\",\n            \"admin-state enable\",\n            \"commit now\"\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{},\n{},\n{},\n{},\n{\n\"text\": \"All changes have been committed. Leaving candidate mode.\\n\"\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Flattened commands are levied from the burdens of the contextual commands, as each command starts from the root. This makes configuration snippets longer, but safer to use.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"enter candidate\",\n            \"/interface ethernet-1/1 description \\\"this is a new interface\\\" admin-state enable\",\n            \"commit now\"\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{},\n{},\n{\n\"text\": \"All changes have been committed. Leaving candidate mode.\\n\"\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>Another popular way to use CLI-styled configs is to dump the configuration from the device, template or change a few fields in the text blob and use it for configuration. In the example below we did <code>info from running /interface ethernet-1/1</code> and captured the output. We used this output as is in our request body just escaping the quotes.</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"enter candidate\",\n            \"/interface ethernet-1/1 {\n              description \\\"this is a new interface\\\"\n              admin-state enable\n          }\",\n            \"commit now\"\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"result\": [\n{},\n{},\n{\n\"text\": \"All changes have been committed. Leaving candidate mode.\\n\"\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>All of the methods should result in the same configuration added: <pre><code>\u276f docker exec clab-srl01-srl sr_cli info from running /interface ethernet-1/1\n    interface ethernet-1/1 {\ndescription \"this is a new interface\"\nadmin-state enable\n}\n</code></pre></p>"},{"location":"tutorials/programmability/json-rpc/basics/#tools-commands_1","title":"Tools commands","text":"<p>Remember how we executed the tools commands within the Set method? We can do the same with CLI method, but in this case we provide the command in the CLI-style. Using the same command to clear statistics counters:</p> RequestResponse <pre><code>curl -s 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"cli\",\n    \"params\": {\n        \"commands\": [\n            \"tools interface mgmt0 statistics clear\"\n        ]\n    }\n}\nEOF\n</code></pre> <p>The result contains the text output of the tools command which confirms that the command worked:</p> <pre><code>{\n\"result\": [\n{\n\"text\": \"/interface[name=mgmt0]:\\n    interface mgmt0 statistics cleared\\n\\n\"\n}\n],\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#https","title":"HTTPS","text":"<p>All of the examples have been using plain HTTP schema. As was explained in the beginning of this tutorial, containerlab configures JSON-RPC server to run both HTTP and HTTPS transports.</p> <p>To use the secured transport any request can be changed to https schema and skipped certificate verification:</p> <pre><code>curl -sk https://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>If you want to verify the self-signed certificate that containerlab generates at startup use the CA certificate that containerlab keeps in the lab directory:</p> <pre><code>curl -s --cacert ./clab-srl01/ca/root/root-ca.pem https://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc -d @- &lt;&lt;EOF | jq\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre>"},{"location":"tutorials/programmability/json-rpc/basics/#error-handling","title":"Error handling","text":"<p>When either of the commands specified in the RPC request message fails, the returned message will contain an error, even if other commands might be correct. This atomicity of the commands is valid for both Get and Set methods.</p> <p>For example, the following request has two commands, where 2<sup>nd</sup> command uses a wrong path.</p> RequestResponse <pre><code>curl -v 'http://admin:NokiaSrl1!@clab-srl01-srl/jsonrpc' -d @- &lt;&lt;EOF\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\": {\n        \"commands\": [\n            {\n                \"path\": \"/interface[name=mgmt0]/statistics\",\n                \"datastore\": \"state\"\n            },\n            {\n                \"path\": \"/system/somethingwrong\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <pre><code>{\n\"error\": {\n\"code\": -1,\n\"message\": \"Path not valid - unknown element 'somethingwrong'. Options are [features, trace-options, management, configuration, aaa, authentication, warm-reboot, boot, l2cp-transparency, lacp, lldp, mtu, name, dhcp-server, event-handler, ra-guard-policy, gnmi-server, tls, json-rpc-server, bridge-table, license, dns, ntp, clock, ssh-server, ftp-server, snmp, sflow, load-balancing, banner, information, logging, mirroring, network-instance, maintenance, app-management]\"\n},\n\"id\": 0,\n\"jsonrpc\": \"2.0\"\n}\n</code></pre> <p>The response will contain just an error container, even though technically the first command is correct. Note, that the HTTP response code is still <code>200 OK</code>, since JSON-RPC was able to deliver and execute the RPC, it is just that the RPC lead to an error.</p> <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>differences in capabilities that different management interfaces provide are driven by the interfaces standards.\u00a0\u21a9</p> </li> <li> <p>using HTTP POST method.\u00a0\u21a9</p> </li> <li> <p>JSON-RPC, as gNMI, can run in a user-configured network-instance.\u00a0\u21a9</p> </li> <li> <p>https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-path-conventions.md \u21a9</p> </li> <li> <p>Tools paths can be viewed in our tree YANG browser.\u00a0\u21a9</p> </li> <li> <p>Support for setting configuration with Openconfig schema will be added at a later date. Currently only CLI method allows working with Openconfig schema via <code>enter oc</code> command.\u00a0\u21a9</p> </li> <li> <p>SR Linux logs JSON-RPC incoming and outgoing requests to <code>/var/log/srlinux/debug/sr_json_rpc_server.log</code> log file.\u00a0\u21a9</p> </li> </ol>"},{"location":"yang/","title":"SR Linux &amp; YANG","text":"<p>Model-driven (MD) interfaces are becoming essential for robust and modern Network OSes. The changes required to create fully model-driven interfaces can not happen overnight - it is a long and tedious process that requires substantial R&amp;D effort. Traditional Network OSes often had to take an evolutionary route with adding MD interfaces on top of the existing internal infrastructure.</p> <p> </p> SR Linux ground-up support for YANG <p>Unfortunately, bolting on model-driven interfaces while keeping the legacy internal infrastructure layer couldn't fully deliver on the promises of MD interfaces. In reality, those new interfaces had visibility discrepancies<sup>1</sup>, which often led to a situation where users needed to mix and match different interfaces to achieve some configuration goal. Apparently, without adopting a fully modeled universal API, it is impossible to make a uniform set of interfaces offering the same visibility level into the NOS.</p> <p>Nokia SR Linux was ground-up designed with YANG<sup>2</sup> data modeling taking a central role. SR Linux makes extensive use of structured data models with each application regardless if it's being provided by Nokia or written by a user has a YANG model that defines its configuration and state.</p> <p> </p> Both Nokia and customer's apps are modeled in YANG <p>SR Linux exposes the YANG models to the supported management APIs. For example, the command tree in the CLI is derived from the SR Linux YANG models loaded into the system, and a gNMI client uses RPCs to configure an application based on its YANG model. When a configuration is committed, the SR Linux management server validates the YANG models and translates them into protocol buffers for the impart database (IDB).</p> <p>With this design, there is no way around YANG; the data model is defined first for any application SR Linux has, then the CLI, APIs, and show output formats derived from it.</p>","tags":["yang"]},{"location":"yang/#sr-linux-yang-models","title":"SR Linux YANG Models","text":"<p>As YANG models play a central role in SR Linux NOS, it is critical to have unobstructed access. With that in mind, we offer SR Linux users many ways to get ahold of SR Linux YANG models:</p> <ol> <li>Download modules from SR Linux NOS itself.     The models can be found at <code>/opt/srlinux/models/*</code> location.</li> <li>Fetch modules from <code>nokia/srlinux-yang-models</code> repo.</li> <li>Use SR Linux YANG Browser to consume modules in a human-friendly way</li> </ol> <p>SR Linux employs a uniform mapping between a YANG module name and the CLI context, making it easy to correlate modules with CLI contexts.</p> YANG modules and CLI aligned <p>The structure of the Nokia SR Linux native models may look familiar to the OpenConfig standard, where different high-level domains are contained in their modules.</p> <p>Source <code>.yang</code> files are great for YANG-based automation tools such as ygot but are not so easy for a human's eye. For living creatures, we offer a YANG Browser portal. We suggest people use it when they want to consume the models in a non-programmable way.</p> <ol> <li> <p>indicated by the blue color on the diagram and explained in detail in NFD25 talk.\u00a0\u21a9</p> </li> <li> <p>RFC 6020 and RFC 7950 \u21a9</p> </li> </ol>","tags":["yang"]},{"location":"yang/browser/","title":"SR Linux YANG Browser","text":"<p>YANG data models are the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse, search through, and process these data models.</p> <p>To answer these demands, we created a web portal - yang.srlinux.dev - it offers:</p> <ul> <li>Fast Path Browser to effectively search through thousands of available YANG paths</li> <li>Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux</li> <li>Source <code>.yang</code> files neatly stored in <code>nokia/srlinux-yang-models</code> repository for programmatic access and code generation</li> </ul> <p></p> <p>The web portal's front page aggregates links to individual releases of YANG models. Select the needed version to open the web view of the YANG tools we offer.</p> <p></p> <p>The main stage of the YANG Browser view is dedicated to the Path Browser , as it is the most efficient way to search through the model. Additional tools are located in the upper right corner . Let's cover them one by one.</p>"},{"location":"yang/browser/#path-browser","title":"Path Browser","text":"<p>As was discussed before, SR Linux is a fully modeled system with its configuration and state data entirely covered with YANG models. Consequently, to access any data for configuration or state, one needs to follow the YANG model. Effectively searching for those YANG-based access paths is key to rapid development and operations. For example, how to tell which one to use to get ipv4 statistics of an interface?</p> <p>With Path Browser, it is possible to search through the entire SR Linux YANG model and extract the paths to the leaves of interest. The Path Browser area is composed of three main elements:</p> <ul> <li>search input for entering the query </li> <li>Config/State selector </li> <li>table with results for a given search input </li> </ul> <p> </p> Path Browser elements <p>A user types in a search query, and the result is rendered immediately in the table with the matched words highlighted. The Config/State selector allows users to select if they want the table to show config, state, or all leaves. The state leaf is a leaf that has <code>config false</code> statement<sup>2</sup>.</p>"},{"location":"yang/browser/#path-structure","title":"Path structure","text":"<p>The table contains the flattened XPATH-like paths<sup>3</sup> for every leaf of a model sorted alphabetically.</p> <ul> <li>Each path is denoted with a State attribute in the first column of a table. Leaves, which represent the state data, will have the <code>true</code> value in the first column<sup>2</sup>.</li> <li>List elements are represented in the paths as <code>list-element[key-name=*]</code> - a format suitable for gNMI subscriptions.</li> <li>Each leaf is provided with the type information.</li> </ul>"},{"location":"yang/browser/#search-capabilities","title":"Search capabilities","text":"<p>Snappy search features of the Path Browser make it a joy to use when exploring the model or looking for a specific leaf of interest.</p> <p>Let's imagine we need to solve the simple task of subscribing to interface traffic statistics. How would we know which gNMI path corresponds to the traffic statistics counters? Should we try reading source YANG files? But it is challenging as models have lots of imports and quite some augmentations. A few moments and - you're lost. What about the tree representation of a model generated with <code>pyang</code>? Searching through something like pyang's tree output is impractical since searching the tree representation can't include more than one search parameter. The search becomes a burden on operators' eyes.</p> <p>Path Browser to the rescue. Its ability to return search requests instantaneously makes interrogating the model a walk in the park. The animation below demos a leaf-searching exercise where a user searches for a state leaf responsible for traffic statistics.  </p> <p>First, a user tries a logical search query <code>interface byte</code>, which yields some results, but it is easy to spot that they are not related to the task at hand. Thanks to the embedded highlighting capabilities, the search inputs are detectable in the resulting paths.</p> <p>Next, they try to use <code>interface octets</code> search query hoping that it will yield the right results, and so it does!</p> <p>Tip</p> <p>Every table row denotes a leaf, and when a user hovers a mouse over a row, the popup appears with a description of the leaf.</p>"},{"location":"yang/browser/#tree-browser","title":"Tree Browser","text":"<p>The Path Browser is great to search through the entire model, but because it works on flattened paths, it hides the \"tree\" view of the model. Sometimes the tree representation is the best side to look at the models with a naked eye, as the hierarchy becomes very clear.</p> <p>To not strip our users of the beloved tree view mode, we enhanced the <code>pyang -f jstree</code> output and named this view Tree Browser.</p> <p> </p> Access Tree Browser <p>The tree view of the model offers a step-by-step exploration of the SR Linux model going from the top-level modules all the way down to the leaves. The tree view displays the node's type (leaf/container/etc) as well as the leaf type and the read-only status of a leaf.</p> <p> </p> Tree Browser view <p>Tip</p> <p>Every element of a tree has a description that becomes visible if you hover over the element with a mouse. </p>"},{"location":"yang/browser/#tree-and-paths","title":"Tree and Paths","text":"<p>If you feel like everything in the world better be in ASCII, then Tree and Paths menu elements will satisfy the urge. These are the ASCII tree of the SR Linux model<sup>1</sup> and the text flattened paths that are used in the Path Browser.</p> <p> </p> Text version of tree and paths <p>The textual paths can be, for example, fetched with curl and users can <code>sed</code> themselves out doing comprehensive searches or path manipulations.</p> <ol> <li> <p>extracted with <code>pyang -f tree</code> \u21a9</p> </li> <li> <p>refer to https://datatracker.ietf.org/doc/html/rfc6020#section-4.2.3 \u21a9\u21a9</p> </li> <li> <p>paths are generated from the YANG model with gnmic \u21a9</p> </li> </ol>"},{"location":"blog/tags/","title":"Tags","text":""},{"location":"blog/tags/#ansible","title":"ansible","text":"<ul> <li>Ansible Collection For SR Linux</li> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>Official Ansible collection for SR Linux</li> </ul>"},{"location":"blog/tags/#backup","title":"backup","text":"<ul> <li>Configuration backup with Event Handler</li> </ul>"},{"location":"blog/tags/#bgp","title":"bgp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#config-management","title":"config management","text":"<ul> <li>Configuration backup with Event Handler</li> </ul>"},{"location":"blog/tags/#demo","title":"demo","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#elk","title":"elk","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#event-handler","title":"event handler","text":"<ul> <li>Configuration backup with Event Handler</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> <li>Event Handler-based Oper Groups tutorial</li> </ul>"},{"location":"blog/tags/#evpn","title":"evpn","text":"<ul> <li>L2 EVPN with SR Linux</li> </ul>"},{"location":"blog/tags/#fss","title":"fss","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#ixp","title":"ixp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#json-rpc","title":"json-rpc","text":"<ul> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>JSON-RPC Management Interface</li> </ul>"},{"location":"blog/tags/#kne","title":"kne","text":"<ul> <li>SR Linux with KNE</li> </ul>"},{"location":"blog/tags/#ldp","title":"ldp","text":"<ul> <li>LDP-based MPLS with SR Linux</li> </ul>"},{"location":"blog/tags/#logging","title":"logging","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#markdown","title":"markdown","text":"<ul> <li>SR Linux Blog Launch</li> </ul>"},{"location":"blog/tags/#media","title":"media","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#mpls","title":"mpls","text":"<ul> <li>LDP-based MPLS with SR Linux</li> </ul>"},{"location":"blog/tags/#ndk","title":"ndk","text":"<ul> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li>NetOps Development Kit</li> </ul>"},{"location":"blog/tags/#nfd","title":"nfd","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#openbgp","title":"openbgp","text":"<ul> <li>Basic IXP Lab with OpenBGPd Route Server</li> </ul>"},{"location":"blog/tags/#openconfig","title":"openconfig","text":"<ul> <li>SR Linux with KNE</li> </ul>"},{"location":"blog/tags/#packet-pushers","title":"packet pushers","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#podcast","title":"podcast","text":"<ul> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/tags/#presentation","title":"presentation","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> </ul>"},{"location":"blog/tags/#pygments","title":"pygments","text":"<ul> <li>SR Linux Syntax Highlighting with Pygments</li> </ul>"},{"location":"blog/tags/#sr-linux","title":"sr linux","text":"<ul> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> <li>JSON-RPC Management Interface</li> <li>Basic IXP Lab with OpenBGPd Route Server</li> <li>Official Ansible collection for SR Linux</li> <li>SR Linux logging with ELK</li> <li>SR Linux Syntax Highlighting with Pygments</li> </ul>"},{"location":"blog/tags/#syslog","title":"syslog","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"blog/tags/#yang","title":"yang","text":"<ul> <li>SR Linux &amp; YANG</li> </ul>"},{"location":"blog/media/","title":"SR Linux in the Media","text":"<p>We love to talk about SR Linux and all things around it. Here are some of the podcats, screencasts and demos we did in the past.</p>"},{"location":"blog/media/#media","title":"media","text":"<ul> <li> DevOps Approaches for Enhanced NetOps with Nokia Data Center Fabric Solution</li> <li> Design, Deploy, And Operate With Nokia Data Center Fabric Solution</li> <li> Develop Custom Network Apps With Nokia\u2019s SR Linux NDK</li> <li> Event-Driven Automation With Nokia\u2019s SR Linux Event Handler Framework</li> </ul>"},{"location":"blog/page/2/","title":"Blog","text":""}]}